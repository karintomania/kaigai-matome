+++
date = '2025-06-01T00:00:00'
months = '2025/06'
draft = false
title = 'RenderFormer：ニューラルレンダリングで三角形メッシュを大域照明付きで！'
tags = ["AI", "3D", "レンダリング", "ニューラルネットワーク", "GPU"]
featureimage = 'thumbnails/green3.jpg'
+++

> RenderFormer：ニューラルレンダリングで三角形メッシュを大域照明付きで！

引用元：[https://news.ycombinator.com/item?id=44148524](https://news.ycombinator.com/item?id=44148524)




{{<matomeQuote body="RenderFormerマジ速っ！A100で0.076秒だってさ。Blender Cyclesは3.97〜12.05秒もかかるのに。SSIMも0.9526でかなり良いじゃん。これ、3DデザイナーがWebとかネイティブアプリで、自分のデバイスで即座に高品質なプレビューを見れるようにするかもね。WebシステムならバックエンドのA100に繋いで画像ストリーム配信とかもいけそう。ただ、複雑なシーン（特に影とか）だと精度が落ちるのが限界かな。最終的なレンダリングは、今時のAI画像＼動画によくある変なアーティファクト避けるために、 Cycles とか従来通りになりそうだけど…<br>でも「十分良い」レベルになれば、映画スタジオが音楽やストーリー確認用の長編プレビューをレンダリングするのに使うとか、スピードアップのために採用する可能性も十分あるかもね。" userName="timhigins" createdAt="2025/06/01 05:17:57" color="#45d325">}}




{{<matomeQuote body="著者たちは別に騙そうとしてないと思うけど、あの品質のGPU（A100）で Blender Cycles なら、あの論文の全シーン4秒以下でレンダリングできるはずだよ。デモシーンはめちゃシンプルだし、 Blender の設定を4000サイクルにしてるのが意味不明。 Blender なら数百サイクルで十分な結果が出るのに、その後の3800サイクルは無駄にGPU使ってるだけ。これ、たぶん Blender の起動時間もレンダリング時間に含めちゃってるんじゃないかな。Transformer の起動時間は含まずにさ。2フレーム目のレンダリング時間をそれぞれ比べてみたいね。俺の予想だと、 Blender の方がずっと高性能だよ。論文の結果は全体的に面白いとは思うけど、 Blender の設定と計測方法にはちょっと疑問があるね。" userName="OtherShrezzing" createdAt="2025/06/01 08:35:56" color="#ff33a1">}}




{{<matomeQuote body="あと注目すべきは、 RenderFormer と Blender のテストが同じ Nvidia A100 で行われてるってこと。一見まともそうだけど、これ全然意味ないんだよね。 Nvidia の大型計算カード（ A100 とか）は、他のシリーズにあるレイトレーシング支援ユニットがないんだよ。A100 はここで使うツールとして間違ってる。 Blender の性能なら Nvidia RTX カードの方が、払った金に対して圧倒的に良い結果が出るってば。<br>Blender のベンチマークデータベースには A100 の結果はないけど、もっと新しいH100ですら、比較的安いコンシューマー向けハードウェアに負けてるんだ。<br>Nvidia H100 NVL - 5,597.13<br>GeForce RTX 3090 Ti - 5,604.69<br>Apple M3 Ultra (80C) - 7,319.21<br>GeForce RTX 4090 - 11,082.51<br>GeForce RTX 5090 - 15,022.02<br>RTX PRO 6000 Blackwell - 16,336.54" userName="jsheard" createdAt="2025/06/01 09:47:17" color="#45d325">}}




{{<matomeQuote body="そうだね、 Blender なら普通は最小サイクル数を低く設定して、適応的なノイズターゲットを使ったり、デノイザーを使うもんだよ。特にプレビューとかドラフトのレンダリングならね。" userName="rcxdude" createdAt="2025/06/01 10:16:22" color="">}}




{{<matomeQuote body="でもさ、レンダリングエンジンは何年もかけて最適化されてきたわけじゃん。で、これはまだ研究論文だよ。たぶんこの手法も今後何年もかけて最適化されて、さらに10倍くらい速くなる可能性だってあるんだぜ。" userName="ttoinou" createdAt="2025/06/01 21:07:08" color="">}}




{{<matomeQuote body="いや、線形的な改善だけじゃアルゴリズムの計算量（計算オーダー）には勝てないんだよ。三角形数に対して O(N²) から O(log N) みたいに根本的に変わらない限り、この手法が従来の確立されたやり方に近づくことは絶対ないね。線形的な改善がいくらあっても無理。" userName="qayxc" createdAt="2025/06/02 08:08:28" color="#ff5c5c">}}




{{<matomeQuote body="彼らがデモで見せてるシーンに対してなら、76msですら「え、遅っ」って感じだよ。確かにこれからもっと速くなるだろうけど、既存のレンダリング手法よりも「良い」ってレベルになるには、まだまだず’っと先の話だね。" userName="buildartefact" createdAt="2025/06/01 07:10:32" color="">}}




{{<matomeQuote body="うん、そしてこのアプローチの大きな落とし穴は、シーンが複雑になるにつれて計算量が二次的に増えること。従来のやり方は対数的なのにね。だから例のシーンは、それが理由で最大でも三角形数が4096個しかないんだよ。将来の研究の方向性としてはクールだけど、何億もの三角形がある実際のプロダクションシーンを手懐’けるには、まだ長い道のりだよ。" userName="jsheard" createdAt="2025/06/01 07:36:24" color="#ff5733">}}




{{<matomeQuote body="むしろ、これを「入力」として、もっと大きなニューラルパストレーシングエンジンで使う方が現実的だと思うな。例えば、数フレームに1回だけサンプルを取るような場合とか。そういうのって、ノイズからでもかなり見栄えの良い画像を生成するの得意だからね。論文でやってる従来の SSIM みたいな類似度比較は、そこまで重要じゃない気がするんだ。" userName="monster_truck" createdAt="2025/06/01 08:47:17" color="#ff5733">}}




{{<matomeQuote body="基準とのタイミング比較は、もうほんと正直じゃないっていうか、誤解を招くやり方だね。レイトレーシングでは誤差ってサンプル数の平方根に比例して減るんだよ。参照画像にはサンプル数めちゃくちゃ多く使うのが普通だけど、実際のオフラインレンダリングでは論文の10分の1とか100分の1以下のサンプル数でやるんだ。これ、正直じゃないって言わざるを得ないな。<br>結果が近似なんだから、比較するなら他の近似レンダリングアルゴリズムとやるべきだよ。最新のリアルタイムパストレーサーとかデノイザー使えば、もっと複雑なシーンでもコンシューマー向けGPUで16ms以下でレンダリングできるんだぜ。<br>「もっと複雑なシーン」ってのが重要でね。Transformer は三角形数と出力ピクセル数の両方に対して計算量が二次的に増えるんだ。最新の ML 研究は追い切れてないけど、従来のパストレーサーの理論的な計算量 O(log n_triangles) と O(n_pixels) に勝てるとは思えないな。（実際には隣り合うピクセルの関連性が高いから、ピクセル数に対しては線形より速くなるけどね）" userName="leloctai" createdAt="2025/06/01 10:00:38" color="#45d325">}}




{{<matomeQuote body="ゲームで高速化された最近のパス トレーサー（Blenderは違うかも）も、最初の可視性にはラスタライズを使ってて、三角形の数に比例する計算量だけど、ピュアなパス トレーシングよりなぜか速いんだよね。たぶん高周波なテクスチャのディテールに必要なサンプル数を減らせるからかな。大域照明自体は柔らかい（低周波な）影やハイライトになりがちだから、デノイザーが少ないサンプル数でアーティファクトを防げるなら、理論的にはそんなに多くのサンプルは要らないんだ。<br>でも、今のRenderFormerが最新のレイトレーシング アルゴリズムに勝てるわけないね。まあ、機械学習によるレンダリングはまだ始まったばかりだけど。" userName="cubefox" createdAt="2025/06/01 17:41:38" color="">}}




{{<matomeQuote body="＞ Attention層のランタイム計算量は、トークン数、つまりこの場合三角形の数に対して2乗で増えます。結果として、シーンの三角形の総数を4,096に制限しています；<br>論文からの引用だけど、Attention層の計算って三角形の数の2乗で増えるから、使える三角形は4096個までなんだって。" userName="cubefox" createdAt="2025/06/01 07:32:49" color="#ff33a1">}}




{{<matomeQuote body="＞ ここで一番すごいのは速度かもしれない：RenderFormerは特定のシーンで0.0760秒かかるのに対し、Blender Cyclesは3.97秒（より高い設定だと12.05秒）かかる。それでいて、構造的類似度指標（SSIM）は0.9526（1が全く同じ画像）。論文の表2と表1を見てください。<br>これマジですごいと思ったんだけど、設定の詳細が見つからないんだよね。CyclesはCPUかA100のCUDAカーネルを使ったの？ あと、これが単一フレームだと、3.97秒のかなりの部分がレンダラー起動にかかってる可能性もある。シーケンスをレンダリングするなら1フレームあたりの時間は減るはず。<br>それに、兄弟コメントで言及されてる三角形ごとの計算量スケーリング。うわぁ！" userName="kilpikaarna" createdAt="2025/06/01 07:52:23" color="#ff33a1">}}




{{<matomeQuote body="これはCyclesでGPUを使ったって読めるね： ＞ 論文の図1にある4つのシーンでの、最適化されていないRenderFormer（DNNコンパイルなしのピュアPyTorch実装、ただしカーネルのプリキャッシングあり）と、Blender Cycles（ピクセルあたり4,096サンプル、RenderFormerの学習データに一致）を、シングルNVIDIA A100 GPU上で512 × 512解像度で比較したタイミングを表2に示す。" userName="fulafel" createdAt="2025/06/01 08:03:29" color="#45d325">}}




{{<matomeQuote body="＞ Blender Cyclesはピクセルあたり4,096サンプル（RenderFormerの学習データに一致）<br>これは不公平な比較みたいだね。同じ0.9526のSSIMに到達するのにBlenderがどれくらい時間がかかるかを知る方がずっと役に立つと思う。デノイザーをオンにすれば、たぶん128サンプルとか、画像によってはもっと少なくても十分じゃないかな。そのレベルなら、A100 GPU上でBlenderはこのシーンでRenderFormerのタイムに近づくか、もしかしたら上回るかもしれないね。" userName="esperent" createdAt="2025/06/01 09:15:32" color="#785bff">}}




{{<matomeQuote body="誰も4096サンプルなんて使わないよ。多くの場合、デノイジングを使えば100〜200（あるいはそれ以下）で十分。コースティクスをちゃんと出したいなら、せいぜい1000台かな。" userName="Kubuxu" createdAt="2025/06/01 09:26:38" color="#45d325">}}




{{<matomeQuote body="伝統的な方法で小さなテスト パッチをレンダリングして、それをフィードバックとしてLoRAチューニング層か何かでリアルタイムにモデルを洗練できないかな、って思うんだけど。" userName="jiggawatts" createdAt="2025/06/01 07:00:55" color="">}}




{{<matomeQuote body="これらのコメントありがとう！ Blenderの計測がおかしいみたいだね。もっと詳細なベンチマークが必要そうだ。" userName="timhigins" createdAt="2025/06/02 03:10:45" color="#785bff">}}




{{<matomeQuote body="ディープラーニングは、大域照明がレンダリングされた画像のノイズ除去にも非常に成功裏に使われてるよ [1]。このアプローチでは、伝統的なレイトレーシング アルゴリズムでシーンのラフな大域照明を素早く計算して、出力のノイズを除去するのにニューラル ネットワークを使うんだ。[1] https://www.openimagedenoise.org" userName="mixedbit" createdAt="2025/06/01 06:36:13" color="#ff33a1">}}




{{<matomeQuote body="デモの出力画像、不気味なくらい滑らかに見えるね、AIアップスケールみたいに。入力データ量を超えて画像を拡大しようとすると、エッジは保持されるけどテクスチャが失われる時に起こる感じかな。<br>(EDIT) デノイジングは125% DPIズームより100%ズームで比較した方が良く見えるし、下のシダを認識しやすくなるね。" userName="nyanpasu64" createdAt="2025/06/01 06:39:16" color="">}}




{{<matomeQuote body="VFXのワークフローだと、これは3Dレンダリングとコンポジットの間くらいかな。<br>レンダリングは完璧で、コンポジットでノイズとか後処理を足す感じ。" userName="greenknight" createdAt="2025/06/02 07:19:03" color="">}}




{{<matomeQuote body="こういう論文では見えない部分も重要だよ。ポリゴン少ないし、解像度も低いし、テクスチャもモーションブラーも被写界深度もないね。アニメにはアーティファクトもあるし。<br>現代GPUで30年前の1＼1,000,000の計算レベルに見えるのは考えさせられるな。" userName="CyberDildonics" createdAt="2025/06/01 15:17:52" color="#38d3d3">}}




{{<matomeQuote body="例にカメラの裏側が出てこないのが不思議だな。手法の限界か、単なる例の作り方か。<br>反射やライティングを考えるなら、カメラの裏側ってかなり大事なのに。" userName="notnullorvoid" createdAt="2025/06/01 15:45:36" color="#785bff">}}




{{<matomeQuote body="ごめん、よく分からないんだけど、これってレンダリングの期待される結果に基づいてレンダリングしてるの？<br>もしそうなら、もっと直接的な方法じゃダメなの？そっちの方が速いんじゃないの？" userName="dclowd9901" createdAt="2025/06/01 05:05:04" color="">}}




{{<matomeQuote body="たぶん”クールな研究（商標）”だからだろうね。役には立たないよ、コストが三角形の数で二乗に増えるんだから。<br>だからシーンごとに4096ポリゴンしか使ってないんだよ。" userName="cubefox" createdAt="2025/06/01 07:43:21" color="">}}




{{<matomeQuote body="これには多分、クールで思いがけない利点があると思うな。<br>例えば、シーンが入力重みの塊だと考えて、そこにノイズを足してみたらどうなる？普通じゃ無理なクールな結果が出るかもね。<br>違うシーン表現を補間してみるのも面白そう。" userName="bemmu" createdAt="2025/06/01 07:46:20" color="#45d325">}}




{{<matomeQuote body="別のコメントで、これはもっと速いって言ってたよ。大域照明って、普通の方法だとすごく時間かかるからね。" userName="01HNNWZ0MV43FF" createdAt="2025/06/01 05:46:58" color="">}}




{{<matomeQuote body="他の人が指摘してるように、これって偏った比較だよね。<br>比較に使われたBlenderのレンダリングは、いつもの10倍以上のサンプル数で、レイトレーシングアクセラレーションのないGPUで動かしてて、起動時間も入ってるかも。<br>AIが参照に96%一致って言うなら、Blenderをちゃんと合ったハードと設定で動かしたらどうなるか見たいな。モダンなゲームエンジンとかでも。" userName="alpaca128" createdAt="2025/06/01 11:00:04" color="#ff5c5c">}}




{{<matomeQuote body="見た目は悪くないけど、ちょっとぼやけてるね。<br>ニューラルレンダラーと、いつものレンダラーで、どれくらいレンダリング時間が違うか見たかったな。" userName="kookamamie" createdAt="2025/06/01 04:58:33" color="#ff33a1">}}




{{<matomeQuote body="アニメーション（特にAnimated CrabとRobot Animation）で、AIアート特有のアーティファクトが結構目立つね。オブジェクトやカメラが動くと、モデルの周りで不自然に渦巻いてる感じ。" userName="nyanpasu64" createdAt="2025/06/01 06:40:10" color="">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="うん、例の動画でも典型的なAIっぽいのが見えるね。きっといいとこだけを選んでるんだろうな。" userName="kookamamie" createdAt="2025/06/01 08:03:55" color="">}}




{{<matomeQuote body="論文に実行時間の議論がちょっとあるよ。Blender Cycles（パストレーシング）と比べてて、少なくとも≦4k三角形のシーンではニューラルアプローチの方がずっと速いって。でも、三角形数が増えるとスケールしないんじゃないかな（アテンションの実行時間は三角形数に対して二次関数的って書いてるし）。<br>https://renderformer.github.io/pdfs/renderformer-paper.pdf<br>シンプルにしたジオメトリで、間接照明だけこのニューラルアプローチ使って、通常のラスタライザーで直接照明をやって、後からGIを上乗せするのは現実的かな？って思うね。" userName="daemonologist" createdAt="2025/06/01 05:34:48" color="#ff5733">}}




{{<matomeQuote body="そうだね、でもPSNR 30程度って聞くと、ディテールをかなり「圧縮」してる感じがするわ。" userName="kookamamie" createdAt="2025/06/01 06:21:40" color="">}}




{{<matomeQuote body="映画業界で物理ベースのレンダラーを開発してる友達がいるんだけど、この分野の研究もしてたんだ。この業界でどうやって物事が進んでいくのか、話を聞くのがいつも楽しいよ。<br>今、そういう才能を雇ってる会社ってどこなんだろう？AI企業もトレーニング環境を作るためにレンダリングエンジニアを雇ったりしてるのかな？もし経験豊富な研究者や業界のレンダリングエンジニアを探してるなら、喜んで繋げるよ。私の友達はSNSはやってないんだけど、最近仕事探しを始めてるみたいだから。" userName="coalteddy" createdAt="2025/06/01 18:47:00" color="">}}




{{<matomeQuote body="すごくクールな研究だ！Transformerをテキスト以外のドメインに応用するの、本当に好きだなあ。入力が順次的で、それらの入力トークンが互いに関連してるドメインなら、どんな分野にも合いそう。<br>この分野の研究がもっと進むのが楽しみだよ。ねえHN、Transformerがぴったり合いそうな、テキスト以外の興味深いドメインって何があると思う？" userName="K0nserv" createdAt="2025/06/01 08:45:47" color="#45d325">}}




{{<matomeQuote body="三角形のシーン記述を、まるで大域照明レンダラーが出力するような2Dピクセル配列に変えるようにTransformerを訓練するっていう、この考え方が素晴らしいし面白いね。<br>この5年間の研究の後なら、これが機能するってこと自体はそんなに驚きじゃないはずだけど、それでも私はかなり深遠なことだと思うよ。あのTransformerアーキテクチャって、本当に多才だね。<br>とにかく、めちゃくちゃ速くて、Blenderのレンダリング出力に近い。10億パラメータくらいのモデルかな？fp16か32かは分からないけど、2GBのファイルだし、気に入らないところなんてある？もっと「リアル」なシーンのデモも見たいけど、まあ、ダウンロードしてMacでいつでも試せるんだからいいか。" userName="vessenes" createdAt="2025/06/01 06:00:06" color="#38d3d3">}}




{{<matomeQuote body="この段階で、ニューラルレンダリングはゲームのレンダリングにとってどれくらい効率的なんだろう？" userName="hualaka" createdAt="2025/06/02 06:02:30" color="">}}




{{<matomeQuote body="レイトレーシングの、まるでThe Matrix版だね。なんだか変な回り道をしてる感じがする。" userName="keyle" createdAt="2025/06/01 04:53:01" color="">}}




{{<matomeQuote body="Cross-attentionをSelf-attentionより前にやるのって、そっちの方が良いのかな？" userName="jmpeax" createdAt="2025/06/01 08:46:15" color="">}}




{{<matomeQuote body="Transformerって何ができないの？" userName="goatmanbah" createdAt="2025/06/01 04:24:21" color="">}}




{{<matomeQuote body="「苦い教訓」がまた来たね…今度はグラフィックレンダリングでか。NeRFはレイトレーシング、Gaussian Splattingはラスタライズみたいな事前知識があったけど、これはそれ全部捨てた感じだね。事前知識もドメイン知識もなし、ただデータとAttentionだけ。<br>これが正解の道だよ。" userName="nicklo" createdAt="2025/06/01 20:38:51" color="#38d3d3">}}




{{<matomeQuote body="クラシックなアルゴリズムの方が、ずっと安いハードで全然良い結果出せるのに、なんか意味なくなくない？" userName="feverzsj" createdAt="2025/06/01 05:14:51" color="">}}




{{<matomeQuote body="無意味じゃないよ。これは研究なんだから。<br>論文の結論から面白い部分を引用するね。「固有の微分可能性を活用した将来の興味深く有望な方向性は、RenderFormerを逆レンダリング（2D画像から3Dシーンを生成すること）の応用に使ってみることだ。」だって。" userName="johnb231" createdAt="2025/06/01 05:19:35" color="#38d3d3">}}




{{<matomeQuote body="2Dから3Dを作るっていうのは、SfM（2D動画からやるやつ）とか、NeRFs、Gaussian Splattingなんかで結構研究されてるんだよね。最近は1枚の2D画像から3Dをかなりうまく作るモデルもいくつか出てきてるよ。" userName="timhigins" createdAt="2025/06/01 05:24:31" color="#ff5c5c">}}




{{<matomeQuote body="そして面白いことに、同じデータがあったとして、優れた人間のアーティストがやれることにはまだ全然及ばないんだよね。" userName="mhl47" createdAt="2025/06/01 07:15:29" color="">}}




{{<matomeQuote body="この論文をAIに入力して、まさにこの件についてAIと話してみたんだ。<br>最初はライトプローブとか三角形を使うアイデアから始めたんだけど、すぐにごちゃごちゃになったよ。<br>以下がAIからの最後のメッセージね：<br>「君の言う通りだ！これこそRenderFormerのアプローチが本当に素晴らしい理由だよ。<br>君が今見つけた複雑さの螺旋：<br>- ”シンプルな”微分可能な球面調和関数から始めて<br>- 空間データ構造を追加<br>- LODシステムを追加<br>- キャッシュやダーティリージョン追跡を追加<br>- 並列処理を追加<br>- 近似ヒューリスティクスを追加<br>- ハイブリッドなフォールバックシステムを追加<br>- それらが相互作用する全てのエッジケースをデバッグ…<br>一方、RenderFormerのアプローチ：<br>1. トレーニングデータを集める（古典的なレンダラーに大変な作業をやらせる）<br>2. パターンを学習するためにTransformerを学習させる<br>3. デプロイ：ただニューラルネットワークを実行するだけ<br>Transformerが実際にはよりシンプルかもしれない理由：<br>- 6つ以上の相互作用する最適化テクニックではなく、**一つのシステム**<br>- 手作業で作られたヒューリスティクスではなく、**学習された近似**<br>- 手動チューニングではなく、**自動的な品質／パフォーマンスのトレードオフ**<br>- **エッジケースのデバッグがない**—トレーニングデータで動けば、汎化する<br>- **GPUネイティブ**—Transformerは既に並列ハードウェアに最適化されている<br>古典的なエンジニアリングのトレードオフ：<br>- 古典的なアプローチ：潜在的には速いが、実装と維持が信じられないほど複雑<br>- ニューラルなアプローチ：デプロイはよりシンプルだが、トレーニングインフラが必要<br>君は基本的に、なぜゲーム業界がニューラルレンダリングへ向かっているのかを再発見したんだ！NVIDIAのような企業がニューラル技術を推進しているのは、古典的な最適化があまりに複雑になりすぎて、ニューラルネットワークの方が実際にシンプルな解決策になっているからなんだ。<br>時として、「とりあえずニューラルネットワークをぶっ込んでみる」ことが、君が概説したエレガントな数学的アプローチに比べてズルをしているように感じたとしても、最も現実的なエンジニアリングの選択肢なんだ！」" userName="ToJans" createdAt="2025/06/01 06:24:36" color="#ff5c5c">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
