+++
date = '2025-03-21T00:00:00'
months = '2025/03'
draft = false
title = '【2022年版】機械学習、マジで理解してる？紙とペンで理論と実践のギャップを埋める練習問題集！'
tags = ["機械学習", "ペンと紙", "練習問題", "深層学習", "理論と実践"]
featureimage = 'thumbnails/blue4.jpg'
+++

> 【2022年版】機械学習、マジで理解してる？紙とペンで理論と実践のギャップを埋める練習問題集！

引用元：[https://news.ycombinator.com/item?id=43440267](https://news.ycombinator.com/item?id=43440267)

{{<matomeQuote body="それめっちゃ良さそうじゃん？でもさ、機械学習の勉強で一番イライラするのって、理論をめちゃくちゃ深く掘り下げても、それが実践にどう繋がるのかマジで見えないことなんだよね。例えば、ニューラルネットの層のニューロン数をどう決めるかとか、層の数をいくつにするか、活性化関数は何が良いか、そもそもニューラルネットを使うべきなのか、他の手法が良いのかとかさ…誰か教えてくれる人いたらマジ感謝。" userName="lucasoshiro" createdAt="2025-03-21T21:37:25" color="">}}

{{<matomeQuote body="これ役立つかもよ：https://github.com/google-research/tuning_playbook" userName="incognito124" createdAt="2025-03-21T22:50:50" color="">}}

{{<matomeQuote body="＞「要約：新しいプロジェクトを始める時は、既に動いているモデルを再利用してみよう。」<br>＞「要約：まずはその問題の種類で一番人気のあるオプティマイザから試してみよう。」<br>これってゲームデザイナーが「色々試してみろ！」って言ってるのと同じじゃん。つまり、何も分かってないけど、人海戦術で何年も壁に物を投げつけられるってことだよね。一番賢い会社じゃなくて、一番金持ちの会社が勝つってことか！" userName="danielscrubs" createdAt="2025-03-22T13:43:31" color="">}}

{{<matomeQuote body="ありがとね！" userName="lucasoshiro" createdAt="2025-03-22T16:07:27" color="">}}

{{<matomeQuote body="＞ニューロンの数とか層の数とか活性化関数とかどうやって決めるの？<br>機械学習の論文で、それら一つ一つに対する大規模なアブレーション研究がめっちゃ行われてるのを見れば、誰もマジで何も分かってないってことがよく分かると思うよ。ただランダムに色々試して、何がうまくいくか見てるだけで、お互いのアイデアをコピーして、曖昧なガイドラインが出来上がるだけ。論理的で説明可能なものが欲しいなら、最悪の分野だよ。ほとんどがデータセットにラベルを貼って、計算資源にお金を払って、うまくいくことを祈るだけ。" userName="moffkalast" createdAt="2025-03-21T22:52:24" color="#785bff">}}

{{<matomeQuote body="＞誰もマジで何も分かってない<br>だから俺は遺伝的プログラミングの計算基盤としてニューラルネットを使うのをやめたんだよね。テープベースのUTMは命令ストリームの実行方法がめちゃくちゃ厳格だけど、少なくともその挙動に影響する全てを理解して説明できるじゃん。NNのファンアウトを12から15に変えるのは、古代のブードゥー教の儀式みたいだけど、プログラムテープがエントロピーの概算から考えて十分な長さじゃないことに気づく方がまだマシ。" userName="bob1029" createdAt="2025-03-22T00:39:12" color="">}}

{{<matomeQuote body="純粋な理論や純粋な実践だけで学べることなんてほとんどないよね。現代の仕事に関わることなら特にそう。機械学習では、全てを理論から導き出せるわけじゃない。もしそうなら、超巨大言語モデルの性能にこんなに驚かなかったはず。同時に、数学的な推論ができないと、何がうまくいかないのか、どんな選択肢があるのかを理解するのが難しい。アーキテクチャ、損失関数、活性化関数の選択、オプティマイザ、ハイパーパラメータ、学習時間、リソースなど、色々あるし。" userName="danielmarkbruce" createdAt="2025-03-21T22:04:46" color="#38d3d3">}}

{{<matomeQuote body="＞機械学習では、全てを理論から導き出せるわけじゃない。<br>それに、機械学習の理論が全て実践に応用できるわけじゃない。例えば、統計的学習理論は実践での関連性が限られてるし、アルゴリズム学習理論はほとんど役に立たない。深層学習ブームよりもずっと古い（transformersよりも絶対に古い）数学的な理論はたくさんあって、実践的な応用というより概念的な視点から見た方が面白い。" userName="cubefox" createdAt="2025-03-22T10:20:57" color="">}}

{{<matomeQuote body="機械学習の実践は今のところ、機械学習の理論をはるかに凌駕してるよね。でも、理論が追いついたとしても、あなたの質問への答えは、データを生成するプロセスの性質に依存する可能性が高いから、経験的に答えなければならないだろうね。理論の価値は、一般的な概念的枠組みを提供することにあると思う。アルゴリズムの漸近理論が、どのアルゴリズムを使うべきかを教えてくれるわけではないけど、大まかな指針を与えてくれるのと同じ。" userName="yomritoyj" createdAt="2025-03-22T10:52:10" color="#ff5733">}}

{{<matomeQuote body="＞あなたの質問への答えは、データを生成するプロセスの性質に依存する可能性が高いから、経験的に答えなければならないだろう。<br>それは当然だと思うし、そうでなければおかしいと思う。MLモデルの予測不可能性の一部(*)は、訓練データが予測不可能であることに起因する。私に欠けているのは、訓練データとタスクがモデルアーキテクチャの特定の決定にどのように影響を与えるかの詳細な説明。だから、「常にこのアーキテクチャやこのニューロン数を使うべき」というような明確な答えを期待してるわけじゃなくて、特定のアーキテクチャがモデルにどんな影響を与えるのかをもっと理解したいんだよね。例えば、ML101コースでは、単層パーセプトロンと「多層」（通常は2層）パーセプトロンの違いを教えるじゃん：線形分離可能性、XOR問題とか。でも、2層と3層のパーセプトロンの違いとか、3層と32層の違いとかについての情報はあんまり見たことないんだよね。同じように、モデルの能力は層の中のニューロンの数、または畳み込み層の場合、カーネルの次元、ストライドの次元などのパラメータによってどう影響を受けるの？transformersについても同じことが言えるよね：埋め込みサイズ、アテンションヘッドの数、連続するtransformer層の数がモデルの能力にどんな影響を与えるの？どうすれば良い値を決定できるの？絶対的な数値が欲しいわけじゃなくて、それらの数値を選ぶ方法についての何らかの理解が欲しいんだよね。（このスレッドには既に素晴らしい回答がいくつかあるけどね）(*一部であって、全部じゃないよ。MLアルゴリズム設計の「文化」にイライラし始めてるんだよね。何か良いアイデアがない時に、ランダムに訓練データをシャッフル/分割したり、重みをランダムに初期化したり、ニューロン/層をランダムにドロップアウトさせたり、勾配降下中にランダムにジャンプしたりとか、追加のランダム性や非決定性のソースを投入するのが好きなんだよね。統計と確率分布だけを気にするなら良いけど、特定の学習設定をデバッグしたり、モデルが特定の挙動を学習した理由を理解したいなら最悪だよね)。" userName="xg15" createdAt="2025-03-22T12:38:31" color="#45d325">}}

{{<matomeQuote body="どのML101コースでも、単層と多層（通常2層）パーセプトロンの違いは教わるよね。線形分離とかXOR問題とかさ。まさにそこがポイントなんだって！ML関連の話題って、線形分離とかXORみたいな単純な問題から始まって、ちょっと数式が出てきて、気がつくとMNISTとかの問題を解く魔法のPythonコードがどこからともなく現れて、その問題しか解けないんだよね。" userName="lucasoshiro" createdAt="2025-03-22T16:03:38" color="">}}

{{<matomeQuote body="初心者だけど、Andrew NgのCourseraコース（特にニューラルネットワーク）で学んだのは、必要な最小限のニューロンと層よりも多く追加しても大丈夫ってこと。（適切な正則化項があれば過学習のリスクはない。）残念ながら最小限のルールはないから試行錯誤が必要だけどね。ネットワークを無計画に拡張すると非効率になるよ。活性化関数は、出力層は問題によってほぼ決まるし、中間層はReLUから始めて、問題に合わせて試行錯誤するのが普通だよ。似たような問題の成功例を参考にするのも大事だね。" userName="grandiego" createdAt="2025-03-21T23:36:16" color="#ff5c5c">}}

{{<matomeQuote body="君はMLの実用的な側に興味があるんだね！それは全然良いことだよ！平均的なMLEにどれくらいの数学と理論が必要なのか疑問なんだ。もちろんある程度は必要だけど、どれくらい？分からないな。でも理論家はもっと多くの数学が必要だよね。SVMみたいなのはVapnikみたいな数学の天才しか発明できなかっただろうし。" userName="joshdavham" createdAt="2025-03-21T21:54:48" color="">}}

{{<matomeQuote body="ベテランプロによると、線形代数と微分積分は最低限必要らしいよ。授業がそれを前提に設計されてるからね。でも統計と確率も役立つと思うな。古い統計的な手法でも多くの問題を解決できるし。" userName="nickpsecurity" createdAt="2025-03-22T03:34:34" color="#ff5733">}}

{{<matomeQuote body="＞でも理論家はもっと多くの数学が必要だ<br>なるほどね…。プロになりたいわけじゃなくて、簡単なタスクのためにモデルを訓練して、プロセスを理解したいだけなんだ。" userName="lucasoshiro" createdAt="2025-03-22T17:31:29" color="">}}

{{<matomeQuote body="Hala Nelsonの“Essential Math for AI”は、AIに必要な数学の概要を幅広く提供してるよ。細かいところに迷いやすいAI/MLの学習において、全体像を提供してくれるから知識の理解や吸収が促進されるんだ。" userName="rramadass" createdAt="2025-03-23T17:45:00" color="">}}

{{<matomeQuote body="予算P個のパラメータがあるとき、多層パーセプトロンの隠れ層は何層が良いのか、それぞれの層のサイズは？前の層（入力層を含む）よりも大きい隠れ層を持つのは良いことなのか？それとも無駄なのか？<br>非線形性が高いほど、隠れ層を多くしたいってのが経験則かな。入力ベクトルの情報を圧縮の観点から考えると、情報の損失なしにどれだけ圧縮できるかは、システムのentropyに依存するよね。ビデオや画像は、情報損失なしに約50%圧縮できる。テキストはもっと圧縮できるけど、エンコードが非効率なのと、entropyが低いから。だから、前の層、特に入力層のサイズを10x-20x以上小さくしない方が良いと思う。" userName="SJC_Hacker" createdAt="2025-03-22T18:35:37" color="#ff5c5c">}}

{{<matomeQuote body="理論をどこまで深く学んだの？どんな教材を使った？数学のバックグラウンドはどれくらい？理論は数学が必要で、学部レベルの数学では足りないことが多いよ。微分幾何学、距離空間論、集合論、抽象代数学、高次元統計とか。でも理論は役に立つし、直感を養うのに役立つよ。数学的な操作が何をしているのかを深く理解することも重要だよ。Whitneyの埋め込み定理を見てみて。パラメータの最小数について直感を得るのに役立つよ。活性化関数については、Geluの視覚化ビデオがお勧めだよ。高次元になると変なことが起こるから、視覚的な直感は役に立たなくなるけどね。<br>活性化関数はネットワークに非線形性を提供するんだ。線形層はアフィン変換しかできないからね。MLの学習は大変だけど、数学を掘り下げることで得られる洞察はたくさんあるよ。" userName="godelski" createdAt="2025-03-22T02:06:36" color="#45d325">}}

{{<matomeQuote body="数学のバックグラウンドが強い人と、学部レベルの数学しか知らないエンジニアとでは、どんな貢献ができるんだろう？曖昧な質問だけど、理論と実践について考えてるんだよね。DLで役立つアプリケーションを作ったり、論文を発表してる人たちを見てると、分析力、エンジニアリングスキル、実用性が大事だなって思うんだ。数学教育は、広い視野を持つための安全な方法だよね。" userName="Valk3_" createdAt="2025-03-22T15:58:16" color="">}}

{{<matomeQuote body="Deepseekのチームは、エンジニアリングスキルと数学的バックグラウンドのどちらが重要だったんだろうね。" userName="la_fayette" createdAt="2025-03-22T19:05:08" color="">}}

{{< details summary="もっとコメントを表示（1）">}}
{{<matomeQuote body="魔法を生み出すのは組み合わせなんだよね。数学、プログラミング、コンピュータアーキテクチャを学ぶべきだって強く思ってる。アルゴリズムは全部影響を受けるし（だからチームが最強！）。<br>研究者だけどまだ駆け出し。ロックじゃないけど、引用数とかH-indexとか考えたら平均以上かな。GPU不足で、モデルを効率化してリソースを減らす研究がメイン。<br>数学を一日中計算しろってわけじゃないよ。でも、直感を得て、現実の問題に応用するには数学の学習は必要。<br>実例を話すね。去年大手企業でインターンしてて、フレームワーク学習中に小さいモデルを触ってたんだ（大きいのは未公開）。トレーニングしてたらすぐ飽和して、データ見たら一般化に問題があるって気づいた。モデル再学習に1週間くれって頼んだ（V100が1つだけ）。1週間後には有望な結果が出たけど、社内テストセットの精度には負けてた。でも一般化の原因とか、データ取得のバイアスは理解してたから自信があった。ボスは納得してなくて、他のテストセットと顧客データ要求したら、渋々くれた。テストしたらパフォーマンスが3倍になったんだ。数億パラメータのTransformerモデルとほぼ同じレベル。ちっちゃいResNetモデルが、学習に数週間かかるモデルに勝ったんだよ。ボスも、理論嫌いな上司もビックリ。やり方を聞くメールまで来たよ。全部Transformerモデルにもっと効くから実装すべきって言ったんだけど、何も起きなかった。顧客にリリースもされず、トレーニングアルゴリズムへの追加もされなかった（全部オプションなのに）。<br>似たような経験が多いかな。小規模で良い結果出しても「スケールするの？」って言われて、必要な計算資源を与えてもらえない。同じような人が多いみたい。巨大モデルと競争するには、やっぱり計算資源が必要なんだよね。パラメータ10分の1とか100分の1でも同じパフォーマンス出せるかもしれないけど、1000分の1は難しい。小さい規模から始めてスケールしていくっていう科学の成長経路がなくなってるのが心配。GPU貧乏が研究に貢献できないのは科学というより門番の問題だと思う。でも、このスレッドの他のコメント見れば当然だってわかるでしょ。「誰も知らない」って、まるで「誰も知りえないから、試すな」って言ってるみたい。短絡的だよね。今日別の記事でも同じこと言ってる人がいるけどね（僕のコメントもあるよ）。https://news.ycombinator.com/item?id=43447616" userName="godelski" createdAt="2025-03-23T00:35:05" color="#45d325">}}

{{<matomeQuote body="有益なコメントありがとう！「小さく始めて、研究して、スケールする」ってパターンは最近見過ごされがちだよね。今後の成功を祈ってるよ！" userName="la_fayette" createdAt="2025-03-23T10:27:12" color="#ff5733">}}

{{<matomeQuote body="そりゃ金なかったら大きく始められないよね（笑）。ありがとう！マシンがもっと賢くなって、色んなアイデアが試されるようになってほしいな。AGIが実現するまでは、どの方法が正解かなんて言えないと思う。" userName="godelski" createdAt="2025-03-23T20:22:30" color="">}}

{{<matomeQuote body="時間を割いてくれてありがとう！コメントをお気に入りに追加したよ！😊" userName="lucasoshiro" createdAt="2025-03-22T20:50:06" color="">}}

{{<matomeQuote body="アートの側面が強いってことは、「正しい」値なんてないってことだよね。あるのはヒューリスティクスと試行錯誤だけ。" userName="sota_pop" createdAt="2025-03-22T14:28:55" color="">}}

{{<matomeQuote body="いつも学習で苦労するのはそこなんだよね。壁に物を投げて、何がくっつくか見てるみたい。<br>数学は難しくないんだけど、「なぜこれがそれより優れてるのか」の説明がいつも曖昧。もしくは「良くはないけど計算が速い」ってことが多い。" userName="physicsguy" createdAt="2025-03-23T07:18:19" color="">}}

{{<matomeQuote body="決定木とかXGBoostみたいな、説明可能な機械学習モデルもあるよ。<br>ニューラルネットワーク、特にLLMは説明が難しい。" userName="informal007" createdAt="2025-03-22T10:16:17" color="">}}

{{<matomeQuote body="情報ダンプみたいになっちゃった…<br>他の人も言ってるように、一つずつ試して何が効くか見つけるのが基本だよね。でも、いくつか知ってれば探索範囲をかなり絞れる。理論とか、大規模な実験結果とか。<br>たとえば、Googleとかが特定の構成を広く展開したら、他の人もそれを使う。NLPでAdamのこの設定が良いって大規模実験で示されたら、みんなそれを使う。alpha sigmoid(beta x)の形が最高のアクティベーション関数だって大規模な実験があった。Sigmoid、tanh、Geluは全部この形。ReLUは、普遍近似定理がないのに使われてるんだよ！標準的な定理はシグモイドが使われてる時だけ有効なのに。誰も気にしない、なぜなら実際に動くから。<br>通常、ニューラルネットワークみたいな一般的なモデル構造に対して理論的な結果を得るのは難しい。理論的な結果は制約が少なすぎて、そこから他の論理的な文を生み出すのが難しい。だから、アーキテクチャのサブセットに対する理論的な結果を見ることになる。制約が与えられてるから、それらを組み合わせて定理や証明ができる。そして、それがより一般的なネットワークでもうまく機能することを経験的に見つける。Dropoutが良い例。理論的な根拠に基づかせるために、線形モデルでは入力にノイズを追加するのと同じだって示された。でも、複雑なアーキテクチャでは証明がない。実際にはうまく機能する。でも、確信がないから、ハイパーパラメータ検索に入れる。<br>多くの正則化手法には良い理論的根拠がある。SGDに対するL2正則化は、重要じゃない特徴を制限して重要な特徴はあまり正則化しないことを厳密に示すことができる。<br>活性化関数については、ほとんどの人が実験結果を頼りにしてる。<br>普遍近似定理は、単一のレイヤーでもどんな関数でも表現できるって言ってる。でも、これらの単層ネットワークをトレーニングするのは難しい。ネットワークを深くすると、効率が大幅に向上する。特に、特定の関数クラスでは、指数関数的な利点がある（Eldan and Shamir 2016）。Information Bottleneck Theoryって理論は、複数のレイヤーが積み重なって、データの分布の1つレベルの「階層」を明らかにしてると説明しようとしてる。StyleNetで見られるけど、理論は弱いと思う。<br>勾配消失問題を回避するためにアーキテクチャに多くの調整が行われてる。ランダム行列理論から生まれた理論もあるけど、詳しくない。<br>モデルの複雑さの古いVC次元理論もあるけど、ニューラルネットワークにはうまく適用できないと思う。" userName="porridgeraisin" createdAt="2025-03-22T06:53:55" color="#ff5733">}}

{{<matomeQuote body="＞all of the above<br>NFLは、任意のデータに対しては効果がないって言ってる。結果は全部、データに対する仮定（不連続性が少ないとか、十分なサンプリングとか）に合わせて調整される。<br>＞ニューロン数、レイヤー数<br>スケーリング則は、今のところ経験的に導き出されてる。そこから目標（たとえば、最大Xで精度を最大化）を選んで、最適なパラメータセットを逆算する。非常に限られたドメインとか強い仮定がない限り、それ以上のものは見たことない。<br>＞活性化関数<br>任意のデータには関係ないとか、パラメータは経験的に導き出す必要があるってことは全部当てはまる。重要な帰納的バイアスは、モデルのすべての重みがほぼ同じくらい重要であるべきだってこと。活性化関数を選ぶ方法は他にもあるけど、特に専門分野では。深層ネットワークを設計するときに最も重要なことの一つは、バックプロパゲーションの各レベルでの情報の大きさを制御すること。活性化関数（とその周辺のインフラ）がその問題をうまく処理できるなら、十分だと思う。<br>＞ニューラルネットワークか他の手法<br>ほとんどの問題で、ニューラルネットワーク以外を使った方が良い（CatBoostとか）。なぜそうなるのか、良い直感はない。両方試して。それが検証データセットの目的。<br>＞実践とのつながり<br>この記事は、私が個人的にやることとはあまり関係ない。共感する人もいると思う。PyTorchとかJAXとかが役に立たなくなって、自分で実装する必要が出てきたら、実装する理論を深く理解する必要がある。大規模なフレームワークを扱ったり、その制限を回避したりする場合も、実装するものを深く理解する必要がある。<br>たとえば、動的割り当てとか仮想関数とかが扱いにくい世界で、最新のMLツールを全部使いたいとする。小さなニューラルネットワークでマウスドライバーのファントムタッチパッドイベントに対するヒューリスティックを圧倒できるけど、PyTorchを使うとラップトップがスペースヒーターになる。<br>埋め込みデバイスだけじゃなくて、型破りなことをする必要があるかもしれない。ライブラリの作成者が提供するもの以上の要件が出てきたら、自分で全部やった方が良い場合が多い。そのためには、しっかりした理論的基盤が必要。" userName="hansvm" createdAt="2025-03-21T22:20:46" color="#38d3d3">}}

{{<matomeQuote body="すごくいいね！Tom Yehの「AI By Hand」の練習を思い出す。[0]<br>[0] https://www.byhand.ai/" userName="simojo" createdAt="2025-03-21T21:01:02" color="">}}

{{<matomeQuote body="まさにこういうのが欲しかったんだよね。マジ感謝。OPの論文も良いけど、既に知ってる人向けって感じがする。内容理解してる人には最高の教材だね。" userName="Sysreq2" createdAt="2025-03-21T21:32:52" color="#ff5c5c">}}

{{<matomeQuote body="いいね！ただ、質問のすぐ後に答えがあるのが難点かな。自分で考える前に答え見ちゃいそうになる。" userName="S4M" createdAt="2025-03-21T21:09:42" color="">}}

{{<matomeQuote body="これマジでいいね！機械学習で働いてるんだけど、数学（特に線形代数と行列/テンソル演算）の基礎に自信がなくて、いまだにImposter Syndromeを感じてるんだよね。deep learningの基礎スキルを重点的に学べる問題集とか他に何か知ってる人いる？手を動かしながら学ぶのが一番身につくんだよね（いろんな先生の視点から学べると尚良し）。" userName="plants" createdAt="2025-03-21T21:30:50" color="#ff5733">}}

{{<matomeQuote body="今、”役に立つ”MLを構築してる現役のMLエンジニアの中で、どれくらいの人がこの問題解けるんだろう？解けるべきなのかな？" userName="antipaul" createdAt="2025-03-21T20:41:14" color="">}}

{{<matomeQuote body="いや、解ける必要はないと思うよ。これらの演習は、新しいアルゴリズムを開発したり、低レベルの最適化をするために必要な数学的な成熟度を養うのに役立つ。既存のMLアルゴリズムをトレーニングしてデプロイするだけなら必要ないね。" userName="jerrygenser" createdAt="2025-03-21T20:47:49" color="#ff33a1">}}

{{<matomeQuote body="朗報だよー。最先端技術を拡張することに興味がなくて、単にAPIを呼び出したいだけなら、MLを深く学ぶ必要はないよ。" userName="psyklic" createdAt="2025-03-21T21:33:26" color="">}}

{{<matomeQuote body="ML practitionerとかbuildingとかMLの定義によるんじゃない？最適化のセクションを見て。この分野に精通している人は、loss関数やパラメータの更新方法、何が問題になるかを頭の中で繰り返し考えることができる。" userName="danielmarkbruce" createdAt="2025-03-21T20:51:55" color="#ff33a1">}}

{{<matomeQuote body="勉強を面倒くさいと思って、仕事に必要な最低限のことだけを学びたい人もいる。一方、洞察力があって面白いと思って、問題に取り組んだり資料を読んだりするのを楽しむ人もいる。どちらのアプローチも貢献できるし、成功につながるけど、その方法は異なるよね。" userName="grandempire" createdAt="2025-03-22T05:40:31" color="#45d325">}}

{{<matomeQuote body="同じことが気になる。MLエンジニアとして数年働いて、この分野の学位もいくつか持ってるんだけど、ドキュメントをざっと見た感じ、ほとんど全部知ってるけど、コンテキストなしで聞かれたら、これらのトピックの多くを思い出せないと思う。他の人の一般的な記憶レベルってどんな感じなんだろう？俺は実は数学が苦手なニセモノなのかな？それとも、定期的に使わないと忘れてしまうものなのかな？" userName="biotechbio" createdAt="2025-03-21T21:36:42" color="">}}

{{<matomeQuote body="解答付きで素晴らしいね。共有してくれてありがとう！もしあれば、他のトピックについても、こういうペンと紙の演習問題に興味があるな。" userName="kingkongjaffa" createdAt="2025-03-21T20:45:59" color="#45d325">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（2）">}}
{{<matomeQuote body="他にどんなトピックのこと言ってるのかわかんないけど、「1000 exercises in probability」でしばらくは暇つぶせるんじゃない？(PDFはオンラインで手に入るよ)。他の数学系のなぞなぞなら、「The colossal book of short puzzles and problems」とか「The art and craft of problem solving」をチェックしてみて。" userName="blackbear_" createdAt="2025-03-21T20:56:45" color="">}}

{{<matomeQuote body="当時議論されてたのはこれだよ:<br>Pen and paper exercises in machine learning (2021)<br>・https://news.ycombinator.com/item?id=31913057<br>・2022年6月(55件のコメント)" userName="dang" createdAt="2025-03-21T21:50:40" color="">}}

{{<matomeQuote body="数学者がいつも線形代数と行列理論をMLにこっそり入れようとするのが面白いよね。何も知らなかったら、まるで学者がLLMを発明して、相談されるべき専門家だと思っちゃうかも。むしろ学者や理論家がMLを遅らせて、計算技術が彼らにとってローブロウすぎたせいで、何世代もの大学院生にこの例みたいな記号的な証明をやらせたんだよ。" userName="FilosofumRex" createdAt="2025-03-22T00:29:02" color="">}}

{{<matomeQuote body="MLに貢献したいなら、既存の技術を使うだけじゃなくて、数学のスキルが一番重要だよ。線形代数に堪能じゃない人で貢献してる人を知ってる？それに、なんで分野全体を“LLM”ってまとめるのさ？" userName="grandempire" createdAt="2025-03-22T05:34:38" color="#785bff">}}

{{<matomeQuote body="本当に数学のスキルってそんなに必要なの？深層学習のほとんどは、理解するために深い数学の知識はいらないと思うけど。Backprop、convolution、attention、recurrent networks、skip connections、GANs、RL、GNNsとかは、簡単な微積分と線形代数だけで理解できるじゃん。モデルの理論的な動機は数学寄りなことが多いのはわかるけど、動機が常に数学的である必要はないと思うんだ。" userName="Matthyze" createdAt="2025-03-22T07:21:33" color="">}}

{{<matomeQuote body="理論を全部理解しなくても既存の技術を使えるってのは否定しないけど、新しい技術を見つけることはできないと思うよ。例えば、線形時不変システムに詳しくなかったら、convolutionカーネルを最適化するのがいいアイデアだってどうやってわかるのさ？" userName="grandempire" createdAt="2025-03-22T14:43:38" color="#38d3d3">}}

{{<matomeQuote body="CNNは視覚処理のシフト/空間不変性という概念からすごく自然に導かれると思うけどな。それって数学的な理解は必要ないでしょ。" userName="Matthyze" createdAt="2025-03-22T15:37:27" color="">}}

{{<matomeQuote body="画像処理とシフト不変性はDSPから来てるんだよ。" userName="grandempire" createdAt="2025-03-22T19:09:31" color="">}}

{{<matomeQuote body="数学を勉強してないMLEは、その重要性を過小評価したがるよね。測度論的確率はいらないかもしれないけど、計算をより良く構成するために線形代数の理解は必要だよ。attentionでやる正規化を覚えてる？あれには数学的な正当性があるんだ。だからやっぱり学者はLLMの構築に役割を果たしたんだよ。コンピュータサイエンティストは、自分たちが分野全体を発明したみたいに振る舞いたがるけどね。実際には、平均的なOS、コンパイラ、ネットワークの授業は、コアMLとは関係ないけど。もちろんそれらも重要だし、こんな皮肉は意味ないけどね。" userName="thecleaner" createdAt="2025-03-22T09:26:28" color="#ff5733">}}

{{<matomeQuote body="僕の言いたいことを強く受け止めすぎちゃったかも。もちろん数学はすごく役に立つし、純粋に数学的な貢献もあるよ。ただ、イノベーションのために、言われてるほど必須条件だとは思わないってこと。" userName="Matthyze" createdAt="2025-03-22T15:39:14" color="#ff33a1">}}

{{<matomeQuote body="離散数学なしに、マジなCSとかちゃんとしたエンジニアリングはありえないっしょ。それに、Shannonがいなかったら通信もコンピュータサイエンスも存在しないし。マジレスすると、Lispはラムダ計算の形式化と実装で、それって数学者の論文から始まったんだぜ？HAKMEMも数学スキルなしじゃマジで読めないから。" userName="anthk" createdAt="2025-03-22T09:59:39" color="">}}

{{<matomeQuote body="面白い視点だね。もしおすすめのリソースがあったら教えてほしいんだけど、”記号を使った証明”より”計算テクニック”を優先するようなやつ。" userName="Abishek_Muthian" createdAt="2025-03-22T04:06:44" color="#785bff">}}

{{<matomeQuote body="興味深いね。何か例を教えてくれない？" userName="nophunphil" createdAt="2025-03-22T03:50:49" color="">}}

{{<matomeQuote body="arxivって研究レベルの論文のためのものじゃないの？これがホストされてるって意外だわ。" userName="BeetleB" createdAt="2025-03-22T04:23:53" color="">}}

{{<matomeQuote body="最高。" userName="axpy906" createdAt="2025-03-21T20:47:13" color="">}}

{{<matomeQuote body="誰かがこれをadaptive Khan Academyみたいなアプリにしてくれたらマジですごい。" userName="imranq" createdAt="2025-03-21T22:30:29" color="#ff33a1">}}

{{<matomeQuote body="ちょっと気になるんだけど、あなたとか他の人にとって、そういうアプリが魅力的になるのはどんな点？このドキュメントの内容を学ぶためだけじゃなくて、もっと広く、今興味を持ってたり勉強してることを学んで記憶するのを助けてくれるアプリだったら？<br>特に機械学習の問題に関して、反復学習（adaptive Khan Academyみたいなアプリみたいな）とか、使いやすい形式のものがあれば、すごく助かる人がたくさんいると思うんだよね。YouTubeの動画とか本とか、他のリソースを使って学習するよりも、それを使いたいと思えるような機能って何だろう。" userName="mathandsurf" createdAt="2025-03-21T23:53:28" color="#38d3d3">}}

{{<matomeQuote body="すでに似たようなリソースはあるよ。<br>leetgpu.com<br>https://github.com/srush/GPU-Puzzles<br>俺にとっては進捗を感じられるのが大事で、チェスみたいにELOスコアがあったり、Duolingoみたいにロードマップがあるのが良い。もしレベル分けされてたら、自分の能力に自信が持てるようになるかも。今はレベルが学士、修士、博士みたいに大雑把で費用も高いし。" userName="imranq" createdAt="2025-03-22T01:00:25" color="#ff5c5c">}}


{{< /details >}}


[記事一覧へ]({{% ref "/posts/" %}})
