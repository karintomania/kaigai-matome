+++
date = '2025-06-03T00:00:00'
months = '2025/06'
draft = false
title = 'ディープラーニングはチヤホヤされるが その実態を深掘りする地味な検証は無視される'
tags = ["AI", "機械学習", "ディープラーニング", "検証", "過学習"]
featureimage = 'thumbnails/light-orange3.jpg'
+++

> ディープラーニングはチヤホヤされるが その実態を深掘りする地味な検証は無視される

引用元：[https://news.ycombinator.com/item?id=44174965](https://news.ycombinator.com/item?id=44174965)




{{<matomeQuote body="BERTを酵素データに使って失敗した経験あるよ。<br>評価では良かったのに、実運用では全然ダメだったんだ。よくある過学習のパターンだね。<br>正直、単純な分類ならSVMかlogisticを選ぶかな。Transformerはクールだけど、データがすごく綺麗じゃないと自信満々にハルシネーションするんだ。<br>最近は、大規模モデルのembeddingだけ盗んで、簡単な分類器を乗せるのが一番うまくいくし、速いし、手間がかからない。<br>この記事に感謝！また変なfine-tuneする前に現実を思い知らされたよ。" userName="b0a04gl" createdAt="2025/06/04 07:09:49" color="#ff5733">}}




{{<matomeQuote body="＞ GPTがでたらめな複数選択テストでもなんか答えて自信満々なの<br>もし大学生のクラスに適当な選択肢しかないテストを受けさせたら、ほとんど全員が何か答えると思うよ。<br>GPTとか他のLLMはテストを拒否するべきなのかな？<br>経験上、全然正しくない選択肢でも一番近い答えを選んじゃうんだ。" userName="teruakohatu" createdAt="2025/06/04 11:04:55" color="">}}




{{<matomeQuote body="＞ 最近は、大規模モデルのembeddingだけ盗んで、簡単な分類器を乗せるのが一番うまくいくし、速いし、手間がかからない。<br>君は知ってるかもしれないけど、多くの人は知らないんだ。これは広く「転移学習」として知られているよ。" userName="stevenae" createdAt="2025/06/04 12:24:13" color="">}}




{{<matomeQuote body="Transformerはテストセットでは満点取れるけど、現実世界に出たとたんダメになるんだ。<br>僕も「うわ、92% accuracy！」って喜んだ後で、自分のデータセットの変なパターンにだけ対応するpattern-matcherを作っただけだと気づいた経験があるよ。" userName="ErigmolCt" createdAt="2025/06/04 07:26:11" color="#785bff">}}




{{<matomeQuote body="正直言って、accuracy/performance metricsが良すぎる場合は、何か問題が起きてるってほぼ確実な兆候だよ。<br>出典：苦い、苦い経験談。random forestを使ってplacebo effectを完璧に予測しちゃったことがあるんだ（たまたまtrain/test splitが良かっただけ）。<br>その時はもうacademiaを辞めてたけど、もし雇用を維持するためにhigh impact paperが必要だったら、もっと深掘りしたかなってよく思うよ。" userName="disgruntledphd2" createdAt="2025/06/04 08:05:32" color="#ff5c5c">}}




{{<matomeQuote body="古典的な分類器みたいな簡単なものに適用してもそうかな？<br>LLMのconversational/inferenceの側面にばかり焦点を当てるのは間違ってると思うんだ。<br>僕が見るに、本当の「magic」はmodelそのものに隠れてるよ。<br>それは効果的な「understanding」の計算表現なんだ。latent spaceの構造にはまだ実現されていない価値がたくさん隠れてる気がする。<br>もっと時間をかけてそれを研究して、探求するための多様でhands-onなツールをもっと作って、あらゆる種類のinsightsをそこから掘り出す必要があるよ。" userName="TeMPOraL" createdAt="2025/06/04 12:52:23" color="#38d3d3">}}




{{<matomeQuote body="これはすごく一般的だと思うよ。<br>ある時、良い結果が出てる（journalsに掲載された）いくつかのstudiesを分析して、それぞれの問題点をshowingするpaperを発表しようと思ったんだけど、途中でgive upしたんだ。<br>元のauthorsをunhappyにさせるだけで、everybody elseは気にしないだろうなって思ったんだ。" userName="dvfjsdhgfv" createdAt="2025/06/04 14:34:56" color="#ff5c5c">}}




{{<matomeQuote body="Ironically、このcommentはTransformer（ChatGPT to be specific）がgeneratedしたみたいに読めるね。" userName="ActivePattern" createdAt="2025/06/04 14:32:13" color="">}}




{{<matomeQuote body="＞ 最近は、大規模モデルのembeddingだけ盗んで、簡単な分類器を乗せるのが一番うまくいくし、速いし、手間がかからない。<br>Sureだけど、これはまだindirectly using transformers。" userName="sebzim4500" createdAt="2025/06/04 11:12:14" color="">}}




{{<matomeQuote body="そうそう、LLMは自分で最終的な答えを出すんじゃなくて、獲得した理解を使って、もっと信頼できるツールをガイドするべきなんだよ。この分野だと、結構幻覚（ハルシネーション）しやすいからね。" userName="TeMPOraL" createdAt="2025/06/04 12:55:16" color="">}}




{{<matomeQuote body="いや、拒否すべきだよ。人間は知らないことは知らないって認めて進歩してきたんだからさ。「責任ある知識」の範囲外ってのをLLMだけ免除するのは無理だよ。無知なLLMを信じるってことは、心臓外科医に股関節の手術任せるみたいなもんだよ。" userName="6stringmerc" createdAt="2025/06/04 12:53:32" color="#785bff">}}




{{<matomeQuote body="同意。これって、LLMの内部でやってる表現学習を単に利用してるだけじゃないの？" userName="lamename" createdAt="2025/06/04 13:09:39" color="">}}




{{<matomeQuote body="ああ、そうそう、それが相互運用性のゲームだよ。ただモデルサイズ大きくして上手くいくって祈るだけじゃなくてさ。みんなスケールに興奮してるけど、「つなぎ」について考えてる人少ないんだよね。Anthropicは早くからそれに気づいてた。彼らの相互運用チームはマジでヤバい頭良い人たちで、何人か個人的に知ってるよ。忖度一切なし、純粋なシグナルだけ。彼らがどう考えてるか知りたいなら、まずここから読むといいよ → https://www.anthropic.com/research/mapping-mind-language-mod...。これ、ただのAIブログじゃないんだ。実際のシステム的な思考が裏にあるんだよ。" userName="b0a04gl" createdAt="2025/06/04 15:28:50" color="#38d3d3">}}




{{<matomeQuote body="これと関連するコメントについてね。そうだよ。基本的に、あるモデルの出力を別のモデルの入力として使うのって、転移学習なんだよね。" userName="stevenae" createdAt="2025/06/04 15:27:36" color="">}}




{{<matomeQuote body="多くのアプリは「--」をem dashに自動で変えるし、スマホとかタブレットのキーボードもそうだよ。俺、em dash好きでさ、「--」を確実にem dashにするプラグイン入れようか迷ったけどやめたんだ。もし入れてたらこのコメントにも使ってただろうね ;) スペルチェックのブラウザプラグインでもそういうの見たことあるし。それに、HNとやり取りするのにサードパーティのツール使う人もいて、それが連続するダッシュをem dashに自動変換したりするんだ。俺がたまにAI使ってる場所でも、em dashはそんなに普通じゃないよ。だから個人的には「em dash」がAIが書いた証拠になるとは思わないな。<br>でも、元の投稿者のコメントについては、たぶん君の言う通りだと思う。書き方が…なんか変なんだよね。小説みたいな、しかもストーリーの中にストーリーがあるのを強調するような書き方で、HNコメントの内容を書いてる感じ。LLMに箇条書きを「物語風に話して」って頼んだらこうなるかも。<br>でもさ、もしその物語が人間が作ったなら、それでもいいんじゃない？簡潔な箇条書きじゃなかったのがムカつくの？<br>それは置いといて、投稿者がすごく本を読んでたり文章書くのが好きで、もしかしたら作家とか趣味で書いてる人で、そういう書き方を身につけた可能性も十分あるよ。" userName="dathinab" createdAt="2025/06/04 17:25:11" color="">}}




{{<matomeQuote body="どんなデータでこの検証を実行したの？" userName="saagarjha" createdAt="2025/06/04 08:12:15" color="">}}




{{<matomeQuote body="査読って報われない仕事だけど、科学はそうやって進むんだ。<br>反論のためのarXivみたいな場所があればいいのにね。" userName="tough" createdAt="2025/06/04 15:04:41" color="#ff5c5c">}}




{{<matomeQuote body="「それはすごく一般的だと思うよ」<br>うん、俺もそう思う。数年前に「コンピュータプログラミングは数学スキルより言語スキルに関係してる」って論文が出回ったんだけど、データをダウンロードして見たら内容がひどかったんだ。たった30件のデータに多項式回帰とか、ゴミみたいなやり方だった。<br>PhD中の経験からしても、こういうのはすごくよくあるよ。科学のインセンティブ構造を考えれば当然だけどね。" userName="disgruntledphd2" createdAt="2025/06/04 15:41:29" color="#785bff">}}




{{<matomeQuote body="実用レベルで言うと、LLMに変な方向に行きそうになったら「回避」させる機能を持たせると、トラブルが減ってすごく助かるよ。特に推論を制約してる場合ね。もちろん誤検知とかもあるけど、「Something else」と言わせて説明させる選択肢があると、変な方向に送っちゃった時にすごく楽になる。" userName="ijk" createdAt="2025/06/04 16:07:38" color="">}}




{{<matomeQuote body="拒否じゃなくて、「どれも正しくないみたい」って言えばいいのに。だって、専門家によればAGIのゲーマー研究者AIまであと2日なんでしょ？ 少なくともこのくらいの振る舞いは期待するよね。<br>1: AIのハイプで金儲けしてる連中" userName="shafyy" createdAt="2025/06/04 11:26:17" color="">}}




{{<matomeQuote body="この検証、誰かやってくれないかな。たぶん学部生なら85〜90%は全部埋めるだろうけど、全員じゃないと思うんだよね。だって、自分が知ってることだけを信じちゃう人とかもいるからさ。" userName="jeremyjh" createdAt="2025/06/04 11:27:31" color="">}}




{{<matomeQuote body="AIに研究やらせる前に、まず研究の再現をさせようよ。deep learningの論文渡して、その実装を作らせてみるとかね。それができないうちは、新しいアイデアなんて生まれないだろうな。" userName="amelius" createdAt="2025/06/03 22:07:06" color="#38d3d3">}}




{{<matomeQuote body="再現性こそ基本中の基本でしょ。モデルが既存の研究をしっかり読んで理解して、正確に実装できるようになるまでは、「AI scientist」なんて言われても、ほとんどブランディングにしか聞こえないな。" userName="ErigmolCt" createdAt="2025/06/04 07:28:50" color="#785bff">}}




{{<matomeQuote body="OpenAIがこの件についてベンチマーク作ったよ。ここ見てみて。https://openai.com/index/paperbench/" userName="slewis" createdAt="2025/06/04 00:04:35" color="#ff33a1">}}




{{<matomeQuote body="とはいえ、まだデータの汚染問題はあるけどね。" userName="suddenlybananas" createdAt="2025/06/04 06:38:29" color="">}}




{{<matomeQuote body="まだLLMはそれに勝ててないから、スタートとしては十分なんじゃない？" userName="Szpadel" createdAt="2025/06/04 13:35:59" color="">}}




{{<matomeQuote body="LLMにはしっかりした監査証跡が要るし、論文がデータセットに全然入ってないことを保証しないとね。学術不正って珍しいけどゼロじゃないし、LLMってすぐにデータ捏造したり嘘ついたりするからさ。" userName="patagurbon" createdAt="2025/06/03 23:23:06" color="#785bff">}}




{{<matomeQuote body="既知の研究も新しい研究も再現できると思うな。LLMも人も学習は同じで、まず完成例を覚えて再現し、次に結果は分かってるけど自分で途中を考える問題を解く訓練をするんだ。最終的には、答えを知らない問題を解いて評価してもらうって段階だね。既存の論文を真似るだけでも、初期にはすごく価値があるよ。" userName="TeMPOraL" createdAt="2025/06/03 23:44:18" color="#ff5733">}}




{{<matomeQuote body="AIには結果責任がないからでしょ。人間は恥とか痛みとか罰を理解するけどね。AIモデルがそういう条件付き推論をできるようになるまでは、能力も信頼性もめちゃくちゃ過大評価されてると思うな。" userName="6stringmerc" createdAt="2025/06/04 12:55:32" color="">}}




{{<matomeQuote body="AIが研究レベルの科学を生み出せるか検証するなら、「論文の最初だけ与えて、AIに続きを書かせる」って言うのかと思ったよ。それができないうちは、新しいアイデアなんて期待できないな。" userName="ojosilva" createdAt="2025/06/03 22:49:21" color="">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="大学でさ、論文の図だけ渡されて、それに合う論文を書く課題があったんだ。データから始める論文執筆みたいな。図書館で元の論文見つけたら、クラスのみんなも同じことしてて、先生にバレてちょっと大変だったな。今なら電子検索ですぐだけどね。" userName="DrScientist" createdAt="2025/06/04 11:30:14" color="">}}




{{<matomeQuote body="実験データも必要だろうし、ちょっとした実験をやって、ダメなアイデアは捨てられる能力も要ると思うな。" userName="bee_rider" createdAt="2025/06/04 00:29:53" color="">}}




{{<matomeQuote body="例えば、AIに深層学習の論文を渡して実装を作らせるとか、実験観察の統計データから元の生データを再現させるとか、そういうのはどう？" userName="tbrownaw" createdAt="2025/06/04 00:13:31" color="#ff5733">}}




{{<matomeQuote body="AIに実験させるってこと？ それは面白いかもしれないけど、コンピューター上でできる実験に限られそうね。" userName="bee_rider" createdAt="2025/06/04 00:27:48" color="">}}




{{<matomeQuote body="再現性のチェックにAIが役立つってアイデア、すごくいいね！ 人間が見直す必要はあるけどさ。LLMは査読プロセスや文献レビューでも役立ちそう。" userName="YossarianFrPrez" createdAt="2025/06/03 22:40:55" color="#ff5733">}}




{{<matomeQuote body="AI研究コミュニティでは再現性はそんなに問題じゃなかったんだよ。みんな論文とかコードとかデータとか共有してたから進歩が早かった。ChatGPT以降、大企業は共有しなくなったけど、アカデミアではまだやってるよ。" userName="darkoob12" createdAt="2025/06/04 07:55:05" color="#45d325">}}




{{<matomeQuote body="AIに再現させるなら、AI論文じゃなく、ミリカン油滴実験とかTaylor-Coutte流れとか、AIが知らない有名な実験結果を再現できるか見てみたい。訓練データを制限して、当時の知識だけで同じ発見ができるか、って実験が面白いと思う。" userName="mnky9800n" createdAt="2025/06/04 08:16:30" color="#ff5c5c">}}




{{<matomeQuote body="多くの人が自分の論文が再現できないのを知ってるから、再現性チェックは色々反発されそう。政治的な側面が大きいね。でもSci-Hub以来の科学への貢献になると思う。人間による監督は少なくとも最初は必要だよ。" userName="raxxorraxor" createdAt="2025/06/04 10:59:02" color="#785bff">}}




{{<matomeQuote body="論文に参考実装が付いてくるのが普通にならないのはなんでだろう？ 簡単なpythonスクリプトでもいいのにね。" userName="thrance" createdAt="2025/06/04 00:38:18" color="">}}




{{<matomeQuote body="論文に実装が付くこともあるけど、分野によるかな。CSとかは比較的多い。やらない主な理由はインセンティブ。大学や政府は論文の引用数とかで評価するから、研究者はそれを最大化するために余計なこと（実装公開とか）はやらないんだ。欠陥が見つかるリスクもあるしね。企業も結局は同じで、何かを最適化してるだけだよ。" userName="mike_hearn" createdAt="2025/06/04 07:35:13" color="#ff5c5c">}}




{{<matomeQuote body="LLMが新しいアイデアを生み出すのがすごいって言われるけど<br>むしろそれって、みんなが止めようとしてる、今のLLMの最も有名な特性じゃないか。" userName="thaumasiotes" createdAt="2025/06/04 03:05:13" color="#38d3d3">}}




{{<matomeQuote body="LLMが新しいアイデアを生み出すってどういう意味？<br>説明してくれるか、簡単な例を教えてくれない？" userName="Bassilisk" createdAt="2025/06/04 06:56:15" color="">}}




{{<matomeQuote body="それはたぶんハルシネーションのことだと思うよ。<br>それはもっともらしい新規のアイデアの例だけど、役に立たない（人を騙せるから単に役に立たないよりたちが悪いかも）。<br>表面的な繋がりしかないから、素人は騙されるけど、専門家はハルシネーションだと見抜けるんだ。" userName="SkyBelow" createdAt="2025/06/04 16:11:21" color="#785bff">}}




{{<matomeQuote body="前に論文の結果を検証するのに半年かけた研究者に会ったことがある。<br>結局、「指摘してくれてありがとう」って言われただけだった。<br>彼は静かに言ってたよ、「見られなくても、間違いを防ぐから意味がある仕事もあるんだ」って。<br>予測が現実と合ってるか確認する気がないなら、どんなにすごい技術に見えても、それはただの儚い幻想だと思う。" userName="Kiyo-Lynn" createdAt="2025/06/04 06:28:31" color="#785bff">}}




{{<matomeQuote body="彼の仕事はノーベル賞にはならないだろうけど、ほとんどの会社員に比べたら、人類への貢献や付加価値の面ではるかに進んでるよ。<br>俺たちの過去何年かの仕事について、そう言えたらいいのにね。" userName="jajko" createdAt="2025/06/04 06:38:11" color="#45d325">}}




{{<matomeQuote body="普通、会社員は会社の仕事に対して、温かい「指摘してくれてありがとう」だけじゃなくて、金銭で感謝されるだろ。" userName="eru" createdAt="2025/06/04 09:19:20" color="">}}




{{<matomeQuote body="自分の仕事って、ほとんど認められず、見向きもされないことが多いよね。<br>でも、もしかしたら、少しでも役に立ったかもしれない。<br>そう考えると、ちょっとだけ気が紛れるかな。" userName="Kiyo-Lynn" createdAt="2025/06/04 10:01:43" color="">}}




{{<matomeQuote body="なんで金払わずに会社を手伝いたいんだ？<br>会社の目標はお前の目標とは違うだろ。<br>金払わないのは、主な目標が使う金より多く儲けることだからだよ。<br>お前が使う金より多く儲けるかなんて、どうでもいいんだ。" userName="pessimizer" createdAt="2025/06/04 11:39:12" color="">}}




{{<matomeQuote body="「会社の目標がお前と違う」<br>それはお前と違うどんな組織や個人にも当てはまることだよ。会社だけが特別じゃない。<br>「金払わないのは使う金より多く儲けるのが主な目標だから」<br>金があればどんな組織や個人の目標も進めやすくなるからね。<br>だから誰だって払う金は少なくしたいインセンティブがあるんだ。<br>でもhttps://news.ycombinator.com/item?id=44179846で指摘されてるように、働いてもらうために彼らは金を払うんだ。<br>https://en.wikipedia.org/wiki/Instrumental_convergenceも見てみ。" userName="eru" createdAt="2025/06/05 04:02:42" color="#ff5c5c">}}




{{<matomeQuote body="十分な金を払ってもらえれば、そこに留まって価値を生み出し続けるだろうね。" userName="bluGill" createdAt="2025/06/04 12:10:06" color="">}}




{{<matomeQuote body="AIがコードを書く件と同じだね。訓練データにない問題で盛大に失敗するって、ずっと言ってた通りだよ。みんな毎回驚くけどさ。URLは省略しないよ: https://news.ycombinator.com/context?id=44041114 https://news.ycombinator.com/context?id=41786908" userName="boxed" createdAt="2025/06/04 06:56:13" color="#ff33a1">}}




{{<matomeQuote body="「AI can code」だけど、問題を回避すればかなり使えるよ。本物のSWEのワークフローを補強して、リンターやテストでガードレールをつけるんだ。アーキテクチャとか難しい部分はできないけど、ほとんどのSWEの時間を占める定型的な部分は大量にやってくれる。人間が品質を保証すれば、品質を落とさずに生産性を2〜5倍にできる可能性は高いね。でも管理もチェックもなしでは、どんな人間の仕事も完全にはできないのは間違いない。全然近くないよ。" userName="kmacdough" createdAt="2025/06/04 13:50:15" color="#45d325">}}




{{<matomeQuote body="「データリーケージがあったかもしれない」って点は忘れられがちだよね。<br><br>ない証拠がないなら「ある」と仮定すべき。証明責任は著者にあるんだ。小さいデータセットでも簡単に入るし、巨大データセットじゃ手動確認無理。フィルタリングしても「ない」とは言えないよ。データ汚染は頻繁に見つかるのに、なんでないって仮定するの？ hype？自分に嘘ついてるだけだよ。問題を直せないよ、嘘ついてたら。" userName="godelski" createdAt="2025/06/03 22:29:57" color="#785bff">}}




{{<matomeQuote body="どのシステムにも問題はあるよね。もっと良い問いは「許容可能な閾値はどれくらい？」ってことだよ。例えばメディケア・メディケイドの不正率は7.66%。たしかにすごい額だけど、93%は意図通りカバーされてる。システム全体が失敗してるわけじゃないよね。AIモデルも同じ。汚染率が10%なら、システム全体がダメなの？それとも許容範囲？ [1]: https://www.cms.gov/newsroom/fact-sheets/fiscal-year-2024-im..." userName="SamuelAdams" createdAt="2025/06/04 00:06:34" color="">}}




{{<matomeQuote body="プロテイン機能予測の世界だと、一般的なエラー閾値はFDR 0.001から10^-6だよ。1%のエラー率でもひどいとされる。予測精度95%は簡単だからね。難しいのは非自明な5%を正確にすることなんだ。「Acceptable」な閾値は問題によるよ。AIが貢献するには、既存手法より相当良くないとダメで、適当な閾値より良いだけじゃダメ。" userName="fastaguy88" createdAt="2025/06/04 00:16:12" color="#ff33a1">}}




{{<matomeQuote body="不正率を小数点以下2桁で言うのは超疑わしいね。不正検出は難しいんだ。メディケアが「自社基準で7.66%を不正と判断した」って言う方が正確だよ。検出されない不正は含まれてないから、本当はもっと高いかも。あと、偽陰性 vs 偽陽性のバランスも大事。科学でモデル使う場合も同じだよ。" userName="mike_hearn" createdAt="2025/06/04 07:27:09" color="#785bff">}}




{{<matomeQuote body="そのCERTってのは不正を探してるんじゃなくて、手続き上のエラーを見てるみたいだね。医療文書を要求して基準と照合してるけど、犯罪者がやるような不正（偽造文書とか）を見つけようとしてないんだ。" userName="alwa" createdAt="2025/06/04 08:23:20" color="">}}




{{<matomeQuote body="そのCERTってのは不正を探してるんじゃなくて、手続き上のエラーを見てるみたいだね。医療文書を要求して基準と照合してるけど、犯罪者がやるような不正（偽造文書とか）を見つけようとしてないんだ。" userName="mike_hearn" createdAt="2025/06/05 08:11:58" color="">}}




{{<matomeQuote body="「許容可能な閾値は？」<br><br>今、それに答えられない。それこそ問題なんだよ！<br>誰にも全然分かってないんだから、エラー率が低いって信じちゃダメ。証明責任は主張する側にあるんだ。<br>あと、パーセンテージには注意ね。数字が大きい場合。LLMは兆単位のトークンで学習。1兆の10%は1000億だよ。シェイクスピア全集は1.2Mトークン…。10%エラー率はどんなデータセットも汚染するには十分すぎ。つらい真実は、絶対数が増えるにつれて、許容可能な汚染の閾値（％）は下がる必要があるってことだ。" userName="godelski" createdAt="2025/06/04 08:39:21" color="#45d325">}}




{{<matomeQuote body="あと「何に失敗してるか？」って問題もあるよね。<br>スープがちょっと塩辛い程度の5%の失敗ならいいけど、毒が入ってる0.1%の失敗はダメだよね。" userName="dgb23" createdAt="2025/06/04 09:04:53" color="">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="それ良い例だね！ちょっとパクらせてもらうわ。毒って言っても色々あるし、もっと詳しく話せるかもね。" userName="godelski" createdAt="2025/06/04 19:47:02" color="">}}




{{<matomeQuote body="Data leakageってさ、精度じゃなくて評価の時に困るんだよ。AIがどれくらい間違ってるかすら分からなくなる。どれくらい漏れたかも分からないし、マジややこしい問題だよ。" userName="wavemode" createdAt="2025/06/04 20:51:06" color="#ff5733">}}




{{<matomeQuote body="何を信じるかの基準ってさ、みんながネットで言う「どっちが証明するかの問題」ってほど単純じゃないと思うんだよね。" userName="antithesizer" createdAt="2025/06/03 23:40:19" color="">}}




{{<matomeQuote body="それってどういう意味？あなたの言いたいこと、もっと詳しく聞かせてくれない？価値あると思うからさ。" userName="mathgeek" createdAt="2025/06/03 23:59:05" color="">}}




{{<matomeQuote body="前の人は「情報源を確認しろ、ネットの意見に流されるな」って言いたかったんでしょ。でも、その書き方がヘタで、自分でネットの意見を信じちゃいけないって証明してるじゃん。面白いね。" userName="NormLolBob" createdAt="2025/06/04 00:05:39" color="#ff33a1">}}




{{<matomeQuote body="その一般的な話、今の議論とどんな関係があるの？よく分かんないんだけど。" userName="tbrownaw" createdAt="2025/06/04 00:03:55" color="">}}




{{<matomeQuote body="記事で「DL論文はドメイン専門家がチェックしてない」って言ってるけど、本当に？私の分野だと結構みんな読んでるよ。まあ、生物学よりCS/ソフトの方がチェックしやすいかもだけど。" userName="kenjackson" createdAt="2025/06/03 22:10:18" color="#ff5733">}}




{{<matomeQuote body="生物学のラベル確認って何年もかかるんだよ。記事の例はたまたま運が良かっただけ。普通の人は、適当なモデルの予測を確かめるために何年もキャリアをかけるなんて無理だよ。" userName="a_bonobo" createdAt="2025/06/03 23:20:06" color="#ff5733">}}




{{<matomeQuote body="ちょっと聞きたいんだけど、なんでそんなに何年もかかるのか、詳しく教えてくれない？" userName="knowaveragejoe" createdAt="2025/06/04 03:16:20" color="">}}




{{<matomeQuote body="バイオ分野でのタンパク質検証がなんで何年もかかるか、色々な工程を説明するね。バイオインフォマティクス的に既知のタンパク質と比較することもあるけど、OPの論文はデータベースに何もない場合だね。<br>タンパク質の精製だけでも大変だし、機能調べたり、細胞内での場所見たり、遺伝子いじったり…一つ一つに時間かかるんだ。PhDまるごと精製に費やす人もいる。<br>E. coliにクローニングして発現させるのも数週間から数ヶ月かかるし、GFPタグで場所を見るのもね。マウスでノックアウト実験も数ヶ月だけど、他のタンパク質が補うことも多くて表現型に変化ないこともあるよ。<br>タンパク質間の結合を調べる酵母ツーハイブリッドとかもある。これらのテスト、どれか一つで機能が確定するんじゃなくて、パズルみたいに少しずつ分かってくる感じ。パズルのピースが矛盾することもあるしね。<br>日本の鹿内利治先生って人が調べたndh遺伝子とか良い例だよ。名前と実際の機能が違ったり、普通の環境だとノックアウトしても変化なかったり。<br>https://www.sciencedirect.com/science/article/pii/S000527281...<br>一つのタンパク質でもこれだけ大変なんだよ。しかもこれは葉緑体にある超一般的なタンパク質なのにね。" userName="a_bonobo" createdAt="2025/06/05 01:34:49" color="#38d3d3">}}




{{<matomeQuote body="論文読んだり結果検証したり正確性証明したりってのは全部違うことなんだよ。俺は論文たくさん読むけど、データを直接見るのは他のことに再利用する時くらい。で、そういう時にグランドトゥルースのラベルの間違いにすぐ気づくことが多いんだ。まあ、大抵のモデルは性能が低すぎて結果にそんな影響ないんだけどね…" userName="yorwba" createdAt="2025/06/04 04:59:12" color="#ff33a1">}}




{{<matomeQuote body="言語学だとさ、こういう技術使った論文を丁寧に調べて批判する人はいるんだけど、言語学者は真剣に受け止められないから、他の関連分野の人はその批判を無視する印象があるんだよね。" userName="suddenlybananas" createdAt="2025/06/03 23:39:58" color="">}}




{{<matomeQuote body="「Nature Communications」を「Nature」って呼ぶなよ、格が全然違うんだから。あと、altmetricsはパブリックな盛り上がりを測りたいならともかく、そんなに関係ないだろ。" userName="croemer" createdAt="2025/06/03 22:59:33" color="">}}




{{<matomeQuote body="更新：どうも著者がこれ読んで修正してくれたみたいだね。ありがとう！" userName="croemer" createdAt="2025/06/04 09:42:13" color="">}}




{{<matomeQuote body="研究者としての俺のLLM経験にピッタリだわ。見た目の言語理解や表現はすごく立派。でも、最良の答え（特に未解決の問題）となると、すぐに来る応答（半日かけても解けない質問に対するやつ）はめったに満足できないね。複雑な質問は探求に時間がかかるし、LLMの解けない（能力不足による）部分は、今のところ自信満々な（たとえ完全に間違ってても）応答の方に放置されてる感じ。" userName="8bitsrule" createdAt="2025/06/03 23:29:52" color="#785bff">}}




{{<matomeQuote body="これは今のAIのバブルが抱える核となる問題の一つを見事に突いてるね。俺たちは精度じゃなくて、注目を集めることを最適化してるんだ。これは生物学に限らない。気候科学、法律、医学とか、色んな分野でMLの応用で似たパターンが見られるよ。" userName="ErigmolCt" createdAt="2025/06/04 07:20:55" color="#ff5c5c">}}




{{<matomeQuote body="レイチェル・トーマスの素晴らしい記事だね！これは基本的に、ディープラーニングが訓練データの損失が大きい表現でしかないから、生成的な情報検索、つまりstochastic parrotとしてしか機能しないっていうもう一つの議論だ。遺伝子のデータが基礎となる生物学を完全に表してるわけじゃないから、出力がおかしくなる可能性があるんだ。うまくいく時はデータリークだよ。LLMは情報検索ツールとして設計されてるからね。アルゴリズムのせいじゃなくて、訓練データセットのせいだと思うな。人間は自然言語が得意だからNLPは成功するけど、訓練データが不完全な領域じゃダメなんだ。" userName="slt2021" createdAt="2025/06/03 22:19:34" color="#785bff">}}




{{<matomeQuote body="これが俺にとって現代のLLMのパラドックスだね。元の領域を直接じゃなく、テキストにできる情報は表現できる。だから情報は表せてるんだけど、それが何？どうやって？が不明確。テキストはごちゃごちゃしたエンコードだからね。あと、生成的にするために、データベースに全部入れるんじゃなく、一部をアルゴリズムに任せて、不正確でも何でも聞けるようにしてるんだ。でも、一つのアルゴリズムで領域内の全部の質問に正確に答えるのは無理だから、情報の精度を失うんだ。これが今のLLMに対する俺の見方かな。" userName="ffwd" createdAt="2025/06/04 04:58:45" color="#38d3d3">}}




{{<matomeQuote body="＞生成的な情報検索としてしか機能しない<br>たとえこれが真実だとしても、LLMは残り続けると思うよ。<br>スキルの低いジュニア開発者がStack Overflowとかで情報「検索」して問題解くみたいに、適切にできたAI自動化ツール（チャットプロンプトだけじゃなく！）を開発者みんなに渡すのは、面倒な単純タスクを全部任せられるジュニア開発者を一人ずつ与えるようなもん。ジュニアの成長を心配しなくていいし。ちゃんとツールがあれば、AIは正しいことやって→ツール実行→問題修正ってループをこなすだろう。値段はジュニア開発者の何十分の一？だから、実際のジュニア開発者を教えるとか、もっと重要なことに時間を使えるってことだ。<br>まだ完璧じゃないけど、今の基盤モデルでも、適切な組み合わせ方次第で十分そこまでいけると思うよ。" userName="dathinab" createdAt="2025/06/04 17:48:27" color="#ff5c5c">}}




{{<matomeQuote body="プログラミング言語は人間が作ったから、LLMの訓練データは十分完璧で結果も良い。自然言語がプログラミングコードのネイティブな領域だしね。でも生物学の領域は、生物や分子の物理化学的な反応にある。相互作用の法則は人間が作ったんじゃない。だから訓練データは領域のごくわずかしか捉えられてないんだ。このせいで、どんなモデルも不十分になるんだよ。" userName="slt2021" createdAt="2025/06/04 22:09:40" color="#785bff">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
