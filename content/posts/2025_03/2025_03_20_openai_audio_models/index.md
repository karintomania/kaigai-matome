+++
date = '2025-03-20T00:00:00'
months = '2025/03'
draft = false
title = 'OpenAIの音声モデルがマジ凄い！ElevenLabsより圧倒的に安いってマジ！？'
tags = ["OpenAI", "音声モデル", "TTS", "文字起こし", "AI"]
featureimage = 'thumbnails/light-orange3.jpg'
+++

> OpenAIの音声モデルがマジ凄い！ElevenLabsより圧倒的に安いってマジ！？

引用元：[https://news.ycombinator.com/item?id=43426022](https://news.ycombinator.com/item?id=43426022)

{{<matomeQuote body="もし値段設定が正しければ、OpenAIのモデルはElevenLabsよりめっちゃ安いみたいだよ。<br>https://platform.openai.com/docs/pricing<br>もしこれが“gpt-4o-mini-tts”モデルで、音声1分あたり0.015ドルの見積もりがあってるなら、ElevenLabsより85%も安いってことだね。<br>https://elevenlabs.io/pricing<br>ElevenLabsだと、一番コスパの良い“Business”プランを月1100ドル（年間請求だと13200ドルで、月払いより17%お得）で契約すると、11000分のTTSが使えて、1分あたり10セントかかる。<br>OpenAIなら、165ドルで11000分のTTSが使える計算になる。<br>誰か計算してみて…これマジ？" userName="benjismith" createdAt="2025-03-20T18:21:05" color="#ff33a1">}}

{{<matomeQuote body="マジで安いよー。っていうか、ElevenLabsが高すぎるんだよね。でも、ElevenLabsに匹敵するクオリティのところはないんだよなー。特に、ボイスアシスタントとかオーディオブック、ポッドキャスト、ニュースキャスター、テレビのアナウンサーみたいな声じゃないのが欲しいならね。<br>OpenAIのこれはマジ面白い。ElevenLabsにはない感情コントロールができるし。でも、ハルシネーションもあるから、そこを直せばマジ使えるようになると思う。" userName="furyofantares" createdAt="2025-03-20T20:08:58" color="#38d3d3">}}

{{<matomeQuote body="OpenAIが全部安いのは、投資家の金で補助されてるからだよ。そのバカ金が流れ続けるうちは良いけどね！その後は、WeWorkみたいになるか、みんなが使えるようにするために改悪されるかのどっちかだと思う。Softbankが1500億ドルくらいつぎ込んで買い取るってことでもない限りね。OpenAIの基礎について見て見ぬふりしてる人が多すぎる気がするんだけど、なんで表に出てこないのかマジわからん。" userName="camillomiller" createdAt="2025-03-21T09:52:59" color="">}}

{{<matomeQuote body="Deepseekとか他のホスティングサービスと比べると、OpenAIはマジで高いマージン取ってるってわかると思うよ(Deepseekは80%の利益率で、OpenAIより10倍安い)。トレーニングとかR&DでOpenAIがVCの金使ってるのは確かだけど、WeWorkみたいにプロダクト自体が金を燃やしてる会社とは違うと思う。" userName="ImprobableTruth" createdAt="2025-03-21T10:12:19" color="#38d3d3">}}

{{<matomeQuote body="OpenAI自身も、推論でさえ赤字だって言ってた気がするんだけど、勘違いかな？" userName="camillomiller" createdAt="2025-03-21T10:43:11" color="">}}

{{<matomeQuote body="OpenAIのサブスク、特にProサブスクは、一番高いモデルが定額で使えるからね。APIの値段はもっと高いよ。通常のサブスクで損してるかどうかは不明だけど、もしそうなら、多分そこまでじゃないと思う。でも、＞mindshare＜とかを得るために補助してるって言う方が近いかもね。" userName="ImprobableTruth" createdAt="2025-03-21T11:27:52" color="">}}

{{<matomeQuote body="コスパで言えば、gpt4oより安いモデルはたくさんあると思うよ。OpenAIがめっちゃ高いレート制限とスループットを提供してるってこと以外は、Sonnet 3.5とか3.7とか、GoogleのFlash 2.0じゃなくてOpenAIを使う理由があんまりない気がする。" userName="yousif_123123" createdAt="2025-03-21T14:00:25" color="">}}

{{<matomeQuote body="OpenAIは、一部の顧客の一部の月では損してるのは間違いないけど、全体的にはサブスク（最近解約した俺のも含めて）からAPIに移行した方が良い人が多いと思うよ。<br>サブスク持ってた人でめっちゃ使ってたって人を知らないし、それが全体的に利益が出てる理由だと思う。<br>Copilotも同じだと思う。特にビジネス版は、俺のアカウントでは間違いなく損してると思うけど、会社全体のサブスクで見ると、支払ってる金額の30%くらいにしかならないんじゃないかな。" userName="Szpadel" createdAt="2025-03-22T06:13:03" color="#45d325">}}

{{<matomeQuote body="それは違うよ。ElevenLabsのマージンはありえないくらい高いし、最大の強みは高品質な音声データだよ。" userName="BoorishBears" createdAt="2025-03-23T16:11:59" color="">}}

{{<matomeQuote body="公平に見て、ElevenLabsも3億ドルくらいのVCマネーを集めてるからね。" userName="ashvardanian" createdAt="2025-03-21T19:10:06" color="">}}

{{<matomeQuote body="あはは、マジこの組み合わせは笑えるし、記事読むたびに内容が全然違うんだよね。リンクはこちら→　https://www.openai.fm/#b2a4c1ca-b15a-44eb-9cd9-377f0e47e5a6" userName="asah" createdAt="2025-03-21T18:12:11" color="">}}

{{<matomeQuote body="Elevenlabsはエコシステム戦略って感じだよね。たくさんの声があって、ちゃんと本人たちがアップロード許可してるんだって。声のマーケットプレイスだね。他の大手は誰もやってないけど、なんでだろ？" userName="com2kid" createdAt="2025-03-20T22:05:24" color="#38d3d3">}}

{{<matomeQuote body="これってAI企業が、声とかのトレーニングデータにお金払うべきってこと？<br>全部パクって政府に例外認めさせる方が、ぜんぜん楽じゃん。" userName="SXX" createdAt="2025-03-21T06:11:32" color="">}}

{{<matomeQuote body="ElevenLabsだけが、イントネーションとかプロソディ、タイミングをそのまんまにspeech to speech生成できるんだよね。だから、表現力豊かな声優さんがいろんな声に変われるんだ。" userName="oidar" createdAt="2025-03-20T22:46:59" color="#38d3d3">}}

{{<matomeQuote body="OpenAIのRealtime speech to speechの方が、ElevenLabsよりぜんぜんすごいよ。" userName="goshx" createdAt="2025-03-20T23:42:45" color="">}}

{{<matomeQuote body="ElevenLabsとOpenAIが言ってる“speech to speech”って、全然違うんだよね。<br>ElevenLabsのは、音声を入力して、イントネーションはそのままに、違う人が喋ってるみたいな音声に変換するんだ。<br>OpenAIのは、エンドツーエンドのマルチモーダル会話モデルで、ユーザーが喋るのを聞いて、音声で答えるんだよ。" userName="noahlt" createdAt="2025-03-21T05:26:56" color="#45d325">}}

{{<matomeQuote body="ElevenLabsはめちゃくちゃ高いから、すごいMRR（月間経常収益）を達成して資金調達できたんだよね。でも、絶対もっと安くて同じくらいのクオリティのやつが出てくるよ。価格競争になるよね。ElevenLabsはこれから大変だよ。高すぎるもん。" userName="echelon" createdAt="2025-03-20T18:34:44" color="">}}

{{<matomeQuote body="もっと独自の製品を見つけてほしいな。みんなテキスト読み上げだと思ってるけど、俺はElevenLabsをvtubingのspeech to speechに使ってるんだよね。マジで高品質なspeech to speechはここしかないんだよ（他に知ってる人いたら教えてほしい）。＞https://github.com/w-okada/voice-changer　はリアルタイムだけど、クオリティが落ちるから、何を言ってるかわからなくなっちゃうんだよね。それにRTX 3090が必要だし。ElevenLabsならクラウドでどこでもできるし。<br>ElevenLabsがないとvtubingできないけど、speech to speechってあんまり需要ないのかな？" userName="MrAssisted" createdAt="2025-03-20T18:53:07" color="#45d325">}}

{{<matomeQuote body="使ってる動画とリップシンクの環境教えてもらってもいい？全然知らないんだけど、最近何ができるのか興味あるんだ。" userName="eob" createdAt="2025-03-20T19:56:33" color="">}}

{{<matomeQuote body="前の動画では＞https://github.com/warmshao/FasterLivePortrait　をRTX 3090で動かして、リアルタイムで録画したんだけど、次はrunpodインスタンスでFasterLivePortraitを動かそうと思ってる。そっちの方が60fpsでスムーズに見えるからね。クラウドでAI vtubingをリアルタイムでやるなら、俺のGenDJプロジェクト（＞https://github.com/kylemcdonald/i2i-realtime　のフォーク）しかないけど、LivePortraitの方が全然キレイなんだよね。誰かFasterLivePortaitのinsightface（商用利用禁止）を外して、GenDJに組み込んでほしいな。そのうちやるかも。" userName="MrAssisted" createdAt="2025-03-20T20:21:53" color="#785bff">}}

{{< details summary="もっとコメントを表示（1）">}}
{{<matomeQuote body="あなたのYouTubeアカウント教えてくれませんか？もしよかったら、それをあなたのHNアカウントにリンクしてもいいんだけど。結果がすごく見たいんだよね。" userName="maest" createdAt="2025-03-21T00:03:19" color="">}}

{{<matomeQuote body="マジでElevenLabsは他のどこよりも桁違いに高いよね。ビジネスの観点からすると、めっちゃ賢いと思う。彼らは（以前は？）最高だったから、人々がそれに対してプレミアムを払うことを知ってるんだ。" userName="huijzer" createdAt="2025-03-20T21:05:16" color="#38d3d3">}}

{{<matomeQuote body="うん、多分その通りだと思う。11labsの100万文字で計算したら、同じ数字になったよ（Proプラン）。<br>めっちゃ嬉しいんだけど、だってまさにこうなるって賭けてたんだもん。私は、100万文字あたりのTTS価格がもっと安くなるか、セルフホストモデルじゃないと動かない消費者向けTTSアプリを作ってるんだ。" userName="lukebuehler" createdAt="2025-03-20T18:27:06" color="#45d325">}}

{{<matomeQuote body="Kokoro TTSはオープンソースとしてはかなり良いよ。チェックする価値あり。" userName="lherron" createdAt="2025-03-20T19:43:29" color="">}}

{{<matomeQuote body="マジかよ、彼らは「Sky」ボイスを持ってるじゃん。それってOpenAIが持ってて削除したのと同じものみたい？どういうことか分かんないけど、めっちゃ嬉しい。" userName="stavros" createdAt="2025-03-20T23:56:43" color="#45d325">}}

{{<matomeQuote body="＞どういうことか分からない<br>スカーレット・ヨハンソンの映画をいっぱいダウンロードして、彼女が話してる音声クリップに分割して、モデルをトレーニングするんだよ（笑）" userName="diggan" createdAt="2025-03-21T01:05:38" color="">}}

{{<matomeQuote body="それ、マジで彼女なの？そうじゃないと思ってたけど、もしかしたら…？" userName="stavros" createdAt="2025-03-21T07:59:17" color="">}}

{{<matomeQuote body="OpenAIからのリークがない限り、イエスかノーか確認できることはないと思う。でも私の脳みそは、最初にその声を聞いたときからヨハンセンだと思ったし、そう感じてるのは私だけじゃないみたい。彼らがその声を削除したってことも、彼女の声でトレーニングされたってことを物語ってる気がする。<br>今日改めてSkyを聞いてみても、やっぱりヨハンセンが根底にある声優さんだと感じる。でも、潜在的なバイアスがあるのかもしれない。" userName="diggan" createdAt="2025-03-21T10:22:58" color="#ff33a1">}}

{{<matomeQuote body="どんなファイルでも(pdf, epub, txt)オーディオブックに変換して、mp3としてダウンロードしたり、Apple PodcastsアプリとかでRSSフィード経由で直接聴いたりできるよ。<br>基本的に、自分自身や数人の友達のために、1回限りのオーディオブックを作るんだ。" userName="lukebuehler" createdAt="2025-03-20T20:23:11" color="#ff5c5c">}}

{{<matomeQuote body="Chrome拡張機能版を作る予定はあるのかな？高品質で安いTTS拡張機能をずっと探してるんだよね（ElevenLabs Human Readerみたいなやつで、値段がもっとお手頃なのが欲しいな）" userName="setsewerd" createdAt="2025-03-20T21:24:20" color="">}}

{{<matomeQuote body="それ考えたことなかったけど、面白いアイデアだね。今はオフラインでも聞けるような長文コンテンツに集中してるんだ。プラグインで長文を読み込めるようにするのはアリかもだけど、今はスクリーンリーダーの開発はしてないんだ。" userName="lukebuehler" createdAt="2025-03-20T22:19:42" color="">}}

{{<matomeQuote body="数式を人間みたいに読み上げてくれるサービスって今あるのかな？ずっと探してるんだけど。（僕が読むのは物理関係だけなんだよね）" userName="wholinator2" createdAt="2025-03-20T22:26:29" color="#45d325">}}

{{<matomeQuote body="ここではどんな用途で使いたい感じ？" userName="dockerd" createdAt="2025-03-21T07:14:13" color="">}}

{{<matomeQuote body="Sesameが無料で結構使えるよ。自分で動かせるし。" userName="whimsicalism" createdAt="2025-03-20T20:05:49" color="#38d3d3">}}

{{<matomeQuote body="制限されたモデルが公開されたみたいだよ：<br>https://github.com/SesameAILabs/csm/issues/63" userName="kuprel" createdAt="2025-03-20T20:27:46" color="">}}

{{<matomeQuote body="朗報！Orpheus-3BのおかげでSesameがほぼ不要になったね。" userName="hnhn34" createdAt="2025-03-20T20:48:44" color="#ff5c5c">}}

{{<matomeQuote body="ありがとう、良さそうだね。<br>他の人のためにリンク：<br>https://canopylabs.ai/model-releases" userName="Foreignborn" createdAt="2025-03-20T21:50:57" color="">}}

{{<matomeQuote body="なんか声が全部イマイチなんだよね。SesameのMilesがいいのは、なんかクールなところなんだよ。" userName="sandspar" createdAt="2025-03-20T22:24:42" color="">}}

{{<matomeQuote body="OpenAIはボイスクローニングはやってないよ。" userName="kuprel" createdAt="2025-03-20T18:48:18" color="">}}

{{<matomeQuote body="よっ、Jeffだよ。OpenAIでこのモデルのPMやってるんだ。今日、3つの最新オーディオモデルを発表したぜ。2つの音声テキスト変換モデルはWhisperより高性能。新しいTTSモデルは話し方を指示できる（openai.fmで試して！）。Agents SDKもオーディオに対応して、テキストエージェントを簡単に音声エージェントにできるようになったんだ。気に入ってくれると嬉しいな。何か質問があればどうぞ！" userName="jeffharris" createdAt="2025-03-20T17:55:24" color="#ff33a1">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（2）">}}
{{<matomeQuote body="Jeffさん、すごいっすね！新しい音声テキスト変換モデルに、単語のタイムスタンプを追加する予定はありますか？<br>＞Other parameters, such as timestamp_granularities, require verbose_json output and are therefore only available when using whisper-1. ”他のパラメータ、例えばtimestamp_granularitiesはverbose_json出力を必要とするため、whisper-1を使用する場合にのみ利用可能です。”<br>単語のタイムスタンプは、複数人が参加する議論とかTwitterのスペースみたいな長い通話でめちゃくちゃ役に立つんですよ。発言を意味の区切りで分割できるから。タイムスタンプがないと、話者がお互いに邪魔し合うような場面で、音声を分割して精度が落ちちゃうんすよね。" userName="claiir" createdAt="2025-03-21T00:06:51" color="#45d325">}}

{{<matomeQuote body="ドキュメント読んだんだけど（ChatGPTに要約してもらった）、これらのモデルには話者分離（speaker diarization）についての言及がないね。これって、ちょっと頑張れば誰でもできることじゃない？マジで解決すべき問題だと思うんだよね。今のところ、話者分離で60%以上の精度を出せるツールはないんだよ。会議の内容をチャットで確認して、誰がいつ何を約束したかを知りたいってニーズは絶対あるはず。だから、話者分離モデルを作ってよ！" userName="noosphr" createdAt="2025-03-21T00:08:22" color="#785bff">}}

{{<matomeQuote body="＞This is a _very_ low hanging fruit anyone with a couple of dgx h100 servers can solve in a month and is a real world problem that needs solving. ”数台のdgx h100サーバーがあれば誰でも1ヶ月で解決できる簡単な問題だ”って意見があるけど、簡単じゃないと思うな。人間には簡単だけど、機械には難しい。でも、見過ごされてるってのはその通り。speechmatics.comで働いてるんだけど、何年もかけて取り組んでるんだ。今では世界最高のリアルタイム話者分離システムだと信じてるよ。ぜひ試してみて。" userName="markush_" createdAt="2025-03-21T07:44:58" color="">}}

{{<matomeQuote body="gpt-4o-transcribeが、音声ストリーム内の指示に混乱して、文字起こしする代わりに指示に従っちゃう可能性ってある？" userName="simonw" createdAt="2025-03-20T19:16:15" color="">}}

{{<matomeQuote body="1) 以前のTTSモデルはアクセントに大きな問題があったよね。例えば、スペイン語の文章で、スペインのアクセントからメキシコのアクセント、アメリカのアクセントへと変化したり。これは改善された？まだ開発中？<br>2) レイテンシーは？<br>3) STT API/Whisperは、ユーザーが言ってないことを勝手に作り出す（hallucinating）問題が大きかったよね。これは修正された？<br>4) Whisperやオーディオモデルは、文法的な誤りとか、スペイン語を話している人が英語の単語を挿入した場合に、自動的に修正してたよね。これはまだ起こる？" userName="dandiep" createdAt="2025-03-20T18:51:45" color="">}}

{{<matomeQuote body="1/ アクセントにはかなり取り組んできたから、改善されてるはず…まだ終わってないけどね。どう思うか気になるな。具体的な指示や例をたくさん試してみて。<br>2/ 可能な限り高速化するために全力を尽くしてるよ。リアルタイムよりも意味のある速さでオーディオをストリーミングできることが非常に重要なんだ。<br>3+4/ ハルシネーション（幻聴）を「解決済み」とは言えないけど、これらのモデルの中心的な焦点だったんだ。だから、かなり改善されてると思うよ。" userName="jeffharris" createdAt="2025-03-20T19:00:56" color="#45d325">}}

{{<matomeQuote body="3) WhisperはSilero VADと組み合わせる必要があるよね。そうしないと、ハルシネーションの問題でほとんど使い物にならない。" userName="jbaudanza" createdAt="2025-03-21T00:56:41" color="">}}

{{<matomeQuote body="Jeffさん、音声テキスト変換モデルで、デュアルチャンネルのオーディオ録音（例えば、Twilioの電話音声）をサポートする予定はありますか？今は、各チャンネルを個別に処理して会話の文脈を失うか、チャンネルをマージして話者識別を失うかのどちらかしかないんです。" userName="a-r-t" createdAt="2025-03-20T19:07:39" color="">}}

{{<matomeQuote body="最近よく話題に上がるよね。まだ発表できることはないけど、十分な数の開発者が求めてくれれば、モデルのトレーニングに組み込むつもりだよ。<br>話者分離も追加する予定の機能だよ。" userName="jeffharris" createdAt="2025-03-20T23:26:29" color="">}}

{{<matomeQuote body="えーと、何が言いたいのか পুরোপুরি理解じゃないんだけど、twilioの録音ってデュアルチャンネルに対応してるよね。" userName="ekzy" createdAt="2025-03-20T21:30:24" color="">}}

{{<matomeQuote body="OpenAIの音声テキスト変換を使って、twilioのデュアルチャンネル録音をチャンネル識別を維持したまま文字起こしするってことね。" userName="a-r-t" createdAt="2025-03-20T22:01:01" color="#785bff">}}

{{<matomeQuote body="なるほど、そういうことね。それ、いい機能だね。でも、タイムスタンプが取得できれば、問題回避は簡単じゃない？" userName="ekzy" createdAt="2025-03-20T22:08:02" color="">}}

{{<matomeQuote body="知ってる限り、2つの方法があるよ。<br>1．両方のチャンネルを1つに結合する(Whisperがデュアルチャンネル録音でやってるように)。そして、文字起こしのタイムスタンプを元のチャンネルにマップし直す。これはスピーカーが互いに話<br>し合わない場合にのみ有効で、そうでない場合が多い。<br>2．各チャンネルを個別に文字起こしし、トランスクリプトをマージする。これは完璧なチャンネル識別を保持するけど、会話のコンテキストが失われるんだよね。<br>だから、技術的には簡単な解決策は2つあるけど、チャンネル識別が不正確になるか、文字起こしの品質が低下するかのどっちかになっちゃう。もっと良い解決策は、チャンネルIDを示す追加のトークンを受け入れるようにトレーニングされたモデルで、両方のチャンネルのコンテキストから恩恵を受けながら、出力に保持することじゃないかな。" userName="a-r-t" createdAt="2025-03-21T02:50:17" color="#ff5733">}}

{{<matomeQuote body="これらの重みを公開する予定ってある？" userName="kouteiheika" createdAt="2025-03-20T18:04:06" color="">}}

{{<matomeQuote body="これらのモデルってダウンロードできるの？Whisperみたいに。実行するための最低限のハードウェアは何？Raspberry Piとかスマホでも動くかな？" userName="nico" createdAt="2025-03-20T18:08:50" color="">}}

{{<matomeQuote body="現時点ではオープンソースじゃないんだ。残念ながら、普通のコンシューマー向けハードウェアで実行するには大きすぎるんだよね。" userName="jeffharris" createdAt="2025-03-20T19:23:25" color="">}}

{{<matomeQuote body="それがオープンソースにしない理由なの？それでも、愛好家向けに提供するのは意味があるんじゃない？" userName="echoangle" createdAt="2025-03-20T19:38:00" color="">}}

{{<matomeQuote body="ダイアライゼーション（話者分離）とかボイスプリンティングを直接サポートする予定ってある？" userName="staticautomatic" createdAt="2025-03-20T18:03:15" color="">}}

{{<matomeQuote body="ダイアライゼーション（GPTモデルに時間認識を追加すること）については検討中だけど、まだ共有できる具体的な計画はないんだ。" userName="jeffharris" createdAt="2025-03-20T18:22:26" color="">}}

{{<matomeQuote body="Jeff、マジで最高なのってさ、ただの「話者1」「話者2」みたいな区別じゃなくて、会話の内容から「この人はJeff Harrisって呼ばれてる」とか「Jeff」って分かって、そう呼んでくれることじゃない？" userName="youssefabdelm" createdAt="2025-03-20T20:08:41" color="#38d3d3">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（3）">}}
{{<matomeQuote body="欲しい機能は話者識別だなー。音声ファイルを突っ込んだら、「話者1：…」「話者2：…」みたいにトランスクリプトが出てくるのが理想。それにタイムスタンプが付いてたら最高じゃん？GoogleのGemini 2.0モデルはちょっと期待できそうだけど、まだ信頼性は分かんない。" userName="simonw" createdAt="2025-03-20T19:50:14" color="#ff33a1">}}

{{<matomeQuote body="プロソディとかイントネーション、タイミングをそのまま保てるspeech to speechモデルって出す予定ある？ElevenLabs、マジ高いんだよね。" userName="oidar" createdAt="2025-03-20T22:48:09" color="#38d3d3">}}

{{<matomeQuote body="GPT-4oベースのモデルをもっとコントロールできるように拡張していくつもりだよ。一番足りない機能ってカスタムボイスのことかな？" userName="jeffharris" createdAt="2025-03-20T23:24:20" color="#ff5c5c">}}

{{<matomeQuote body="Jeffさん、TTSエンドポイントのアップデートありがとう！マジでチャットの補完エンドポイントで、トランスクリプトが合うか運任せの回避策を作ろうとしてたんだ…アップデートされた音声モデルを使うにはそれしかなかったから。GPT-4o-mini-ttsって、チャットの補完で言うところのgpt-4o-mini-audio-previewと同じ？タイミングを測ったら短いフレーズを返すのに2秒くらいかかって、gpt-4o-audio-previewに近い気がする。そっちはアドリブがなかったから、運任せ作戦には良かったんだよね！あと、指示にアクセントを付けられるけど、ローカライズされた音声モデルを出す予定はある？" userName="robbomacrae" createdAt="2025-03-20T18:42:36" color="#ff5733">}}

{{<matomeQuote body="TTSのためにもう少し良いモデルだよ。スクリプトを正確に読み上げることに重点を置いた追加のトレーニングを行ったんだ。例えば、audio-previewモデルに「イタリアの首都はどこですか」と話すように指示すると、「ローマ」と答えることがよくありました。このモデルはその点ではるかに優れているはずだよ。<br>ローカライズされた音声モデルを提供する予定はないけど、さまざまなアクセントに最適な音声でメニューを拡張したいと考えています。" userName="jeffharris" createdAt="2025-03-20T19:02:56" color="#38d3d3">}}

{{<matomeQuote body="リアルタイムAPIのアップデートはいつ頃になるか知ってる？まだベータ版だし、色々問題があって（例えば、声がランダムに途切れたり、VADの問題、特にmulawとか…）本番環境で使うのは無理だけど、OpenAIからのコミュニケーションが少ないんだよね。何を信じていいか分かんない。stt->llm->ttsを推進してるってことは、リアルタイムAPIでの開発は続けるべきか悩む。" userName="ekzy" createdAt="2025-03-20T21:39:27" color="#ff5c5c">}}

{{<matomeQuote body="だよねー、放置されてる感じがマジ嫌だ…。裏で頑張ってて、すぐに何かリリースしてくれると良いんだけど。" userName="taf2" createdAt="2025-03-20T21:43:11" color="#ff5733">}}

{{<matomeQuote body="S2Sに一番力を入れてるんだ…遅くなっててごめん。頑張ってるよ。<br>今の最優先事項は<br>1）関数呼び出しのパフォーマンス向上<br>2）認識精度の向上（聞き間違いをなくす）<br>3）指示に従う信頼性の向上<br>4）バグ修正（カットオフ、run ons、モダリティステアリング）" userName="jeffharris" createdAt="2025-03-20T23:32:57" color="#45d325">}}

{{<matomeQuote body="＞2つのspeech-to-textモデル—Whisperを上回る<br>何の指標で？あと、Whisperはもう精度で最先端じゃないけど、このベンチマークの他のモデルと比べてどうなの？https://artificialanalysis.ai/speech-to-text" userName="progbits" createdAt="2025-03-20T18:31:30" color="#45d325">}}

{{<matomeQuote body="FLUERSの評価を使ってて、他のモデルとの比較が投稿にあるよ！<br>https://openai.com/index/introducing-our-next-generation-aud...<br>一番信用してるベンチマークってある？" userName="jeffharris" createdAt="2025-03-20T19:04:33" color="">}}

{{<matomeQuote body="ジェフさん、OpenAIのウェブとスマホアプリにあるTTSを改善してほしいな。ルーマニア語で数字を読ませると、数字が不明瞭になっちゃうんだよね。普通の単語でもたまにそうなるし。英語以外の言語のリソースも増やしてほしいな。" userName="visarga" createdAt="2025-03-20T19:15:22" color="#38d3d3">}}

{{<matomeQuote body="ご指摘ありがとう！数字の正確さ（特にトレーニングデータが少ない言語）は、まだ改善が必要な点なんだ。" userName="jeffharris" createdAt="2025-03-20T19:18:43" color="#38d3d3">}}

{{<matomeQuote body="安定したリアルタイム音声翻訳モデルをリリースしてくれー！今のバージョンだと、常に若いティーンが大学に進学するのを悲しんだり、急に興奮したりしてるみたいになっちゃうんだよ。" userName="taf2" createdAt="2025-03-20T21:41:53" color="">}}

{{<matomeQuote body="性的なコンテンツに使うのは利用規約違反？" userName="TheAceOfHearts" createdAt="2025-03-20T18:02:23" color="">}}

{{<matomeQuote body="うん、規約に書いてあるよ。“未成年者にとって不適切なツールを作っちゃダメ。たとえば、性的に露骨なコンテンツとかね。科学的または教育目的で作成されたコンテンツは除く”って。<br>https://openai.com/policies/usage-policies/" userName="jeffharris" createdAt="2025-03-20T18:22:41" color="#ff5733">}}

{{<matomeQuote body="WebRTC経由のリアルタイムAPIの文字起こしサンプルコードでエラーが出てるよ。確認してくれる？" userName="mazd" createdAt="2025-03-20T21:36:17" color="#45d325">}}

{{<matomeQuote body="Whisperingって機能する？試してみたけど、うまくいかなかった。" userName="mclau156" createdAt="2025-03-20T19:04:43" color="">}}

{{<matomeQuote body="イギリス英語のアクセントはないの？" userName="wewewedxfgdf" createdAt="2025-03-20T18:53:50" color="">}}

{{<matomeQuote body="balladかfableのボイスを試してみて。" userName="jeffharris" createdAt="2025-03-20T19:04:58" color="">}}

{{<matomeQuote body="今回のtext-to-speechとspeech-to-textモデル、指示とデータを同じトークンストリームで混ぜてるせいで、信頼性に問題があるみたいだね。🤔<br>これって実用上どれくらい問題なんだろう？まだよく分かんないんだ。<br>詳しくはここにメモっといたよ！→ https://simonwillison.net/2025/Mar/20/new-openai-audio-model…" userName="simonw" createdAt="2025-03-20T20:55:53" color="">}}


{{< /details >}}


[記事一覧へ]({{% ref "/posts/" %}})
