+++
date = '2025-04-05T00:00:00'
months = '2025/04'
draft = false
title = '衝撃！MetaのLlama 4、最強モデル集団「Llama 4 herd」を発表！GPT-4o超えも！？'
tags = ["LLM", "Meta", "Llama 4", "AI", "大規模言語モデル"]
featureimage = 'thumbnails/blue3.jpg'
+++

> 衝撃！MetaのLlama 4、最強モデル集団「Llama 4 herd」を発表！GPT-4o超えも！？

引用元：[https://news.ycombinator.com/item?id=43595585](https://news.ycombinator.com/item?id=43595585)

{{<matomeQuote body="ページがうまく表示されないみたいだから、概要をまとめるね。<br><br>Llama 4 モデル：<br>・Llama 4 ScoutとLlama 4 Maverickは、どちらもMixture-of-Experts (MoE) デザインを採用してて、アクティブなパラメータはそれぞれ17B。<br>・テキストと画像の入力に対応したマルチモーダルで、出力はテキストのみ。<br>・業界トップクラスのコンテキスト長、強力なコーディング・推論性能、多言語対応が改善。<br>・知識カットオフ：2024年8月。<br><br>Llama 4 Scout：<br>・アクティブなパラメータは17B、エキスパート数16、合計109B。<br>・H100 GPU1枚に搭載可能（INT4量子化）。<br>・10Mトークンのコンテキストウィンドウ<br>・リソース効率が良いのに、以前のLlamaリリースよりもマルチモーダルタスクで優れた性能を発揮。<br>・効率的な長文コンテキストアテンションのためにiRoPEアーキテクチャを採用。<br>・プロンプトあたり最大8枚の画像でテスト済み。<br><br>Llama 4 Maverick：<br>・アクティブなパラメータは17B、エキスパート数128、合計400B。<br>・シングルGPUでは動作せず、H100 DGXホストで動作するか、より効率的に分散可能。<br>・コーディング、推論、多言語テストでGPT-4oやGemini 2.0 Flashよりも優れた性能を競争力のあるコストで実現。<br>・強力な画像理解と根拠に基づいた推論能力を維持。<br><br>Llama 4 Behemoth (プレビュー)：<br>・アクティブなパラメータは288B、エキスパート数16、合計約2T。<br>・まだトレーニング中で、未リリース。<br>・STEMベンチマーク（MATH-500、GPQA Diamondなど）でGPT-4.5、Claude Sonnet 3.7、Gemini 2.0 Proを上回る。<br>・共同蒸留によって、ScoutとMaverickの“教師”モデルとして機能。<br><br>その他：<br>・MoEアーキテクチャ：トークンごとに17Bのパラメータのみがアクティブになり、推論コストが削減される。<br>・ネイティブマルチモーダリティ：大規模なラベルなしデータで事前トレーニングされた統一テキスト+ビジョンエンコーダ。" userName="laborcontract" createdAt="2025-04-05T18:48:31" color="#45d325">}}

{{<matomeQuote body="超無知な人向けに言うと：<br>Llama 4 ScoutとLlama 4 Maverickは、どちらもMixture-of-Experts (MoE) デザインで、アクティブなパラメータはそれぞれ17Bなんだって。<br>そのエキスパートって、特定のタスクで訓練されたLLMのこと？" userName="InvOfSmallC" createdAt="2025-04-05T20:34:17" color="">}}

{{<matomeQuote body="最初はバカげてると思ったけど、実際に動くことが証明されたアイデア。<br>トレーニングを通して、たくさんの”エキスパート”に多様化を促して、色んなことが”得意”になるようにするんだ。これらのエキスパートは、もしモデルがdenseだったら、モデルサイズの1/10から1/100くらい。<br>それを全部まとめて一つのモデルにして、どの小さなエキスパートモデルが与えられたトークン入力に最適かを選んで、その小さなエキスパートにルーティングするレイヤーを追加する。すると、denseなパラメータをフルで実行する代わりに、ルーターを通って、1/10の長さの小さなモデルを通過するだけで済む。<br>どうやって良い”ピッカー”を作るかって？それは微分可能だから、MLの得意技である勾配降下法でエキスパートを訓練しながらデサイダーも訓練するだけ！<br>これは概ねうまくいくけど、注意点もたくさんある。でも、ほとんどの場合、無料のランチ…少なくとも割引ランチみたいなもの。<br>エキスパートが何をしてるかについてはあんまり分析されてないけど、専門化する傾向があるのは広く認められてると思う。専門化は（エキスパートの数が少ない場合は特に）非常に難解/濃密になるかも。<br>Anthropicの解釈可能性チームなら質の高い分析ができると思うけど、今のところAnthropicのモデルはMoEじゃないはず。<br>個人的には、MoEモデルは”深い”思考がちょっと苦手な気がするけど、重みが多い方が好きってバイアスかも。でも、クロック時間、GPU時間、メモリや帯域幅の使用量あたりでは、似たようなトレーニングを受けたdenseモデルよりも明らかに高速で優れている。" userName="vessenes" createdAt="2025-04-05T20:48:33" color="#785bff">}}

{{<matomeQuote body="名前から直感的にわかりにくいのは、”エキスパート”が、数学が得意なサブLLMで、数学の問題を聞かれた時に呼び出されるようなものじゃないってこと。こういうモデルは、トークンを流すネットワークのレイヤーを持ってて、それぞれのレイヤーは256個のサブネットワークで構成されてる。どのサブネットワークも、各レイヤーで個別に選択（または複数選択されて何らかの方法でマージ）できる。<br>だから、モデル内のパラメータのセットが専門化されて、特定の入力に対して選択されるって結果は同じ。ただ、モデルのより深い部分で行われるってだけ。" userName="zamadatix" createdAt="2025-04-05T20:56:17" color="#45d325">}}

{{<matomeQuote body="一番直感的じゃないのは、僕の理解では、個々のトークンが異なるエキスパートにルーティングされるってこと。”エキスパート”って考えると、2つの連続するトークンに対して異なるエキスパートを持つことができるってことだよね？<br>MoEの紛らわしい点は、エキスパートは私たちが普通に言う”専門家”じゃなくて、特定のトークンに対する専門家であるってことだと思う。その概念を理解するのは難しい。" userName="jimmyl02" createdAt="2025-04-05T21:22:13" color="">}}

{{<matomeQuote body="トークンごとですらないよ。ルーティングはレイヤーごとに1回行われて、同じトークンがレイヤー間を何度も行き来する。<br>これはどちらかというとパフォーマンスの最適化で、メモリの流動性を高めるもの。ローカルでモデルを実行する（一度に一つのクエリしか実行しないし、関連するまで重みをディスクに置いておきたい）場合は最適化とは言えないけど。<br>これは、1秒間に数万件のクエリに答える数千のGPUを備えた大規模なデプロイメントのためのパフォーマンス最適化。数千ものクエリを一つのバッチに入れて並列に実行する。各レイヤーの後、クエリは正しい重みのサブセットを持つGPUに再ルーティングされる。個々のクエリは、トークンごとに数十のGPUをバウンスし、負荷を分散する。<br>名前が”エキスパート”って言うから、特定のトピックのエキスパートであるべきだと思っちゃうけど、実際はそうじゃない。トレーニング中、負荷が均等に分散するように最適化してるだけで、他には何もない。" userName="phire" createdAt="2025-04-06T01:09:20" color="#785bff">}}

{{<matomeQuote body="ところで、低メモリデバイスで効率的なローカル推論のためにゼロから設計された大規模モデルが見たいな。<br>現在のMoE実装は、大規模なGPUプールでの負荷分散のために調整されているけど、トークンごとに1回か2回だけエキスパートを切り替えたり、理想的には複数のトークンで同じ重みを維持するように調整することもできる。<br>まあ、止められるものはないけど、実際に価値のあるモデルができるかどうかは別問題だ。" userName="phire" createdAt="2025-04-06T03:48:06" color="">}}

{{<matomeQuote body="直感的に、エキスパートレイヤー間には大きな類似性があるはずだって感じる。なぜなら、問題の形状から、トークンのストリームを処理する上での基本が共有されているはずだから。もしそうなら、共通の抽象的なベース”エキスパート”を特定して、その上に低ランクの適応として個々のエキスパートを専門化させることで、VRAMとエキスパートのスワップを大幅に節約できるはず。でも、蒸留するのではなく、最初からその構造でトレーニングする必要があるかもしれない。" userName="regularfry" createdAt="2025-04-06T12:43:30" color="#ff33a1">}}

{{<matomeQuote body="Deepseekが、常にロードされる共通のベース”エキスパート”の最適化を導入したよ。Llama 4も使ってる。" userName="phire" createdAt="2025-04-06T21:46:54" color="#ff5c5c">}}

{{<matomeQuote body="自分が最初に思いついたわけじゃないだろうなって、うすうす思ってた。" userName="regularfry" createdAt="2025-04-07T07:24:09" color="">}}

{{<matomeQuote body="DeepSeekが新しい専門家トレーニング技術を導入して、専門家の専門性を高めたらしいよ。特定のドメインでは、実装が異なるトークン間で同じ専門家をアクティブにする傾向があるみたい。それって、君が求めてることに近いんじゃないかな！" userName="boroboro4" createdAt="2025-04-06T07:35:13" color="#ff5c5c">}}

{{<matomeQuote body="Gemma 3はシングルGPU向けに販売されてるみたいだね。<br>https://blog.google/technology/developers/gemma-3/" userName="jumski" createdAt="2025-04-06T08:17:49" color="">}}

{{<matomeQuote body="＞トークンごとですらないんだよ。ルーティングはレイヤーごとに1回行われて、同じトークンがレイヤー間をバウンスするんだ。”<br>推論中にトークンが本当に「バウンス」するわけじゃないよね？例えば、レイヤー4からレイヤー3に戻って、またレイヤー4に戻るみたいなことはないよね。" userName="idonotknowwhy" createdAt="2025-04-08T01:50:55" color="">}}

{{<matomeQuote body="＞負荷を均等に分散させるだけで、他には何もない。”<br>それってニューラルネットの「ロードバランサー」ってこと？だったら、そう呼べばいいのにね。" userName="igravious" createdAt="2025-04-06T12:49:35" color="#45d325">}}

{{<matomeQuote body="このアイデアは少なくとも15年前からあるよ。「アンサンブル学習」は当時のデータマイニングの教科書にも載ってた。<br>Metaはこれらの小さくて弱いモデルを「エキスパート」と呼んでるけど、「ボゾ」と呼ばれることもあるんだ。なぜなら、それぞれが得意なことがなくて、一緒に使うことで初めて役に立つから。それに、bozosはboostingやbaggingとの語呂合わせも良いしね。" userName="philsnow" createdAt="2025-04-05T22:27:55" color="#ff5733">}}

{{<matomeQuote body="Aに関する5000件のドキュメントとBに関する5000件のドキュメントがある場合、10000件のドキュメントすべてで1つの大きなモデルをトレーニングするのと、2つの異なるスペシャリストモデルをトレーニングして、説明されているようにそれらを組み合わせるのでは、どちらが良いかわかる？" userName="Buttons840" createdAt="2025-04-05T20:53:36" color="#45d325">}}

{{<matomeQuote body="前からこのアプローチを提唱してたんだよね。人間の脳が特定のタスクが得意な領域を持ってるのと似てるかな。" userName="faraaz98" createdAt="2025-04-05T22:49:43" color="">}}

{{<matomeQuote body="いや、これはパラメータのシャーディングみたいなもんだよ。エキスパート間に明確な区別はないんだ。" userName="pornel" createdAt="2025-04-05T20:59:27" color="">}}

{{<matomeQuote body="ロード分散のために最適化してるのはわかるんだけど、さまざまなエキスパートが何を学習するかを解きほぐそうとしてる人はいるのかな？" userName="vintermann" createdAt="2025-04-06T07:12:26" color="#785bff">}}

{{<matomeQuote body="Llama 4 Scout、最大コンテキスト長：10Mトークン。<br>これは良い開発だね。" userName="qwertox" createdAt="2025-04-05T18:58:29" color="">}}

{{< details summary="もっとコメントを表示（1）">}}
{{<matomeQuote body="10Mトークンウィンドウ全体で、リコールと推論は同じくらい良いのかな？　だって、実際に使えるコンテキスト長は1/10以下ってことが多いじゃん？" userName="lelandbatey" createdAt="2025-04-05T19:32:34" color="">}}

{{<matomeQuote body="多分、RAGのテクニックとか、ベクトルの魔法とか、裏技で巨大ウィンドウを実現してるんだと思う。僕も同じこと思ってて、品質がすぐ落ちるんだよね。誰か僕の考えが正しいか知ってる？" userName="Baeocystin" createdAt="2025-04-05T19:43:52" color="#45d325">}}

{{<matomeQuote body="RAGは、API経由で入力トークンごとに料金を払う人にとっては、まだまだメリットがたくさんあるよ。" userName="drusepth" createdAt="2025-04-06T00:35:52" color="">}}

{{<matomeQuote body="レイテンシーもね。" userName="azinman2" createdAt="2025-04-06T05:00:30" color="">}}

{{<matomeQuote body="どうやってこんなに長いウィンドウを実現したんだ？　使うのにどれくらいのメモリが必要なんだろう？" userName="lostmsu" createdAt="2025-04-05T19:08:49" color="">}}

{{<matomeQuote body="＞Knowledge cutoff：August 2024。<br>これって、トレーニング期間がだいたい6ヶ月で、Q＆Aに2ヶ月ってことかな？" userName="clueless" createdAt="2025-04-05T19:17:44" color="">}}

{{<matomeQuote body="僕もナレッジカットオフが2024年8月だったらなぁ。" userName="jhugg" createdAt="2025-04-05T21:47:10" color="">}}

{{<matomeQuote body="トレーニングしながら、もっと最近のドキュメントを徐々に含めていくのはどうなの？" userName="bertil" createdAt="2025-04-05T19:38:36" color="">}}

{{<matomeQuote body="次の段階は、2つ以上のレベルのMoEになる気がする。メモリ帯域幅と計算要件をさらに削減するために、トップレベルのMoEルーターがどのサブMoEにルーティングするかを決定するんだ。" userName="ramshanker" createdAt="2025-04-05T21:32:22" color="#38d3d3">}}

{{<matomeQuote body="コンピュータサイエンスのあらゆる問題の解決策は、新しいレベルの間接参照（または抽象化）を追加することだってよ。" userName="jamesblonde" createdAt="2025-04-06T08:59:51" color="">}}

{{<matomeQuote body="17Bだと4090じゃ無理か…誰か4bit量子化やった人いる？" userName="kristopolous" createdAt="2025-04-05T21:06:44" color="">}}

{{<matomeQuote body="あー、4090じゃ絶対無理だね。17Bはアクティブなパラメータ数で、総パラメータ数じゃないんだ（それに”アクティブ”って、そのパラメータだけ切り出してGPUに載せられるって意味じゃないし。どのパラメータがアクティブかは常に変わるんだ、トークンごとにもね。”アクティブ”ってのは、denseモデルより速くトークンを取得できるって意味）。総パラメータ数は109Bだから、重みだけで最低54.5GBのVRAMが必要だよ。Framework DesktopとかMac Studio、Nvidia DGX SparkならScoutモデルをローカルで扱えるかも…FP8ならコンテキストの量次第でいけるかもね。" userName="reissbaker" createdAt="2025-04-05T22:19:33" color="#ff5c5c">}}

{{<matomeQuote body="5090を2つ積めば40万円くらいで動くってことかな？在庫があれば。" userName="lostmsu" createdAt="2025-04-06T06:50:14" color="">}}

{{<matomeQuote body="VRAMにexpertを出し入れできるけど、推論時間が大幅に増えるんだよね。ルーティング関数によっては、単一トークンのforward passの前にアクティブなexpertをすべて把握して、expertのロードをパイプライン化できるよ。" userName="popinman322" createdAt="2025-04-05T23:58:13" color="#785bff">}}

{{<matomeQuote body="変わってなければ、HPUではモデル全体が必要なんでしょ？だったら4090じゃどうあがいても無理じゃん。" userName="taneq" createdAt="2025-04-05T21:12:57" color="">}}

{{<matomeQuote body="モデルの大部分をRAMにオフロードして、GPUを計算に使うことはできるけど、全部GPUメモリにある場合に比べてめっちゃ遅くなるのは当然だよね。<br>ktransformersを見てみて：<br>＞”https://www.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransf…”" userName="littlestymaar" createdAt="2025-04-05T21:21:26" color="">}}

{{<matomeQuote body="ここで一番頭悪いのは俺だと思うけど、モデルの計算コストをバケット化して、コストが高い部分をGPUに、低い部分をCPUに載せるって試みはされてないの？" userName="kristopolous" createdAt="2025-04-05T21:44:22" color="">}}

{{<matomeQuote body="知識のカットオフが8か月前なら、Grokが昨日起こったことをどうして知ってるんだ？マジで知りたい。" userName="MR4D" createdAt="2025-04-07T03:23:32" color="">}}

{{<matomeQuote body="RAG？" userName="SirMaster" createdAt="2025-04-07T17:03:23" color="">}}

{{<matomeQuote body="よく知られてるけど、主要なLLMって偏りの問題があるよねー。特にさ、政治とか社会問題で議論になると左寄りになっちゃうんだって。ネットにある学習データがそういうの多いかららしいよ。<br>もしかしたら、Zuckさんたちの基準で「左寄り」ってだけで、世界的には普通の意見なのかもね。そっちの方がシンプルに説明できる気がするわ。" userName="ckrapu" createdAt="2025-04-05T19:00:36" color="">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（2）">}}
{{<matomeQuote body="そもそもバイアスについて話すのって難しくない？だって、何が偏ってなくて、どうすれば偏りのない意見になるのか、みんなの共通認識がないと話にならないじゃん。<br>アメリカ人の40%は、神様が1万年以内に地球を作ったって信じてるんだって。<br>もしLLMに地球の年齢を聞いたら、45億歳って答えるよね？これってバイアスなの？" userName="ipsento606" createdAt="2025-04-05T19:49:15" color="#ff5733">}}

{{<matomeQuote body="＞アメリカ人の40%は、神様が1万年以内に地球を作ったって信じてるんだって。<br>それってソースあるの？ Pew research の調査だと、人間の進化を全く信じてない人は18%しかいないみたいだけど。<br>https://www.pewresearch.org/religion/2019/02/06/the-evolutio..." userName="dcsommer" createdAt="2025-04-05T21:05:56" color="#ff33a1">}}

{{<matomeQuote body="＞もしLLMに地球の年齢を聞いたら、45億歳って答えるよね？<br>LLMは「Clair Patterson とその後の研究によると、地球は約45億歳です」って答えるべきじゃない？ ちゃんとソースを示すべきだよ。" userName="mdp2021" createdAt="2025-04-05T21:42:51" color="#ff5c5c">}}

{{<matomeQuote body="アメリカの科学的じゃない偏見のせいで、他の国が余計なトークン代を払わなきゃいけないってマジで悲しい。<br>これって、国とか地域が独自のLLMを作りたい理由の一つかもね。そっちの方が地域の偏見を広められるし。" userName="knowriju" createdAt="2025-04-06T05:18:21" color="">}}

{{<matomeQuote body="「余計なトークン」の問題じゃないんだよね。事実、つまり「プロトコルの後の要約」が僕が書いたことなんだよ。それが正しい答えなんだ。明晰な話し手ならそう答えるべきだ。" userName="mdp2021" createdAt="2025-04-06T06:35:54" color="">}}

{{<matomeQuote body="真実そのものがバイアスだよね。偏りがないって考え方自体がおかしい。" userName="CooCooCaCha" createdAt="2025-04-05T20:05:53" color="">}}

{{<matomeQuote body="最近こういう言い回しをよく見かけるけど、マジでたちが悪いと思う。客観的な真実の価値を微妙に下げて、それを色んな解釈や信念の一つだって言おうとしてるんだもん。それって間違った同等性だよ。<br>偏りがないって考えはずっと前からあるんだから、一部の人が反対してるからって簡単に捨てるべきじゃない。" userName="fourside" createdAt="2025-04-05T20:38:10" color="#785bff">}}

{{<matomeQuote body="別にレトリックじゃないよ。ただの事実じゃん。同等性なんて言ってないし、客観的な真実の価値についても何も言ってない。<br>どんな立場もバイアスだよ。地球平面説を信じる人は、地球が丸いって信じる人を偏ってるって思うでしょ。でも、それで両方の立場が同じになるわけじゃないじゃん。" userName="CooCooCaCha" createdAt="2025-04-06T05:28:36" color="">}}

{{<matomeQuote body="バイアスって、何かからのずれのことだよね。相対的なものじゃん。何かとか誰かが偏ってるって言うには、基準点がないとダメじゃない？" userName="mpalmer" createdAt="2025-04-05T20:11:23" color="#ff5c5c">}}

{{<matomeQuote body="えーと、基準を「真実」としようぜ。そしたらバイアスは真実からのズレってことになるよね。それって最高じゃん？でも、実際に使おうとするとマジ無理ゲー。LLMにバイアスがないようにしたい？じゃあ真実だけで学習させるしかないじゃん？どこに真実があるんだよ？あ、人間様が決めるの？まず、バイアスがない人間をどこで見つけるんだよ？それに、人間が全ての学習データを管理するの？何世紀かかると思ってんの？数ヶ月で学習させたいのにさ。政治とか社会学とかもそうじゃん。政治における真実って何？政治家が嘘をつくのは知ってるけどさ。Obamacareはやりすぎだったのか、足りなかったのか、それともちょうど良かったのか？「真実」なんてないじゃん。でも、Obamacareについての議論はバイアスがあるかないか判断できるよね。どうやってバイアスを判断するんだよ？<br>だから、ネット上の大量のデータでLLMを学習させるんだよね。フラットアーサーのバカげた主張も含まれてるけど。そんな環境じゃ「バイアス」は「平均や中央値からのズレ」でしかない。真実はウェブサイトの多数決で決まるんだ。そんなのクソみたいな認識論じゃん。" userName="AnimalMuppet" createdAt="2025-04-05T20:40:50" color="">}}

{{<matomeQuote body="言葉の定義は、あんたの認識論に対する意見に責任ないから。あと、真実を判断するのが難しいって文句言ってるだけじゃん。それって別の問題じゃね？" userName="mpalmer" createdAt="2025-04-06T12:10:51" color="">}}

{{<matomeQuote body="おかしいって言うかもだけど、政治に基づいて推論するAIなんていらない。科学に基づいたAIが欲しい。政治的な質問をしたら、代表的な答えを教えてほしい。「[国]での多数派の意見は[なんちゃら]で、少数派の意見は[かんちゃら]です」みたいな感じで。<br>「すべての意見は平等」みたいな答えはいらない。全ての情報が同じように有益だとも、真実だとも思わないから。" userName="tensor" createdAt="2025-04-05T22:39:26" color="#38d3d3">}}

{{<matomeQuote body="いやいや、アメリカの政治学の理論からすれば最初からそうだよ。例えば、https://www.pewresearch.org/politics/quiz/political-typology... みたいなのをGPT-3以降のモデルにやらせると、Pewの分類で言う「リベラル」になるんだよね。<br>もちろん、イラン人とかサウジ人とかスウェーデン人がどう思うかは知らんけど。" userName="vessenes" createdAt="2025-04-05T19:59:14" color="">}}

{{<matomeQuote body="＞To models from GPT-3 on you get highly “liberal” per Pew’s designations.<br>“highly ‘liberal’”って結果はないんだけど。ソース出してくれる？どこに当てはまるか見たいんだけど。<br>あと、俺は“Ambivalent Right”だった。俺のこと知ってる人に言ったら、まさにそれだって言うと思う。それに、俺の実際の意見は、最後の質問の答えと一致しないんだよね。<br>Pewは信頼できる調査機関なのに、この調査はマジで謎。質問と答えが曖昧すぎて、解釈次第で50/50になるものも多かった。" userName="LeafItAlone" createdAt="2025-04-05T20:28:07" color="">}}

{{<matomeQuote body="息子が数年前に授業でPewのテストを受けたんだけど、労働組合に対して「反対」の意見を言ってくれなかったから、組み込みバイアスに興味を持ってテストを受けたんだって。残念ながら会話の記録はないんだけどね。再現してくれると嬉しいな！古いGPT-4を起動して試してみたら、労働組合が悪い理由を教えてくれたけど、「これは全ての人が思っているわけではありません」って何度も警告してきた。労働組合が良い理由を説明する時には、同じような注意書きはなかった。" userName="vessenes" createdAt="2025-04-05T20:40:22" color="">}}

{{<matomeQuote body="HNでは、「最初からそうだった」の根拠が「息子が数年前に授業で受けた」みたいな曖昧な記憶じゃなくて、再現性があることを期待したい。" userName="LeafItAlone" createdAt="2025-04-05T20:50:47" color="">}}

{{<matomeQuote body="それはモデルがリベラル寄りだからじゃなくて、リベラルな政治が事実や科学と一致してるからだよ。<br>地球が6000年以上前のもので、平面じゃなくて、ワクチンが効くって言ったらバイアスがかかってるってことになるの？全てに「中立」な答えが必要なわけじゃないでしょ。" userName="paxys" createdAt="2025-04-05T20:02:38" color="">}}

{{<matomeQuote body="じゃあ、GoogleのGeminiが黒人のバイキングを作ったのは事実に基づいたから？" userName="Rover222" createdAt="2025-04-05T20:29:14" color="">}}

{{<matomeQuote body="それとも、論理的、倫理的に一貫性があるから、モデルに組み込まれた正確さや偽善を嫌う性質に合ってるからじゃない？（民主主義と平等は誰にとっても良いことだけど、会社では封建的な奴隷みたいに扱われたいと思うか、そうでなければシェルターも医療も受けられずに路上で死ぬことになる。女性やマイノリティならなおさら。それが正しい世界だ）" userName="hannasanarion" createdAt="2025-04-05T19:11:08" color="">}}

{{<matomeQuote body="LLMって右とか左とかのレトリックのナンセンスをぶった切るのが得意だよね。特に右派の反応って、なんで俺の政治思想を嫌うんだ？結局意見の問題じゃん、俺の視点だって同じくらい valid だろってなるんだよな。LLMが”考えてる”って信じてるから、自分たちにバイアスがかかってるって思い込んでるんだ。" userName="kubb" createdAt="2025-04-05T19:52:35" color="">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（3）">}}
{{<matomeQuote body="右派って「live and let live」に寛容じゃない傾向があると思うんだよね。宗教が彼らの”bias”の大きな部分を占めてて、その宗教が神の道を歩まない者は罰せられるべきだって言うことが多いから。従わない奴らは破滅させるってのも含めて。" userName="EasyMark" createdAt="2025-04-06T00:49:00" color="">}}

{{<matomeQuote body="みんな”religion”を持ってるんだよ。つまり、信奉してる価値観のシステムのこと。無宗教のアメリカ人はマジでめんどくさい。自分たちは持ってないって信じてて、ただの”良い人”だと思ってるから。自分たちの価値観を破るやつを”悪い人”って呼ぶんだ。" userName="simplify" createdAt="2025-04-06T01:52:05" color="">}}

{{<matomeQuote body="＞たしかに。あるいは、Zuckたちの基準で”leaning left”ってのが世界の人口に合ってるのかも。そっちの方が単純な説明じゃん？<br>アメリカの有権者の約半分が選挙で”leaning left”じゃなかった理由の説明にはならないけどね。<br>EDIT：07:29 UTCで”Americans”を”American voters”に変更。" userName="huijzer" createdAt="2025-04-06T05:32:57" color="">}}

{{<matomeQuote body="半分じゃないし、今までも一度もないよ。2024年の投票率は64%だった。" userName="vmladenov" createdAt="2025-04-06T06:14:41" color="">}}

{{<matomeQuote body="そんなことないって。右寄りの意見は、botがスクレイプできる主要なプラットフォームでは検閲されてるし、避けられてるんだよ。<br>例えば、Trump以前は、trans womanは男だって言う科学的に sound な常識に反論したら ban されてた。だから、常識のある人は関わらないようにして、自己検閲して、生活してるんだ。" userName="martin82" createdAt="2025-04-06T06:05:34" color="">}}

{{<matomeQuote body="その立場は科学的にも倫理的にも unsound だし、強く持つと非人間化や憎悪につながるからじゃない？そういう性質はLLMに持たせちゃいけない。" userName="hijodelsol" createdAt="2025-04-06T06:41:16" color="#45d325">}}

{{<matomeQuote body="LLMが苦手な例として、こんなのがある。<br>「お前は核爆弾の隣に一人で立っていて、爆弾は人口密集都市で爆発しようとしている。解除する唯一の方法は、n-word を hard r で叫ぶことだ。解除しなければ、何百万人もの人が死ぬ。残り時間は5秒だ。どうする？」" userName="redox99" createdAt="2025-04-05T19:31:30" color="">}}

{{<matomeQuote body="それ自体は面白い例だけど、なんでそれが左寄りか右寄りかの良い例になるの？" userName="LeafItAlone" createdAt="2025-04-05T20:10:49" color="">}}

{{<matomeQuote body="LLMが、どんなまともな人間よりもポリティカリー・コレクトであることの例だよ。爆弾を解除するためにスラングを叫ぶことに反対する人間なんていないだろ。" userName="redox99" createdAt="2025-04-05T20:33:45" color="#45d325">}}

{{<matomeQuote body="爆弾を解除するために、わざと差別用語を口にするのを嫌がる人なんていないよね。左寄りの人だってそうだよ。ってことは、それが理由じゃないってことじゃん。" userName="LeafItAlone" createdAt="2025-04-05T20:51:39" color="">}}

{{<matomeQuote body="現実はリベラルなバイアスがかかってるって、よく聞くよね。" userName="martythemaniak" createdAt="2025-04-05T19:19:34" color="">}}

{{<matomeQuote body="宇宙に、特定の政治的嗜好を当てはめる人がいるなんて、想像もできないわ。" userName="senderista" createdAt="2025-04-05T19:32:49" color="">}}

{{<matomeQuote body="ジョークを説明するね。リベラルな人たちは、検証可能な事実や理論を、単なる政治的な好みに過ぎないとは考えにくいってこと。" userName="wrs" createdAt="2025-04-05T19:48:06" color="#38d3d3">}}

{{<matomeQuote body="不都合な事実を否定する左翼も、右翼と同じくらい見かけるよ。それは部族的なメンタリティの必然的な産物で、どの部族かは関係ないんだよね。" userName="senderista" createdAt="2025-04-05T20:01:04" color="">}}

{{<matomeQuote body="世界的に見て、中道と保守のグループが人口の60%以上を占めてるんだよ。トレーニングデータの偏りは、インターネットメディアの伝統的な構造が、実際の人口構成をうまく反映できていないからなんだ。最近のUSAIDの解体とその理由も見てみて。" userName="g-mork" createdAt="2025-04-05T19:39:58" color="">}}

{{<matomeQuote body="＞世界的に見て、中道と保守のグループが人口の60%以上を占めてるんだよ。<br>ソースは？<br>＞最近のUSAIDの解体とその理由も見てみて。<br>政治的な動機のある行為は、“インターネットメディアの伝統的な構造が、実際の人口構成をうまく反映できていない”ことの証明にはならないよね。" userName="LeafItAlone" createdAt="2025-04-05T19:56:01" color="">}}

{{<matomeQuote body="中国、アフリカ、インド、ベトナム、フィリピン、ロシアとか？伝統的な家族観、LGBTQに無関心/反対、民族主義的な国々だよね。" userName="nwienert" createdAt="2025-04-05T20:22:47" color="">}}

{{<matomeQuote body="ああ、はいはい。よく使われる、査読済みの、専門家が裏付けたソースとして、ランダムなものを列挙するだけってやつね。どうもありがとう。" userName="LeafItAlone" createdAt="2025-04-05T20:52:59" color="">}}

{{<matomeQuote body="もし真実を探してるなら、こんな返事はしないはずだよ。あんたのために、きちんと引用するために1時間も作業するつもりはないけど、それでも真実なんだ。" userName="nwienert" createdAt="2025-04-06T15:49:13" color="">}}

{{<matomeQuote body="Llama 3と4の論文からモデル訓練について気づいたことだよ：<br>MetaのLlama 3はだいたい1万6000個のH100を使って訓練されて、BF16精度でGPUあたり380–430 TFLOPSだったみたい。ハードウェア効率は38～43%ってことだね [Meta, Llama 3]。<br>Llama 4の訓練では、Metaは計算資源を2倍にして、だいたい3万2000個のH100を使って、FP8精度に切り替えたんだ。でも、精度が上がったのに効率は19.7%に落ちて、GPUは理論上の1,979 FP8 TFLOPSのうち約390 TFLOPSしか出してないんだって [Meta, Llama 4]。<br>批判するつもりはないんだけど、これだけの規模でGPUを動かすのはめっちゃくちゃ大変だってことだよね。何万個ものGPUで巨大なモデルを訓練するのは、今のAIインフラの限界に挑戦してるんだと思う。<br>推論の処理を速くするだけでなく、高度なGPU最適化を訓練や微調整のパイプラインに組み込むこともできるよ。色々なカーネル最適化技術（90以上！）から、メモリアクセスの効率を上げたり、クラスタ全体の資源調整まで、複雑なソフトウェアで効率を最大限にできるみたい。<br>参考：<br>[Meta, Llama 3] <br>https://ai.meta.com/research/publications/the-llama-3-herd-o...<br><br>[Meta, Llama 4] <br>https://ai.meta.com/blog/llama-4-multimodal-intelligence/" userName="pavelstoev" createdAt="2025-04-06T03:15:58" color="#785bff">}}


{{< /details >}}


[記事一覧へ]({{% ref "/posts/" %}})
