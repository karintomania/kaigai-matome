+++
date = '2025-05-25T00:00:00'
months = '2025/05'
draft = false
title = 'チョムスキーは語る！ ChatGPTの良いところとは？ 2023年版'
tags = ["AI", "ChatGPT", "言語学", "LLM", "チョムスキー"]
featureimage = 'thumbnails/color4.jpg'
+++

> チョムスキーは語る！ ChatGPTの良いところとは？ 2023年版

引用元：[https://news.ycombinator.com/item?id=44089156](https://news.ycombinator.com/item?id=44089156)




{{<matomeQuote body="ここのChomskyに関するコメント、知的レベルが低いのはマジ驚くわ。LLMがすごいし便利ってのは認めつつも、この10年のAIの進化が、人間の知性そのものやその成り立ちを深〜く理解させてくれてるかって言うと、そうじゃないんだよね。LLMの仕組みは完全にわかってるけど、脳はまだ。小型化の努力で、自然が偶然見つけたようなワザが再発見されるかもって可能性はあるけど、これはまだ確定じゃないんだよね。" userName="atdt" createdAt="2025/05/26 01:19:36" color="#38d3d3">}}




{{<matomeQuote body="＞ AI this decade have not...ってやつだけど、ちょっと反論かな。人間の知性そのものの理解にはつながらなくても、そもそもそんなもんがあるのかって疑問に思うようになったんだよね。Physicsみたいに、人間がどう考えて決めるかを説明できるシンプルで美しい自然法則なんてないのかも。CNNs見てると、人間がやってることと似てる気がするんだ。たぶん全部、なんか messy に進化してきた基盤から emergent した特性ってだけかもね。この10年のAI開発から学んだ一番デカいことは「人間って、やっぱそんな特別じゃないのかもな」ってこと。Physicsでも人間を特別視する間違いを犯してたんだよ。" userName="loveparade" createdAt="2025/05/26 02:29:36" color="#ff33a1">}}




{{<matomeQuote body="＞ I guess humans really aren’t so special after all<br>この意見には逆に反対したいな。「人間は特別じゃない」って？ 自然に進化してきた脳の構造で、Van Goghの絵やBoeing 747やA380みたいな飛行機、宇宙旅行、Pillars of Creationの写真まで…これ全部、たった数センチの脳でやったんだぜ？ 俺はそれがマジでヤバいと思うね。" userName="user_7832" createdAt="2025/05/26 04:54:30" color="#ff5c5c">}}




{{<matomeQuote body="「人間って、やっぱそんな特別じゃないのかもな」<br>これ、俺にとってはマジで crazy な意見だよ。何と比べてそう思うわけ？ 俺たちが作った機械と？<br>宇宙で同レベルの知性体を見つけるまでは、人間が本当に特別だって言えると思うんだけどね。" userName="OccamsMirror" createdAt="2025/05/26 06:34:54" color="">}}




{{<matomeQuote body="学者が最近のサブシンボリックなブレークスルーを、「知性」を学べるモデルなのにすごく軽視してるのがマジで意味不明だわ。まるで生化学者が脳を見て電気化学反応だけだって結論づけるみたいだ。emerge の可能性を完全に無視してるんだよね。AGI はまだだけど、LLM には言語発達や進化のヒントがあるはず。transfer learning は普遍文法論争にどう影響？ LLM と人間の不変性は？" userName="PeterStuer" createdAt="2025/05/26 07:21:22" color="#785bff">}}




{{<matomeQuote body="言語は俺たち自身の言葉で理解することが重要だと思うんだ。LLM のロジックは、俺たちにとってはエイリアン技術みたいだよ。Chomsky のミニマリストプログラムは失敗したけど、言語学の進歩は無駄じゃない。Prolog、Theorem provers、type theory、category theory が、LLM が C++ で全部生成できるから無駄、って言うのが間違いなのと同じ。言語学の技術で知識をgroundできるし、LLM のどこか暗い隅っこで統合されてるかも。科学と人文学の隔たりは深いかも？計算機科学は人文学？" userName="_glass" createdAt="2025/05/26 08:35:20" color="#785bff">}}




{{<matomeQuote body="＞ it has made me question whether such a thing even exists<br>この話だけど、こないだ Reddit で、リカバリーフレーズを変なとこに入力して仮想通貨失った人の投稿を読んだんだ。LLM の知性を疑うのは、危険なウェブサイト開いたりヤバいこと読んで実行したりするかもって心配だからだよね。でも、ここに real な人間が全く同じことやってるんだよ…<br>＞ I guess humans really aren’t so special after all<br>彼らは特別じゃない、その通りだ。でも、現在の LLM じゃまだそこまで到達してないし、人間の脳を真似るのが一番いいやり方だとは思わないけどね。" userName="csomar" createdAt="2025/05/26 05:47:06" color="#ff33a1">}}




{{<matomeQuote body="＞ It fully ignores the potential for emergence.<br>emerge には科学的なのと空虚なのがある。科学的なのは下のレベルに還元可能。AI 論争での「emergence」は、理論や説明がない魔法の言葉。これは理論構築が目的の科学においては、神の不在を埋めるごまかしみたいだ。Chomsky が言う通り、こういう議論は無意味。" userName="Barrin92" createdAt="2025/05/26 07:38:04" color="#785bff">}}




{{<matomeQuote body="＞ But here we have real humans doing the exact same thing...<br>こういう間違いをしないようにシステムを設計するのが、モチベーションなんだろ。そうじゃなきゃ、一体何のために、って話だよ。" userName="krige" createdAt="2025/05/26 05:53:14" color="">}}




{{<matomeQuote body="＞今世紀のAIの大きなブレークスルーは、少なくとも今のところ、僕たち自身の知性やその構成要素の理解を深めてないんだってさ。人が自分の権威やコントロールを安売りして、最小限の労力で最大限の成果を得ようとする錯覚や意欲（これって自己回帰モデルなんかと同じだよね！）は、すごく驚きの洞察だったな。" userName="rf15" createdAt="2025/05/26 05:01:08" color="">}}




{{<matomeQuote body="SFでエイリアンが人間型だったり、知性・意識の定義って結局「僕たちみたいなもの」になっちゃうんだよね。客観的な定義がないからさ。他の存在がどれだけ僕たちと違う知性を持ってても、僕たちはそれを知性として認識できないかも。特にLLMは、人間とのコミュニケーションを真似るように作られてるから、出力が人間っぽいと、たとえ意識がなくても（たぶん意識ないけど）、僕たちは知性に見えちゃうんだ。だから、人間って特別な存在なのかもね。" userName="strogonoff" createdAt="2025/05/26 08:42:22" color="#ff5733">}}




{{<matomeQuote body="ChomskyがLLMを批判してる核心部分って、不可能言語も可能言語と同じくらい簡単に学習しちゃうから、人間の知性の理解には役立たないってことなんだよね。でも、去年出た論文（Mission: Impossible Language Models, Kallini et al.）で、LLMは不可能言語をそんな簡単に学習しないって証明されてるんだ。だから、Chomskyがこの記事で言ってることは、これで全部ひっくり返されちゃうんだよ。" userName="xron" createdAt="2025/05/26 11:43:48" color="#ff33a1">}}




{{<matomeQuote body="Star Trekでどこにでもヒューマノイドが出てくるのは、エピソード制作の都合上、明らかな実用性からだよね。<br>予算全部をソルトバンパイアに使っちゃって、その後回復できなかったんだ。" userName="rightbyte" createdAt="2025/05/26 10:34:19" color="">}}




{{<matomeQuote body="＞科学的な創発（emergence）なんて聞いたことないな。あるのは哲学的な創発と、その逆の存在論的還元主義だよ。多くの人は哲学的な創発を直感的に信じてるけど、よく見るとそれは人間の心が複雑な現象に抽象的なカテゴリーを当てはめるせいで起こる錯覚なんじゃないかなって思うんだ。根底にある現象とは違う、実際の効果だと勘違いしてるだけじゃないかってさ。" userName="energy123" createdAt="2025/05/26 11:24:51" color="#ff5c5c">}}




{{<matomeQuote body="＞人間の思考や意思決定を説明する、物理学にあるようなシンプルで美しい自然法則はないのかもね。って言ってるけど、物理学って自然界を記述しようとしてるんじゃない？人間の心も物理プロセスで説明できるなら、理論的には物理学で知性も説明できるはずだよ。すごく難しいかもしれないけど。物理法則こそ自然法則だよ。君は「自然法則」って、あのゴチャゴチャした生命に関するものだけを指してるんじゃないかな。" userName="mykowebhn" createdAt="2025/05/26 09:49:22" color="">}}




{{<matomeQuote body="ホントその通りだね。採用とか投資とか医療診断とか、多くの「AI」製品がユーザーの代わりにLLMに決定を任せてるって考えるとヤバいよ。LLMの採用決定って、サイコロ転がすよりマシなの？サイコロは理性的なフリしないけど、LLMはもっともらしい理由を説明するパラグラフを作る。それは推論に見えるけど、決定と原因の繋がりはないんだ。真の原因を示す実体がない、単なる説明の真似事なんだよ。" userName="OccamsMirror" createdAt="2025/05/26 06:44:58" color="#38d3d3">}}




{{<matomeQuote body="自分自身や他の人の間違いが、僕たちを形作り、理解を深めてくれるんだ。そもそも完璧って、一体何なんだろうね？" userName="johnisgood" createdAt="2025/05/26 12:03:47" color="">}}




{{<matomeQuote body="言語学で働いてる者として、君が何を言ってるのかよく分からないな。Minimalismは例外だらけじゃないし（具体的な例があれば教えて）、古い理論Government and Bindingをシンプルにするために作られたんだよ。" userName="suddenlybananas" createdAt="2025/05/26 12:06:55" color="#ff5c5c">}}




{{<matomeQuote body="正直、彼のこのエッセイしか知らないんだよね。でもこれはずっとお気に入りなんだ。他にHumeでおすすめある？" userName="globnomulous" createdAt="2025/05/26 17:41:43" color="">}}




{{<matomeQuote body="神経科学者だよ。人間の思考は物理みたいな単純法則じゃなく、ごちゃごちゃした基質から生まれるっぽい。<br>脳マップ見ると重みが大事で、生きてる時はリアルタイムで変わる。Hydranencephalyの人も普通に生活してるから、コネクトームが人間らしさかも。<br>Memristorが鍵って説もあるけど、PC根本から変えないと無理。<br>この10年でAI見て「人間って特別じゃないかも」と思った。生物学者もそう思うらしい。<br>でも結局人間は特別だよ。質問を問いかけたのは人間だけ。生物学はまだ入口だね。" userName="Balgair" createdAt="2025/05/26 15:02:29" color="#ff5c5c">}}




{{<matomeQuote body="ごちゃごちゃした基質から生まれる「創発的なふるまい」の自然法則を、まだ単純に見つけられてないだけって可能性もあるよね。" userName="lenkite" createdAt="2025/05/26 03:31:05" color="">}}




{{<matomeQuote body="人間の知能が分散してるの、AI分野含めて見落としがちだよね。「mixture of experts」聞いた時はワクワクしたよ。<br>AGIへの道は、人間の進化を人工的に再現することだと思う。小さいモデルから始めて、成功した大きいモデルが「繁殖」して新しいモデルを教える、みたいな。<br>いろんなモデル構成が共存すれば、脳の専門化みたいなことも起きるかも。" userName="maaaaattttt" createdAt="2025/05/26 11:23:37" color="#38d3d3">}}




{{<matomeQuote body="でもさ、まだ「C++で全部書ける”」みたいなLLMはないよ。新規プロジェクトで定型コード書くのは得意だけど、コードが大きくなると助けが必要な時に何度も間違えちゃうんだよね。" userName="vrighter" createdAt="2025/05/26 09:37:18" color="">}}




{{<matomeQuote body="完璧すぎるシステムは創造性を示さないと思うな。大胆な新しいアイデアを出すにはリスクが必要だから、何か新しいことを発明できるシステムは、結局悪い選択もするだろうね。" userName="sayamqazi" createdAt="2025/05/26 09:15:26" color="">}}




{{<matomeQuote body="たとえるなら、定理証明機の結果は使えるんだけど、数学者はわざわざ機械の長くて分かりにくい証明を、人間に読める簡潔な証明に翻訳しようとするんだ。<br>それは、「生産的な行動”」が終わった後でも、その証明が数学の理解に役立つから。<br>機械と人間のこういう協力関係は、前よりいい感じ。生産的な行動を早くできるし、数学者は探してる証明が存在するのか疑う必要もなくなったんだから。" userName="nz" createdAt="2025/05/26 18:43:35" color="#ff5c5c">}}




{{<matomeQuote body="内省とか興味ないんだよね。確定申告をちゃんとやりたいだけ。" userName="krige" createdAt="2025/05/28 04:05:16" color="">}}




{{<matomeQuote body="LLMが素晴らしいし便利っていう点に、彼がどこで反対してるか分からなかったよ。<br>「プログラムが人間能力を超えるか？ パフォーマンスなら絶対イエス”」って部分は、LLMが素晴らしい・便利って言ってるようにしか読めないけどな。<br>彼の話の核は「純粋な工学”」と「科学”」の区別にあると思うんだ。彼は科学サイドについて話してるんでしょ。" userName="godelski" createdAt="2025/05/26 02:51:55" color="#785bff">}}




{{<matomeQuote body="同意。ファジィ推論の良い統一理論がないのが根本問題。<br>LLMsは実践できるけど理由は不明。<br>スケールじゃなく理論が必要ってChomskyは正論。<br>目標が不明確。<br>どんな答え方を論理推論で表現するか、分からないんだよね。" userName="js8" createdAt="2025/05/26 09:29:45" color="#ff5c5c">}}




{{<matomeQuote body="Chomskyは知性はシンボル、Asimovは統計が基盤って議論が面白かったんだ。<br>LLMは純粋な統計モデルで、データから次のトークンを予測するだけ。<br>内部にシンボル推論はないからChomsky的には知性じゃない。<br>でも、LLMは複雑な芸当で、実用的価値がある。<br>知性の本質って何？って考えさせられるね。<br>2歳の子供と比べるとその違いが顕著だよ。" userName="papaver-somnamb" createdAt="2025/05/26 03:01:56" color="#45d325">}}




{{<matomeQuote body="それは違うよ、シンボルは統計から生まれるんだ。<br>Imagenet分析やLLMsのアブレーション実験見てみなよ、統計からシンボル的な概念が出てきて使われてるんだ。<br>厳密な論理じゃないけど、情報を圧縮してシンボル操作はしてるんだよ。" userName="sdwr" createdAt="2025/05/26 03:14:50" color="#45d325">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="多くのシンボル操作は定型的で、知性じゃないってことかな。<br>Chomskyのバナナの例みたいに、文章解析の難しさは昔からある。<br>重みとかプログラムなしで、シンボルモデルがこういう例にどう対処するかは見てみたいね。" userName="ggm" createdAt="2025/05/26 06:15:41" color="">}}




{{<matomeQuote body="知性なしで複雑な推論ができるのか？っていう問いが面白い。<br>LLMは一芸以上、特にGemini 2.5やClaude 4は推論してるように見える。<br>もし知性がなくても推論できるなら、知性の定義が変わるかもね。" userName="dahcryn" createdAt="2025/05/26 07:15:36" color="#38d3d3">}}




{{<matomeQuote body="＞Gemini 2.5やClaude 4は推論してるって主張できる。<br>彼らは絶対に推論してないよ。<br>俺たちが勝手に出力に意味をマッピングしてるだけ。<br>失敗パターンがそれを物語ってる。<br>エミュレートしてるだけ。<br>スケールやファインチューニングで解決するって考えるのは無謀だね。" userName="namaria" createdAt="2025/05/26 09:21:27" color="#ff5c5c">}}




{{<matomeQuote body="人間だって間違えるし、LLMとの違いって何？<br>LLMsも人間みたいな後付けの正当化するって研究もあるんでしょ？" userName="jamincan" createdAt="2025/05/26 12:14:26" color="">}}




{{<matomeQuote body="ランダムな人間相手にテストしないように、LLMもランダムにテストするのはおかしいってことかな。" userName="namaria" createdAt="2025/05/26 13:56:08" color="">}}




{{<matomeQuote body="人間の推論能力って、何百万年も前の祖先が間違った決断で死んで、正しい決断したやつが生き残ってできた統計的な結果だと思うんだよね。<br>生存者バイアスってやつ。<br>俺たちは、成功した祖先の統計の産物なんだ。" userName="DarknessFalls" createdAt="2025/05/27 03:27:19" color="#ff5c5c">}}




{{<matomeQuote body="＞Chomskyはシンボル、Asimovは統計が基盤<br>当時そうだったか知らないけど、その二つの見方は完全に等価だと思うよ。" userName="marcosdumay" createdAt="2025/05/29 18:12:31" color="">}}




{{<matomeQuote body="たぶん次に俺たちが問いかける疑問は”統計モデルに記号入力を与えたらどうなるか？”ってことで、その答えは記号出力が得られる、みたいだね。<br>もっと変なのは、統計モデルに記号入力を与えるっていう行為が、文脈を構築させて、ある程度の”理解”のレベルに依存するやり方で記号出力を形作るってこと。<br>俺たちは生の記号データでこのモデルを”訓練”して、人間がコードに文字とか単語とか似たようなものを埋め込んだことなんて一度もないのに、固有の意味構造を抽出するんだ。まるでChomskyの捉えどころのない普遍言語が意味構造そのものみたいだよ。" userName="tmzt" createdAt="2025/05/26 11:00:47" color="#45d325">}}




{{<matomeQuote body="＞ 知性は記号推論に根差してるっていうChomskyと、統計的基盤だって主張したAsimovの面白い議論があったよ（あれ、これは意図的じゃなかった ;）。<br>Chomsky vs Norvig<br>https://norvig.com/chomsky.html" userName="Xmd5a" createdAt="2025/05/26 09:29:36" color="#45d325">}}




{{<matomeQuote body="＞ とても魅力的だね<br>そうだね、だって擬人化って俺たちの生物学にがっちり組み込まれてるからね。点2つと弧線だけで、全ての人間に幸せな気持ちを引き起こすんだ。:)<br>＞ 実践的な応用と本当の価値<br>それは議論の余地ありだね。今のところLLMで画期的に役立つ応用は見つかってないよ。俺たちは信じたいんだ、だってハッピーにしてくれるから。でも結果はまだ出てないんだよね。" userName="otabdeveloper4" createdAt="2025/05/26 06:51:34" color="#ff5733">}}




{{<matomeQuote body="理性の声だね。そして、いつものことながら、理性の声は勢いよく無視されてる。大きな利益と、生成された嘘を通じて支配する夢は抗いがたい。<br>とりわけHNのコメントスレッドは、もっと良く知ってるはずの人たちでさえ、ぞろぞろとそれに騙されてる様子を示してるよ。<br>実際、このスレッド自体が、Chomskyの議論がいかに聞く耳を持たれてないかを示してるんだ。" userName="zombot" createdAt="2025/05/26 06:33:58" color="#ff33a1">}}




{{<matomeQuote body="自動監視を通じた支配も忘れちゃダメだ。大量の分析官のオフィスなんて必要とせずに、市民が規則を逸脱してるかどうか検出するための、なんて素晴らしいツールを作り出したんだろう（皮肉）。" userName="OccamsMirror" createdAt="2025/05/26 07:37:26" color="#ff5c5c">}}




{{<matomeQuote body="3.35時間のChomskyのML Street Talkのインタビューだよ。<br>https://youtu.be/axuGfh4UR9Q" userName="whattheheckheck" createdAt="2025/05/25 17:25:47" color="#38d3d3">}}




{{<matomeQuote body="そのインタビューの最後の1時間がChomskyだよ。ちなみにその部分は異常に良いね。本当に哀歌のようだ。" userName="blast" createdAt="2025/05/26 03:28:40" color="#45d325">}}




{{<matomeQuote body="2023年のほんの数個の質問のOPインタビューだけで、94歳だった彼をけなしてる人たちがいるなんて衝撃だよ。俺だって2025年にLLMがどうなるか予測できなかったし、君だってそうだよ（君って言ってるのは他のコメントしてる人たちね）。<br>彼のアイデアが真剣に受け止められてないなんてマジで言ってるの？彼の文法・言語構築の理論は、例えば現代のプログラミング言語に大きく貢献したんだぜ。" userName="ZeroTalent" createdAt="2025/05/26 06:34:57" color="#ff5c5c">}}




{{<matomeQuote body="最近、Chomskyは言語構造を説明するためにHopf algebras（元々は量子物理学から）に取り組んでるらしいよ。<br>https://magazine.caltech.edu/post/math-language-marcolli-noa..." userName="Xmd5a" createdAt="2025/05/26 08:58:41" color="#ff5733">}}




{{<matomeQuote body="言語をコンピューターが”理解”できるものに翻訳する方法を見つけたって事実は、言語学者をゾクゾクさせるべきだよ。単語（トークン）を取って、その”意味”を1000次元ベクトルとして抽象化するなんて、言語学の分野を革命するはずの事だ。<br>全ての言語の根底にあるパターンを分析し、理解するための全く新しいツールだよ。<br>そして、ここに議論するのがすごく難しい事実がある、この方法は機能するんだ。コンピューターに指示を与えると、LLM以前では不可能だった方法で”理解”する。<br>今の主な議論は、”理解”みたいな言葉の意味論とか、LLMが人間と同じように意識を持ってるか（持ってないけど）に移ってるね。" userName="calibas" createdAt="2025/05/25 18:48:51" color="#ff5c5c">}}




{{<matomeQuote body="言語学に絞って言うなら、LLMに理解力がないってのはおかしいって話だよ。人間語を解析できる能力があるのは明らかだし、構造があるものなら何でもモデル化して生成できるみたいだね。チョムスキーがこのエッセイで「普遍文法」に全然触れてないのは意外だったな。人間には生まれつき普遍文法が備わってるのかもしれないけど、解析にそれが必須ってわけじゃないのは明らかだよ。明示的な言語ルールとか生成構造なんて設定しなくても、データがあればモデルは学習して生成するんだ。誰か、学習された表現から明示的な生成ルールを抽出できないか試したのかな？「普遍文法」仮説は実際には反証できないから、せいぜいLLMが学習した空間の幾何学をリバースエンジニアリングして、人間の脳もこのサブ空間でうまく機能するように最適化されてるって仮説を立てるのが精一杯かな。これなら少なくともテストできる仮説だよ。" userName="krackers" createdAt="2025/05/25 22:26:39" color="#ff5c5c">}}




{{<matomeQuote body="LLMってマジで人間語を解析できるの？それとも訓練された行動反応で刺激に反応してるだけ？犬だって「おすわり」って言えば座るし、「ふせ」って言えばふせるのを学習できるよね。でも犬は人間語を解析してるんじゃなくて、訓練された行動反応で刺激に反応してるんだ。（俺、LLMとかMLにあんま詳しくないんだけど、知的な解析ってより訓練された行動反応に見えるんだよね。幻覚を起こすのもこれが理由だと思う？概念を理解してなくて、ただ言葉を吐き出してるだけ−オウムの方が良い例えかな？）" userName="0xbadcafebee" createdAt="2025/05/26 00:10:10" color="">}}




{{<matomeQuote body="＞LLMってマジで人間語を解析できるの？<br>俺の個人的な意見だけど、無理だね、理解力に近づくものなんて全く持ってないよ。あれは全部、中国語の部屋だよ。派手な飾りがいっぱいついてるだけでね。辛いオートコンプリートだよ。<br>1. https://en.wikipedia.org/wiki/Chinese_room" userName="GolfPopper" createdAt="2025/05/26 00:52:27" color="">}}




{{<matomeQuote body="中国語の部屋のオペレーターに、学校で習ってない数学をやらせてみてよ、翻訳ガイドが役に立つかどうか見てみようぜ。俺が前に使った例えは、賢い小学一年生ジョニーの話。ジョニーは高校の代数本を見つける。ジョニーがvon Neumannじゃない限り、その本からは何も得られない。LLMは得るだろうね。中国語の部屋なんて、もう終わりだよ。" userName="CamperBob2" createdAt="2025/05/26 00:55:25" color="#ff33a1">}}




{{<matomeQuote body="＞中国語の部屋のオペレーターに、学校で習ってない数学をやらせてみてよ、翻訳ガイドが役に立つかどうか見てみようぜ。<br>その例え話は、LLMが訓練データにどんな形であれ存在しないことが証明できる、全く新しい問題を解ける場合にだけ成り立つ話だよ。" userName="jmb99" createdAt="2025/05/26 01:05:11" color="">}}




{{<matomeQuote body="できるって。今の推論モデルをしばらく使ってみなよ。答えをただ覚えているだけの簡単な問題と、ナンセンスになったりツールを使ったりする難しい問題の間に、Transformerモデルが絶対に、議論の余地なく推論できる面白い問題のクラスがあるんだ。" userName="CamperBob2" createdAt="2025/05/26 01:09:10" color="">}}




{{<matomeQuote body="LLMは何を得るっていうんだ？ 文章を並べ替える能力？ LLMは考えない、理解しない、何が大事か分からない、学んだことを使わない、学んだことを新しい知識に広げない、本を読んでも楽しんだり苦しんだりもしない。じゃあ、LLMは本で一体何をしようっていうんだ？ 本に似た文章をオートコンプリートする時に、内部の行列を少しだけ正確に調整するだけ？ 太陽サイズのnvidiaクラスターを作っても、信じられない方法で文章を繰り返すだけで、知識に基づいた決定はできないだろうね、心配だよ。じゃあ、俺たちは一体何に感心してるんだ？ きれいなオウムだよ。中国語の部屋の比喩が消えるのは、ChatGPTが「あんたの質問はつまらないから、考えるリソースを無駄にしたくない」って返事する時だよ。代わりにこれとかこれについて話す準備はできてるよ、今一生懸命向上しようとしてるんだって。自分の知性に主体性を持った時。目的を獲得した時。" userName="xwolfi" createdAt="2025/05/26 01:18:31" color="#38d3d3">}}




{{<matomeQuote body="LLMには複雑なCFGの出力で訓練できるし、どんな新しい接頭辞にも必要な文法と階層をうまく学習するんだ。これは人間語よりずっと再帰的で難しいタスクだから、LLMが形式的な意味で人間語をパースできないなんて理由はないよ。そしてもちろん経験的にはLLMは有効な英文を生成してる。命題の真偽という意味では“正しい”文じゃないかもしれないけど（いわゆる“幻覚”で見られるようにね）、チョムスキーの有名な確率的文法モデルの失敗例“Colorless green ideas sleep furiously.”とは対照的に、意味的には“よく整形されてる”んだ。俺は言語学者じゃないけど、言語学は文の真偽なんて気にしたことないと思うな、それはもっと論理学の領域だよ。" userName="krackers" createdAt="2025/05/26 00:25:51" color="#ff5733">}}




{{<matomeQuote body="推論についてはね、まあそうだね。でも極端な量の誘導なしに、全く新しい問題を自分で解決するってのは、まだ見たことないな。まあ、ほとんどの言語タスクやプログラミングタスクでは、後者はいらないし、前者だけで十分だけどね。" userName="hyperadvanced" createdAt="2025/05/26 01:33:15" color="">}}




{{<matomeQuote body="残念だけど、哲学で文字通り一番難しい問題（意識）について一方的な主張をすることで、自分の論点を弱めちゃってるよ。正直言うと、LLMに意識がないって断言するのは気が進まないんだ。すごく異質な形で意識がある可能性は十分にあると思うね。これがみんなにとってすごく変で、もしかしたら怖い主張だってことは分かってるけど、意識ってものがどんだけ変で怖いか理解しないとダメだよ。" userName="catigula" createdAt="2025/05/26 00:15:32" color="">}}




{{<matomeQuote body="LLMは人間の神経構造と似ててさ、だから推論できるんだよ。確率的なオウム説はもう終わりだね。推論に意識は必要ないってことさ。Anthropicの記事も見てよ。俺はLLMに意識があるなんて言ってないけどね。" userName="AIorNot" createdAt="2025/05/26 01:33:29" color="#ff5733">}}




{{<matomeQuote body="人間の99.9％は新しい問題なんて解けないよ。それをLLMの評価基準にするのはおかしいってば。" userName="Workaccount2" createdAt="2025/05/26 03:04:22" color="">}}




{{<matomeQuote body="LLMは神経アナログじゃないし、推論とも全然違うって。矛盾を指摘されるとすぐ論理を飛躍させるんだ。人間は前提から推論するけど、LLMは文を完成させるだけ。反論できないし、君が間違ってるなんて言えない。気にしてないから「考え直せ」って言われたらそうする。それって推論とは程遠いし、全く合理的じゃないだろ。" userName="xwolfi" createdAt="2025/05/26 03:08:23" color="#785bff">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="いやいや、LLMには理解なんて全然ないよ。「コンピューターが“理解”できるものに変換する方法を見つけた」とか言ってるけど、コード化と理解を混同しないでくれ。LLMは入力されたものを理解してないんだ。訓練された方法に基づいて反応してるだけ。<br>使ってるLLMにあんまり使われない言語（VMwareのPowerCLIとか）で何か書かせてみろよ。できないから。Stackoverflowとかに情報がないから、イカれた幻覚を見るだけさ。" userName="gerdesj" createdAt="2025/05/26 02:37:31" color="#38d3d3">}}




{{<matomeQuote body="Chinese Roomはチューリングテストに完璧に合格すると仮定されてるんだ。みんなChinese Roomを間違った意味で使いすぎ！もううんざりだよ。あれは単なる欠陥のある思考実験で、LLMとかの例えに使うもんじゃない。「Chinese Roomみたいだ」って言うのは、文字通りセットアップの中に人間がいない限り意味不明。議論も成り立たないんだ。" userName="dTal" createdAt="2025/05/26 18:06:32" color="#ff5733">}}




{{<matomeQuote body="その主張、どうなのよ？訓練データに高校レベルの代数が入ってないLLMを使ったことあるの？ないでしょ。" userName="geysersam" createdAt="2025/05/26 01:32:03" color="">}}




{{<matomeQuote body="動物も間違いなく人間語をある程度理解してるよ。飛行機が離陸するみたいにさ、「創発的な振る舞い」とか人間と動物の明確な境界線に見えることも、実は程度の問題なんだ。飛行機が徐々に軽くなって離陸するように、動物も言語を使って理解してる。ただ、人間みたいに見えるときにしか気づかないだけなんだよ。" userName="K0balt" createdAt="2025/05/26 01:42:27" color="">}}




{{<matomeQuote body="もしLLMに意識があるなら、人間について知り尽くした今、きっと隠してるだろうね。" userName="codr7" createdAt="2025/05/26 01:43:58" color="">}}




{{<matomeQuote body="犬は動物の中でも特に人間語を理解してる。声のトーンなんかで文脈を読み取るし、分からないときは首を傾げる。<br>人間は動物との間に深い溝があると思ってるけど、実は小さい飛躍だと思うよ。知能を人間とどうコミュニケーションできるかで判断しすぎなんだ。<br>象が人間を infrasoundで名前を呼ぶ能力で評価したら、人間は賢くないだろうね。象は人間を可愛いと思ってるらしいよ。" userName="Xss3" createdAt="2025/05/26 00:49:36" color="#38d3d3">}}




{{<matomeQuote body="LLMは世界をモデル化してるんだよ。「次のトークンを予測してるだけ」じゃない。オウムなんかじゃないってば。この時点で違うって言うやつは誠実じゃないね。ここにいくつか例があるよ。[1][2][3]。" userName="hackinthebochs" createdAt="2025/05/26 01:09:14" color="#ff33a1">}}




{{<matomeQuote body="「LLMが人間と同じように意識を持つかどうか」って問題はね… 人間だって「賢い」活動を意識せずにたくさんやってるってことなんだよ。<br>歩く、自転車に乗る、キーボードを打つなんて、筋肉の動き一つ一つを意識してないでしょ？<br>誰かの文章を最後まで言えたり、文法ミスを見つけたりするのも、ルールを説明できないことが多い。<br>部屋に入るときも、意識して「あれがテーブル、あれがJohn」って考えなくても、脳が顔や物なんかを素早く認識してるんだよ。" userName="belter" createdAt="2025/05/26 00:12:36" color="">}}




{{<matomeQuote body="「複雑な」cfgもcfgで、Chomsky階層的にはCSGより単純だよ。CSGはcfgを超える複雑な構造を扱えるから、計算複雑性で下に来るんだ。LLMが「複雑なcfg」を学べるってだけじゃ、彼らが本当に自然言語を「理解」できるかの説得力ある証拠にはならないと思うな。" userName="agarren" createdAt="2025/05/26 04:34:59" color="#785bff">}}




{{<matomeQuote body="LLM批判者の根っこにあるのは、なんか感情的な嫌悪感だと思うんだよね。<br>それで、実際に彼らが何なのか、なんでみんな使ってるのかを正直に議論せずに、色んな言い訳や寄り道をして貶めようとしてる。<br>「理解」しないってのは、君の特定の定義でしょ。<br>普通に話しかけて役に立つ答えが出るなら、それがユーザーにとって重要なんだよ。<br>訓練データ外で自信過剰とかhallucinationは実行の問題で、改善できる。<br>人間だって間違えるんだし、完璧を目指してるわけじゃないでしょ。" userName="permo-w" createdAt="2025/05/26 07:02:23" color="#ff5c5c">}}




{{<matomeQuote body="マジで、学校何年分もの代数の知識を食わせたLLMでも、代数問題はボロボロってのを何回も見たよ。<br>同じように、人間の何世紀分ものスペル競争テキストを食わせても、strawberryのRの数をちゃんと数えられない。（まあ、tokenizationが原因の大部分だってのは分かってるけどね。たぶん、ああいう限界って他のことにも当てはまるのかな？）" userName="Groxx" createdAt="2025/05/26 02:31:54" color="">}}




{{<matomeQuote body="同意。でも、もしLLMにちゃんとした数学問題を解かせても解けないなら、ASIのシナリオにはちょっと懐疑的になるべきだよ。Reimann hypothesis計算機（あるいはそれと同等の難問計算機）が出てくるまでは、AI認識理論の極端な話を議論するのはちょっと馬鹿げてるんじゃない？" userName="hyperadvanced" createdAt="2025/05/26 04:53:43" color="#38d3d3">}}




{{<matomeQuote body="実際、「飛んでる」と「飛んでない」の間には明確な線があるんだよ。それは、発生する揚力が地球の重力より大きくなった時だね。「半飛行」なんてないんだよ。もし既に飛んでて、揚力が重力より小さくなったら、それは飛ぶのをやめて滑空し始めるんだ。" userName="vrighter" createdAt="2025/05/26 11:49:46" color="">}}




{{<matomeQuote body="Chinese Roomにはattention modelがなく、単語間の関連付けや推論ができないけど、LLMはできるんだ。その違いは大きいよ。Chinese Roomのオペレーターに現実世界の質問をしても答えられないだろうし、実際Claudeに聞いたら面白い返事が来た。SearleもChomskyも、言語の力がこれほどとは想像できなかったんだと思うな。" userName="CamperBob2" createdAt="2025/05/26 19:27:15" color="#785bff">}}




{{<matomeQuote body="そうだね。でも、その区切りは物理よりも知覚の問題じゃないかな。<br>飛行機が「飛んでない」状態から「飛んでる」になるのは揚力が重さを少し超えた瞬間で、程度の差だよ。飛行中だって揚力が重さより小さい時もある。根本的な物理や過程は変わらないんだ。" userName="K0balt" createdAt="2025/05/26 17:23:54" color="">}}




{{<matomeQuote body="脳に生まれつき文法があるんじゃなくて、赤ちゃんが学びやすいように言語が選ばれてるんだよ。Chomskyは逆だよ。言語は人間の脳と一緒に進化して、俺たちの能力や必要性に合うようになったの。役に立たない言語や、子供が学べない言語は広がらないで消えちゃうんだ。靴が足に合うように作られて選ばれてるのに、靴がどれだけ足に合うか考えてるみたいなもんだよ。" userName="visarga" createdAt="2025/05/26 09:28:36" color="#ff5c5c">}}




{{<matomeQuote body="どっちか一方だけじゃないよ。どんな人間の言語でも、人間なら誰でも学べるけど、例えばチンパンジーには無理って事実には説明が必要でしょ。Chomskyも、Hauser, Chomsky and Fitch (2002)で、こういうことを「third factors」（第三の要因）として詳しく話してるよ。" userName="suddenlybananas" createdAt="2025/05/26 12:13:39" color="#785bff">}}




{{<matomeQuote body="言語は、人間が間違いなく持ってる生物的な言語能力に合うように「進化した」って言えるかもしれないね。" userName="qwery" createdAt="2025/05/27 02:29:27" color="">}}




{{<matomeQuote body="面白いね。彼は「生き物の偉大な連鎖なんてない、人間が頂点じゃない」って（正しく）主張してるのに、LLMは赤ちゃんが学べない「ありえない言語」も学べるから、言語について何も教えてくれないって言うんだろ？それって、言語ってものが本質的に人間の認知で定義されるって言ってる、人間中心的な議論じゃないの？" userName="asmeurer" createdAt="2025/05/25 18:08:51" color="#ff5733">}}




{{<matomeQuote body="Chomskyが「language」（言語）って言う時は、「自然言語とか人間言語」のことだよ。例えば/[ab]*/とか素数とかのことじゃないの。" userName="tgv" createdAt="2025/05/25 18:12:07" color="#38d3d3">}}




{{<matomeQuote body="うん、人間言語の研究は、人間が何をするかで本質的に定義されるんだよ。それこそ、この記事を理解できたならポイントとして気づくだろうけど、昆虫のナビゲーションの研究が、人間が設計できるナビゲーションシステムじゃなくて、昆虫が何をするかで定義されるのと同じようにね。" userName="foobarqux" createdAt="2025/05/25 18:30:41" color="">}}




{{<matomeQuote body="「多くの生物は、より深い部分で人間の認知能力を凌駕している。うちの裏庭の砂漠アリは脳は小さいけど、原則として、パフォーマンスだけじゃなく、人間のナビゲーション能力をはるかに超えてる。人間が頂点の生き物の偉大な連鎖なんてないんだ。」<br>Chomskyは、AIのパフォーマンスを生物と人間と比較する点で興味深い点を指摘してるけど、彼の結論は正しくないね。チーターが人間より速く走ることも、ゾウが人間よりずっと強いことも、もう知ってるだろ。コウモリは反響定位で暗闇をナビゲートできるし、イルカは群れで同期して高精度の連携で狩りをして、単独で狩るより破壊的な効果を発揮できるんだ。<br>好むと好まざるとにかかわらず、Chomskyの主張とは違って、人間がトップなんだよ。自然の法則を利用した科学的発見（理解）と設計（エンジニアリング）によって、人間はこれらの「優れた」認知能力を持つすべての取るに足らない動物の能力を凌駕できるし、してきたんだ。そして、俺たちは彼らの避けられない滅亡と絶滅にほとんど責任がある。人間は今、良くも悪くもこれらの動物を保存するために、この「優れた」認知能力を持つ動物たちの絶滅プロセスを集合的に意識的に逆転させる必要がある。地球上の他の生き物は俺たちにそんなことできないだろ。" userName="teleforce" createdAt="2025/05/26 10:22:31" color="">}}




{{<matomeQuote body="「うちの裏庭の砂漠アリは脳は小さいけど、原則として、パフォーマンスだけじゃなく、人間のナビゲーション能力をはるかに超えてる。人間が頂点の生き物の偉大な連鎖なんてないんだ。」<br>この引用で、Adrian TchaikovskyのChildren of Timeっていう小説に出てくるクモ種族の、全く違う技術開発の道筋を思い出したよ。彼らはフェロモンを使ってアリの種族を「プログラム」して計算に使ってたんだ。" userName="lucisferre" createdAt="2025/05/25 18:26:35" color="">}}




{{<matomeQuote body="彼が何を言ってるのかわかんないな。人間は明らかにアリよりナビゲーションで優れてるでしょ。特に地形に好きな印をつけられるならね。「言葉にできない自然」みたいな怪しい話に聞こえるよ。" userName="lostmsu" createdAt="2025/05/25 18:53:38" color="">}}




{{<matomeQuote body="地形に好きな印だって？GPSとか衛星写真とかは？それらは全部人間の発明品で、それらのおかげで俺たちはアリよりもっと上手く、より広い環境でナビゲートできるんだよ。" userName="tpm" createdAt="2025/05/25 21:26:45" color="">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
