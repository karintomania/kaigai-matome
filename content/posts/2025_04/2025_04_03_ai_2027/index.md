+++
date = '2025-04-03T00:00:00'
months = '2025/04'
draft = false
title = 'AIは2027年に何を起こすのか？衝撃の未来予測が話題に！'
tags = ["AI", "AGI", "未来予測", "技術革新", "社会問題"]
featureimage = 'thumbnails/green2.jpg'
+++

> AIは2027年に何を起こすのか？衝撃の未来予測が話題に！

引用元：[https://news.ycombinator.com/item?id=43571851](https://news.ycombinator.com/item?id=43571851)

{{<matomeQuote body="もう十分賢いAIが出てきてるから、AGIが2年で急激に進歩するなんてありえないって気づいたんじゃないかな。今のAIは2023年と根本的に変わってないよ。得意なことはめっちゃ得意になったし、新しい機能もいくつかあるけど、結局は次の単語を予測してるだけじゃん。相変わらず長期的なタスクは苦手だし、人間のようにはちょっとのデータから学習できないし。コードは書けるけど、シンギュラリティが来る気配もないしね。裏ではすごいことが起きてるって言う人もいるけど、証拠を見たことないし、AIの話ってすぐ大げさになるから。" userName="Vegenoid" createdAt="2025-04-04T17:20:13" color="">}}

{{<matomeQuote body="＞some new capabilities that are big, but they are still fundamentally next-token predictors”<br>Anthropicが最近発表した研究によると、Claudeが詩を作るとき、ただ単に単語を予測して、韻が必要だと思ったら文脈を見て考えるんじゃなくて、先の単語をいくつか見て、最終的にどうなるかを予測して調整してたらしいよ。Anthropicは、これは言語モデルが「計画を立ててる」証拠だって言ってる。詳しくはここの「Planning in poems」を見てみて！" userName="jug" createdAt="2025-04-04T18:54:46" color="#785bff">}}

{{<matomeQuote body="それって結局、次の単語を予測してるだけじゃないの？韻を踏める可能性を残すために、韻を踏める単語を選んだり、ニッチな話題よりも広い話題に焦点を当てたりするってことでしょ。" userName="percentcer" createdAt="2025-04-04T19:11:07" color="">}}

{{<matomeQuote body="タスクが単語を生成するだけなら、どこからが「ただの単語予測」じゃなくなるの？どのくらいの推論や計画があればそう言えるの？" userName="DennisP" createdAt="2025-04-04T19:59:35" color="">}}

{{<matomeQuote body="面白い質問だね。でも、基本的な操作が「単語を生成する」だけなら、結局はただの単語予測にしかならないんじゃないかな。LLMが出てくるまで、人間の思考を単語の連続だなんて思ってなかったし。うまくまとまってないけど、「単語を生成する」ことがシステムの一部でしかないAIが必要なのかもね。" userName="Vegenoid" createdAt="2025-04-05T03:30:00" color="#ff5733">}}

{{<matomeQuote body="それって、AIが話すだけじゃAIって言えないってことにならない？どんなにTuringテストに合格しても、関係ないってこと？どんな分野の専門家とも対等に話せて、どんな数学や物理の問題も解けて、オリジナルの哲学論文も書けても、結局は単語を生成してるだけってことになっちゃう。昔から、そういうコンピューターはAIって考えられてたのに。" userName="DennisP" createdAt="2025-04-05T13:32:35" color="">}}

{{<matomeQuote body="人間とテキストでやり取りすることだけが問題ってわけじゃないんだ。今は、AIはどんなものとも単語を生成してやり取りしてる。APIを呼び出すのも、ツールを使うのも、知識を検索するのも、人間が質問するのと同じように単語を予測してるんだよ。もっと完全な知能にするには、LLMがインターフェースとなる他のサブシステムが必要なのかもね。" userName="Vegenoid" createdAt="2025-04-09T18:57:41" color="">}}

{{<matomeQuote body="AGIを実現する上で、一番の課題は、指示されなくても行動するってことじゃないかな。好奇心とかさ。窓が開いてる部屋に人型ロボットが立ってるとするじゃん。いつになったらAIは窓を閉めるべきだって判断するんだろう？LLMは、シードを変えるだけで違う反応をするアルゴリズムに過ぎないって思うのは、そういう理由もあるのかも。" userName="LoveMortuus" createdAt="2025-04-09T14:31:10" color="">}}

{{<matomeQuote body="ただの単語予測だよ：https://old.reddit.com/r/singularity/comments/1jl5qfs/its_ju…" userName="z7" createdAt="2025-04-05T15:12:57" color="">}}

{{<matomeQuote body="意味のある区別かわからないけど、世界を「次の単語予測器」として記述できるんじゃない？世界をある量子の時間ステップを持つシミュレーターとして扱えばいいんだよ。たぶん全部は捉えられないけど、実用上は現実と区別がつかないよ（時間があらゆるところで一定じゃないことは置いておいて）。" userName="hnaccount_rng" createdAt="2025-04-05T08:45:13" color="">}}

{{<matomeQuote body="わかる気がするなー。そのモデル(AGIじゃないやつね)なら、たしかにネクストトークン予測の延長線上って感じだよね。でも、ちっちゃなルールとか癖とか改善とか、いろんな挙動やプロセスを引き起こすサブシステムとか、そういうのが全部集まったら、それって結局人間じゃん？<br>めっちゃ時間かかるだろうけど、人間が特別だとか、説明できない魔法みたいな存在だとか、再現不可能だとか、どうしても信じたい人たちとは違うんだよね、俺は。" userName="fennecfoxy" createdAt="2025-04-08T11:05:36" color="">}}

{{<matomeQuote body="説明することにはならないんじゃないかなー。テーマに合った、韻を踏んだ意味不明な文章がたくさん出てくるだけになりそう。" userName="pertymcpert" createdAt="2025-04-04T22:11:56" color="">}}

{{<matomeQuote body="再帰的な予定説か。LLMのアルゴリズムは、「起源」の「文字列」を「学習」するために「自己破壊」を暗示してる。" userName="rcrsvpreordnmnt" createdAt="2025-04-05T09:49:47" color="">}}

{{<matomeQuote body="人間の脳が次の筋肉の収縮を予測してるのと同じってこと？" userName="throwuxiytayq" createdAt="2025-04-04T19:30:16" color="">}}

{{<matomeQuote body="そうかもね。でも、俺はもっと反応してるって思うな。例えば、かゆみを感じて、無意識のうちに掻いちゃうとか。それって意識とは別のサブシステムじゃん？<br>初期の進化は、個体の生命を維持するための小さくて特殊なバックグラウンドプロセスで構成されてたってことだよね。単細胞生物にはニューロンはないけど、まさにそういうプロセス、つまり「生きてる」状態を保つ化学反応がある。<br>で、そういうプロセスが複雑になって、論理的な表現が必要になって、ニューロンが進化したんだと思う。その後、何千、何万ものニューロンサブシステムを持つ生物は、より高度な刺激や組み合わせに基づいてサブシステムを制御するために、より高次のサブシステムを発達させた。<br>そして、最終的に俺たち人間になった、と。次のステップとして、進化は意識/知性、つまりすべてのサブシステムの努力の方向性（まだ全部意識的に制御されてるわけじゃないけど）が、個人の方向性としてはるかに効果的であることを見出したんだと思う。予測、計画とか、最高レベルの行動とかね。<br>十分な時間と適切な条件があれば、持続的な進化によって、この惑星上のほとんどの生物が意識的な脳を持つようになるんじゃないかな。俺たちはたまたまラッキーだっただけ。" userName="fennecfoxy" createdAt="2025-04-08T11:15:45" color="#785bff">}}

{{<matomeQuote body="いや、そうはならんやろ…" userName="alfalfasprout" createdAt="2025-04-04T19:46:56" color="">}}

{{<matomeQuote body="公平に言って、人間の心がどう機能してるかは実際にはわかってないんだよね。<br>確実なのは、それが物理的なシステムだってことと、そのシステムであることは何かを感じるってこと。" userName="Workaccount2" createdAt="2025-04-04T20:57:13" color="">}}

{{<matomeQuote body="たぶん、これのことかもね。<br>https://en.m.wikipedia.org/wiki/Predictive_coding" userName="ToValueFunfetti" createdAt="2025-04-05T18:28:59" color="">}}

{{<matomeQuote body="LLMがやってることは、人間がやってることと全く同じ。テキストを読んで、フラグを立てて、そのテキストが連想させる様々なトピックについてフラグを立てて（ポジティブもネガティブも）、それらのフラグに対応する応答を書き始める。計画を立てるってのも、対処すべきフラグの一つに過ぎない。それは学習された予測行動なんだよ。特に目新しいことじゃない。経験がフラグを与えてくれる。まるで、途方もない圧力をかければダイヤモンドが出来るみたいに。" userName="childintime" createdAt="2025-04-05T11:04:54" color="">}}

{{<matomeQuote body="METR[0]は、長期的なタスクの進捗を明確に測定してる。今のところ、他の進捗と同じように急なシグモイド曲線を描いていて、まだ変曲点はない。<br>他のスレッドでも指摘されてるように、RLHFはネクストトークン予測を超えて進歩しており、最新のモデルは概念をモデル化してる[1]。<br>[0] https://metr.org/blog/2025-03-19-measuring-ai-ability-to-com...<br>[1] https://www.anthropic.com/news/tracing-thoughts-language-mod..." userName="benlivengood" createdAt="2025-04-04T18:02:37" color="#ff33a1">}}

{{< details summary="もっとコメントを表示（1）">}}
{{<matomeQuote body="ドジっ子みたいで超的外れだったらごめんねー。AGI予測でこの手の指標はあんまアテにしてないんだよね。AIが確実にこなせるタスクの長さが7ヶ月ごとに倍になるって言っても、それって人間が何週間も何ヶ月もかかるタスクをAIがこなせるようになるにはまだ何年もかかるってことじゃん？その倍増トレンドがずっと続くとは思えないんだよねー。数週間とか数ヶ月かかるタスクって、数分とか数時間で終わるタスクとは質的に違うと思うんだよね。人事の人とか、エンジニア採用の時にソレ感じてると思うよ。AIはまだ根本的なところで汎用的な知能が足りない気がするんだよね。" userName="Vegenoid" createdAt="2025-04-04T18:37:23" color="">}}

{{<matomeQuote body="確かに、数週間とか数ヶ月かかるタスクって質的に違うよねー。新卒と10年目のエンジニアの違いみたいなもんかも。新卒って優秀な人も多いけど、1年半くらいかかるプロジェクトを任されたら、今の俺なら普通にこなせるのに、昔の俺じゃ絶対無理だったと思うもん。LLMは、そういうプロジェクトを成功させるために必要なことを文章化したものをエミュレートするのは得意だと思う。でも、それだけじゃダメなんだよねー。LLMは今のやり方じゃ無理ゲー。文脈を理解して、重要度を判断して、間違ってたら捨てて、新しいパラダイムが来たら全部捨てて、それを全部インテリジェントに取り込んで意思決定を変えていかないと、人間の代わりにはなれないよ。" userName="Enginerrrd" createdAt="2025-04-04T19:37:52" color="#ff5733">}}

{{<matomeQuote body="＞”数週間とか数ヶ月かかるタスクって、数分とか数時間で終わるタスクとは質的に違うと思うんだよね”<br>長期的な計画と実行機能ってことかな？あと、委任も絡んでくるよね。長期プロジェクトって、一人じゃ無理だし、委任するためにはタスクを細分化する必要がある。タスクを細分化するには、全体像を把握してコンポーネントに分割する必要があるんだよねー。今のモデルでも個々の要素はできそうだけど、学習データにガントチャートとかプロジェクト計画とかプロジェクトマネージャーの会議とか、あんまないんじゃないかな？概念は学べても、実践は苦手そう。" userName="benlivengood" createdAt="2025-04-04T19:27:07" color="#ff5c5c">}}

{{<matomeQuote body="METRのグラフは、2024年より前の4つのデータポイントに基づいて6年間のトレンドを予測してるけど、統計的にどうなの？AIすごいと思うけど、これはちょっと微妙じゃない？" userName="Fraterkes" createdAt="2025-04-04T18:11:09" color="">}}

{{<matomeQuote body="AI開発の良い統計モデルがないってのは同意。もし予測可能なら、特異点を既に超えてるか、統計モデルをリバースエンジニアリングして冬の時代になってると思うよ。" userName="benlivengood" createdAt="2025-04-04T19:18:09" color="">}}

{{<matomeQuote body="特異点の兆候は見られないけど、指数関数的ではないにしても、急上昇してるのは確かだよね。LLMが人間の知能レベルに達するだけで超えられないなら、失速する可能性もあるけど。元記事は面白かったし、僕の小説より36万語も短いし！" userName="boznz" createdAt="2025-04-04T20:03:19" color="">}}

{{<matomeQuote body="LLMは現時点では知能なんて持ってないよ。大量のデータを持ってて、それをコピーして修正してるだけ。" userName="grey-area" createdAt="2025-04-04T20:45:40" color="">}}

{{<matomeQuote body="人間レベルの知能じゃないのは確かだけど、全く知能がないって言うのはどうかなー？明らかに汎化はできてるじゃん。どこからが知能なのさ？" userName="EMIRELADERO" createdAt="2025-04-04T22:05:42" color="">}}

{{<matomeQuote body="逆を証明する必要があるんじゃない？<br>閾値は「入力された学習データと全く同じもの、またはちょっと変えただけのものを生成しない」ことじゃない？<br>俺のコードエディタのAIアシスタントは、ドキュメントに書いてあることしかできないし、ドキュメントに書いてあることすら間違えるときあるし。例えば、Terraformのdynamic credentialsでAIに助けてもらおうとしても、ドキュメントが少なすぎて無理なんだよね。ありふれた使い方から外れると、マジで役に立たない。結局、AIはすごい検索エンジンってだけで、ドキュメントとか事例にあるロジックを組み合わせて、名前をちょっと変えてるだけなんだよねー。大量の学習データがあるから、すごい奴に見えるだけだよ。" userName="dangus" createdAt="2025-04-04T23:36:55" color="">}}

{{<matomeQuote body="それって創造性じゃね？知能と創造性は別物だと思う。LLMは創造性がなくても知能を持つことは可能だよ。" userName="EMIRELADERO" createdAt="2025-04-04T23:45:46" color="">}}

{{<matomeQuote body="それはコンセプトを説明するための極端な例だよ。俺が言いたいのは、創造性が低い（現在のモデルが持ってるレベル）のは、知性が完全に欠けてるってことじゃないってこと。" userName="EMIRELADERO" createdAt="2025-04-05T00:09:33" color="">}}

{{<matomeQuote body="マジかよ、お前に売る辞書があるぜ！" userName="dangus" createdAt="2025-04-05T11:43:23" color="">}}

{{<matomeQuote body="同意。ただ、インテリジェンスの部分がマジで欠けてると思う。でも人間って賢いから、ギャップに気づくはず。だから誰か（大手じゃなくても）が最終的に解決すると思う。" userName="boznz" createdAt="2025-04-04T21:59:40" color="#38d3d3">}}

{{<matomeQuote body="…俺の小説のラストの方がもっとエキサイティングだったけどね。" userName="boznz" createdAt="2025-04-05T22:23:05" color="">}}

{{<matomeQuote body="違うね。SWE-benchで0%から63%になったんだぜ。これは2年間でマジで大きな進歩だよ。<br>赤ん坊が数歩歩けるのと、大人が歩けるのを同じ「歩く能力」って言うようなもんじゃん。それって間違ってる。" userName="killerstorm" createdAt="2025-04-05T15:54:03" color="#ff33a1">}}

{{<matomeQuote body="まだStrawberryのRの数を数えられないんだぜ" userName="OrangeMusic" createdAt="2025-04-08T07:24:07" color="">}}

{{<matomeQuote body="それ明らかに嘘じゃん。4oとかo3-miniでもできるって。" userName="senordevnyc" createdAt="2025-04-09T16:41:19" color="">}}

{{<matomeQuote body="＞彼らは得意なことはマジで上手くなったし、新しい能力もいくつかあるけど、根本的にはまだ次のトークンを予測してるだけだよね。<br>これマジでわかんねー。あなたは自己回帰LLMは、定義上、AGIとして認められないって言いたいの？ Mercuryみたいな拡散モデルは？推論の方法が同じ結果なら、関係ないんじゃない？" userName="ComplexSystems" createdAt="2025-04-04T19:19:17" color="">}}

{{<matomeQuote body="＞自己回帰LLMは、定義上、AGIとして認められないって言いたいの？<br>いや、AGIとして認められるような能力には達しないだろうと推測してるだけ。" userName="Vegenoid" createdAt="2025-04-04T19:36:29" color="#ff5c5c">}}

{{<matomeQuote body="SFとしては面白いと思うよ。細かい技術的なことにとらわれず、物語の核心は、これがAGIにつながらなくても、少なくとも、それが突然、不可逆的に現れる前の最後の警告になる可能性が高いってことだよね。Alignment、地政学、社会的な安全対策の欠如といった問題は、すべて現実で、今起こってることだよ（“AGI”を“corporations”に置き換えるだけで、気候危機と規制の虜についての物語になる）。AGIや仕事がAIに置き換えられるようになる前に、これらの問題を解決するべきだよね。さもないと、社会の崩壊や種の絶滅っていうマジでヤバいリスクを冒すことになるよ。こういう話の目的は、アラームを鳴らすことなんだって。時間があるうちに積極的な対応を促そうとしてるんだってさ。" userName="stego-tech" createdAt="2025-04-04T05:17:35" color="#785bff">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（2）">}}
{{<matomeQuote body="誰も何も解決しないって。今の世界は欲深いバカが権力を集中させてて、そいつらがまたバカを使って俺たちを叩くシステムになってるから。このシステムは「俺たちが何をすべきか、何をしてはいけないか」なんて考えてないし、誰もまともなこと考えてないよ。俺たちの役目は、このAIってのがただの宣伝文句じゃなければ、機械に取って代わられること。危険を冒してるのは「彼ら」じゃなくて「俺たち」なんだ。金持ちは自分の金のことしか考えてない。金はどんどん集まるだけ。" userName="wruza" createdAt="2025-04-04T05:48:09" color="">}}

{{<matomeQuote body="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/" userName="jrvarela56" createdAt="2025-04-04T08:19:12" color="">}}

{{<matomeQuote body="記事ありがとね。もう関わるのをやめればいいじゃんって思うかもしれないけど、君は生き残るために他のやつらを蹴落としてきたやつらの一員なんだよね。自然ってマジでクソなジョークだよね。設計者を温かい気持ちで見れるわけないじゃん。" userName="wruza" createdAt="2025-04-04T09:02:11" color="">}}

{{<matomeQuote body="宇宙の石の上で互いを食い合ってクソを垂れ流し、苦痛に満ちた恐ろしい死に向かって進む肉の袋…天才的なデザインじゃね？" userName="braebo" createdAt="2025-04-04T12:57:30" color="">}}

{{<matomeQuote body="＞たとえこれがAGIにつながらなくても、少なくとも、それが突然、不可逆的に現れる前の最後の“警告”になる可能性が高い。<br>SFとして面白いのは同意だけど、真面目に考えすぎだって。こういう予測は全部、架空の証拠から一般化してるんだよ。deep learningが登場する前から、Nick Bostromみたいなやつらが知能爆発の話をしてた。「機械は脳をどんどん忠実にシミュレートできるようになる。猫、村の馬鹿、アインシュタインの順にシミュレートできるようになる」みたいな。でも、LLMはそうじゃない。猫並みの知能の機械はまだない。ネットの平均的な人並みのマルチ段落オートコンプリートがあるだけ。これは不完全な類似点だけど、このプロセスが自己改善する兆候はないんだよね。むしろ逆。スケール則は、リソースを追加すると進歩が遅くなることを示してる。指数関数的な成長の証拠はない。新しいテクノロジーが最初に生産されると、急速な成長が見られるけど、それは当然。簡単にできることがたくさんあるからね。AGIがすぐそこにあるってのは違う。SFだよ。" userName="fmap" createdAt="2025-04-04T12:01:19" color="">}}

{{<matomeQuote body="＞指数関数的な成長の証拠はない<br>自己改善型のAIの成長には、AIが人間の開発者/研究者と同じくらい賢くなる必要があると思う。まだそこには到達してないけど、いつかは到達する可能性は高いと思うよ。" userName="tim333" createdAt="2025-04-04T13:08:42" color="">}}

{{<matomeQuote body="＞いつか猫、村の馬鹿をシミュレートできるようになる…これはLLMのやり方じゃない。<br>その議論を誤解してると思うよ。「脳のシミュレート」は「最初から始める」議論じゃなくて、「よくある反論に答える」議論なんだ。2000年頃、Nick Bostromがこういう話をしてた頃は、コンピュータは人間を出し抜くほど賢くなかったんだよ。Bostromのポイントは、「プログラムを知らなくても、脳をシミュレートすれば、数十年で超知能に到達できる」ってことだったんだ。猫をシミュレートするって話じゃなくて、脳より効率的なものを思いつかなくても、猫、馬鹿、アインシュタインの順にシミュレートできるってこと。 Moore’s lawは指数関数的で、脳のシミュレートって予測はそこから来てるんだ。「架空の証拠」ってのは、生物学的な類似点がないってことだけじゃないかな。AIに注意すべき理由はこうだ。<br>A. 超知能AIを作ることは可能だ<br>B. 超知能AIへの進歩は指数関数的だ<br>C. 超知能AIが、俺たちが望まないことをする可能性がある<br>D. そんなAIは成功する可能性が高い<br>AかBが間違ってるって信じてるから懐疑的なんだよね？どうしてそう思うの？" userName="gwd" createdAt="2025-04-04T13:05:54" color="">}}

{{<matomeQuote body="＞脳より効率的なものを思いつかなくても、猫、馬鹿、アインシュタインの順にシミュレートできるってこと<br>この議論の問題点は、より賢い機械への直線的な軌道に乗ってるって仮定してることなんだ。LLMは一般的な知能じゃない。マルチ段落オートコンプリートで、既存のテキストにますます近づいてる。現在のモデルは言語処理には最適で、ソーステキストに存在する限り、単純な推論能力を持ってる。RLHFを使ってモデルを特定のタスクに役立つようにするのはすごいことだけど、トレーニング方法や元のトレーニング目標は変わらない。LLMは、トレーニングデータのすべての単語シーケンスを忠実に再現できるモデルになったらどうなるの？AGIを作成する賢い方法があるかもしれないけど、現在のモデルとは関係ないってこと？それは記事よりも弱い主張だよ。記事は現在の能力から外挿してる。" userName="fmap" createdAt="2025-04-04T14:05:58" color="#ff5733">}}

{{<matomeQuote body="＞マルチ段落オートコンプリートで、既存のテキストにますます近づいてる。<br>E. LLMはマルチ段落オートコンプリートしかできない。考えることはできない<br>F. AGIを実現できるアプローチは、完全に異なる構造になる。AGIがいつ開発されるかはわからないけど、開発されたらゼロから始めることになるから、心配する時間はたっぷりある。<br>Eは真実かもしれないし、そうじゃないかもしれない。理論的な議論だね。でも、理論的にはEが間違ってる可能性もある。天気を予測する最良の方法は、天気システムを近似する内部モデルを持つこと。人間が次に書くことを予測する最良の方法は、人間の心、世界のモデルを持つことだ。LLMは世界をモデル化してるってことだよ。EDIT:もし(潜在的に危険な)人格が「マルチパラグラフオートコンプリート」からどのように生まれるかを見たいなら、”Alignment over time”を見てくれ" userName="gwd" createdAt="2025-04-04T20:37:35" color="#785bff">}}

{{<matomeQuote body="こういった予測って、みんなフィクションに基づいた一般化だって言ってるけど、それ間違ってるよ。Danielとかチームのメンバーは、世界レベルの予測のエキスパートなんだから。Danielは2021年にも2026年のAIの世界を予測して、それが驚くほど当たってたんだ。信じる価値ありだよ。<br>＞当時の議論はこんな感じだった。“マシンは脳をどんどん忠実にシミュレートできるようになる”<br>これって根本的な考え方を全く理解してないよね。的外れにもほどがあるよ。<br>＞ここ数年で便利なツールは出てきたけど、AGIがすぐそこにあるって話はもう終わりにすべき。SFみたいなもんで、フィクションに基づいて判断を誤らせるだけ。<br>それはめっちゃ危ない考え方かも。AI業界じゃ、AGIは50年以内に実現するって予測がほぼ常識だし、10年以内って人も多いんだぜ。これはマジで難しい問題で、火星の人口過多みたいなもんだと思って無視するのはマジでヤバい。" userName="vonneumannstan" createdAt="2025-04-04T15:04:13" color="">}}

{{<matomeQuote body="関係者の予測能力はリスペクトしてるけど、そのレポートが「驚くほど正確」って言われてるの何度か見たけど、ホントかな？ narrative形式だから解釈の余地があるし、2021年から考えると方向性は合ってるけど（外交の予測とか、計算コストが大幅に下がるって予測とか）、具体的な予測は間違ってる気がするんだよね。LLMの能力の急成長を捉えられてない気がするし。<br>2021年～2026年の予測が直線的なのに、今回の予測が指数関数的なのも気になるし、納得できる理由が見当たらないんだよね。" userName="loganmhb" createdAt="2025-04-04T15:55:20" color="">}}

{{<matomeQuote body="＞現在のAIバブルが弾けた後、資金不足になるであろうエキサイティングなアプリケーションはありますか？<br>例を教えてほしいな。マジで興味ある。" userName="whiplash451" createdAt="2025-04-04T14:08:18" color="">}}

{{<matomeQuote body="AIで世界を変えるために、アインシュタインをシミュレートする必要はないと思うな。自動運転車だけでも十分すごいじゃん。" userName="whiplash451" createdAt="2025-04-04T14:10:46" color="">}}

{{<matomeQuote body="遠回しな言い方じゃなくて、むしろめっちゃ回りくどいよね。<br>大企業とか政府とか教会とか政党って、ある意味AGIみたいなもんだよね。不滅で、眠らないし、分散してるし、どこにでもいるし、人間の知能とか富とか権力を超えてるじゃん。機械仕掛けのTurk AGIみたいなもんよ。人間が出たり入ったりしても、あんまり変わらないのは、彼らがメンバーとは別に存在して、意志を持ってるから。<br>AGIに備えるためにやるべきことって、機械仕掛けのTurkを抑制して、整合性を確保することと一緒だと思うんだ。それができなきゃ、もっと早くて賢いものには勝てないよ。<br>過去50年間でやったことは、逆のことばっかり。彼らを解き放っただけでなく、整合性を持たせるって考え方すら捨てちゃった。<br>AI alignmentの議論って、隠された進歩的な政治議論じゃないの？昔、魔女狩りしてた頃、哲学者たちは異端的な考えを暗号化して、意味不明な言葉で表現してたんだ。それがオカルティズムの始まり。企業権力を抑制するなんて言ったら叩かれるけど、そういう議論を排除しようとする動きがあるんだよね。<br>将来のAGIを想像してみてよ。熱狂的な信者がいて、政治家やCEOみたいなチャンピオンがいるはず。人間がそれをやらなくても、AGI自身が同じような機能を持ってるはず。<br>株式会社とかって、AGIのデジタル頭脳を委員会とか株主に置き換えただけじゃない？<br>何が違うの？どっちも危険なほど整合性が取れてないじゃん。<br>規模くらい？デジタルAGIの方が賢くて速いかもしれないけど、それ以外に違いが見当たらない。" userName="api" createdAt="2025-04-04T11:25:31" color="#38d3d3">}}

{{<matomeQuote body="オカルティズムが異端的な考えを暗号化したものだっていう証拠は見つからなかったよ。ルネサンス期のフランスで、隠された力を研究することとして広まったみたい。幻覚見てるんじゃない？" userName="brookst" createdAt="2025-04-04T12:00:01" color="">}}

{{<matomeQuote body="どこで調べたの？" userName="balamatom" createdAt="2025-04-04T12:01:21" color="">}}

{{<matomeQuote body="Google、Wikipedia、Kagiで調べたよ。あなたのソースは？" userName="brookst" createdAt="2025-04-04T23:53:54" color="">}}

{{<matomeQuote body="＞AGIが提起する問題点（整合性、地政学、社会的な安全対策の欠如）は、すべて現実のものとして今起こっている（AGIを企業に置き換えるだけで、気候危機や規制の虜囚についての話になる）。<br>悪い企業が地球を破壊しているってデータあるの？西側諸国では1990年代から炭素排出量は減ってるよ。一人当たりじゃなくて、絶対量で減ってるんだ。貿易調整しても同じだよ。規制とか善意じゃなくて、コストを最小限に抑えようとする市場システムのおかげ。炭素使用にはコストがかかるからね。コストを下げれば炭素も減る。<br>他の指標でも、西側諸国の水質は安全になってるし、環境関連の死亡者数も減ってる。<br>炭素排出量が増えてるのは中国とインドのせいだよ。中国とインドの邪悪な企業の話をしてるの？" userName="bko" createdAt="2025-04-04T13:27:42" color="">}}

{{<matomeQuote body="念のため、私たちが混乱して反対のことを考えないように、すべて順調であることを知らせてくれてありがとう。" userName="boh" createdAt="2025-04-04T14:42:47" color="">}}

{{<matomeQuote body="どういたしまして。すげー多くの中流以上の教育を受けた人たちが、地球が10年以内に住めなくなるって信じて子供を作りたがらないんだよね。マジで奇妙だし、年取って老人ホームで一人ぼっちになった時、世界はまだ存在してるって気づいて後悔するだろうね。この話題に関する神経質な感じが若い人たちをマジで暗い場所（抗うつ薬、神経質な反社会的行動、ニヒリズム）に追いやっちゃってると思う。だから、世界の終わりの日の誤情報と戦うのは大事だと思うんだよね、事実と常識の両方で。" userName="bko" createdAt="2025-04-04T16:22:52" color="">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（3）">}}
{{<matomeQuote body="暗い場所とか言って、カッコの中に抗うつ薬って書くのはどうかと思うよ。全ての脳が正常に機能するわけじゃないし、必要な助けを得る人を stigmatize すべきじゃない。自分の意見が正しいって主張もしてないし、相手の worldview を自分のと対立させて、それが間違ってるって決めつけてるだけじゃん。自分が正しいと思ってるだけで。" userName="WXLCKNO" createdAt="2025-04-04T18:52:15" color="#785bff">}}

{{<matomeQuote body="彼はきっと、カーボン排出を中国とかインドの corporation に outsource した、善良な西洋の corporation について言ってるんだよ。" userName="ktusznio" createdAt="2025-04-04T15:15:28" color="">}}

{{<matomeQuote body="＞これらの evil corporation が地球をダメにしてるってデータを示せる？<br>これが corporation のせいじゃなくて、 corporation にも関わらずそうなってるってデータを示せる？" userName="philipwhiuk" createdAt="2025-04-04T15:10:55" color="">}}

{{<matomeQuote body="最初に corporation のこと言ったんだから、証明責任はあんたにあるよ。" userName="om8" createdAt="2025-04-04T15:13:08" color="">}}

{{<matomeQuote body="企業によるカーボン排出の“削減”について読むときは、ある程度の skepticism は必要だと思う。数字をごまかす動機があるのに、なんで彼らの言葉を鵜呑みにしなきゃいけないの？" userName="jplusequalt" createdAt="2025-04-04T15:14:07" color="#785bff">}}

{{<matomeQuote body="一番笑えるのは、人類が2027年までに datacenter に電力を供給するための nuclear reactor を一つでも作れるっていう揺るぎない信念だよね。ネットワークなんてありえないのに。" userName="torginus" createdAt="2025-04-04T12:25:50" color="">}}

{{<matomeQuote body="＞社会崩壊とか種の絶滅のすごい現実的な risk<br>いや、気候変動が原因で近い将来に種の絶滅が起こる risk はないし、同じことを繰り返してたら、人々の分断が進んで、他の人や真面目な気候科学者の言葉に耳を傾けなくなるだけだよ。" userName="YetAnotherNick" createdAt="2025-04-04T10:39:20" color="#45d325">}}

{{<matomeQuote body="人が聞きたくないことを言わなければ、全てうまくいく？<br>それってマジで愚かさの極みじゃん。" userName="Aeolun" createdAt="2025-04-04T10:56:29" color="">}}

{{<matomeQuote body="risk は定量化できる 0.0% なの？信じられない。今の trend だと、環境破壊が続けば社会が滅亡する risk があると思う。" userName="ttw44" createdAt="2025-04-04T11:22:35" color="#ff5c5c">}}

{{<matomeQuote body="リスクをゼロにすることなんて絶対に無理だし、確実性が100%になることもないよね。ありえない確率だけど、量子泡のせいで今夜キミのベッドの上にカバが突然現れて押しつぶされる可能性だってゼロじゃないんだよ。だから「リスクなし」って考えるんじゃなくて「無視できるほどのリスク」って考えた方がいいんじゃないかな。気候変動は無視できないリスクだと思うし、個人的には20%くらいあると思ってる。もしカバがベッドに現れるリスクが20%もあるなら、今夜はベッドで寝ないけどね。" userName="brookst" createdAt="2025-04-04T12:07:30" color="">}}

{{<matomeQuote body="それな、マジでSF。AIの誇大宣伝って感じがするわ。この文章、科学的な言葉で飾られてるけど全然厳密じゃないし。証拠も結論を支持するものもないし、データとか事実に基づいた説明もない。ただの雰囲気でしかない。「AGIは3年以内に実現するってAI企業のCEOが言ってる！」って、まるで自意識過剰な研究みたいで笑える。LLMにプロンプトを書いて競わせるのがプロンプトエンジニアリングで、LLMに“推論”を説明させるのがDeep Research Chain Of Thoughtってこと？" userName="andrepd" createdAt="2025-04-04T13:20:20" color="#45d325">}}

{{<matomeQuote body="彼らがどうやってタイムラインとか能力予測にたどり着いたのか説明してる追加資料、見た？ https://ai-2027.com/research" userName="somebodythere" createdAt="2025-04-04T16:16:20" color="#45d325">}}

{{<matomeQuote body="気候変動の危機は企業の責任じゃないと思うけどな。政治家は支持を失うのが嫌だからガソリンに税金をかけないし。Trumpの貿易政策が酷いことからもわかるように、彼は企業に操られてないでしょ。むしろ、アメリカ人が新聞とかテレビのニュースを見なくなったせいで、ソーシャルメディアが政治の中道を破壊してると思う。EUは、ルーマニアの選挙がTikTokとかメディアの操作で歪められたって言ってるし。" userName="nroets" createdAt="2025-04-04T07:24:16" color="">}}

{{<matomeQuote body="人の心臓にナイフを刺したら、やったのは自分だし、最終的な責任は自分にあるよね。誰かに言われてやったとしても、命令に従っただけだとしても、やったのは自分。心臓にナイフを刺しちゃダメってルールがなかったとしても、やったのは自分だし、責任は自分にある。企業の場合は違うって言うなら、どう違うのか教えてほしい。" userName="baq" createdAt="2025-04-04T07:57:52" color="#38d3d3">}}

{{<matomeQuote body="石油会社は自分たちの製品が経済にとって不可欠だって言ってるけど、間違ってないと思うよ。農場から店に食料を運ぶのも、救急車が病院に行くのも、他の色々なことも、どうやって可能になるの？行動を変えるには税金が一番効果的（小型車に乗って運転を減らすとか、飛行機に乗るのを減らすとか）。だから政府と、彼らに投票した人たちが悪いんだよ。" userName="nroets" createdAt="2025-04-04T08:29:14" color="">}}

{{<matomeQuote body="企業が支援してるボットファームとかシンクタンクとか、そういうのに人々が操られてたらどうなるの？人間は手に入る情報を全部見て、冷静に自分の利益になるように判断するって考えは間違ってると思う。僕らはいつも操られてるんだよ。スーパーで余計な砂糖を買わずに済むことなんてほとんどないし。世界最大の企業は、僕らが衝動買いするような商品を宣伝して肥え太ったんだ。僕らは肉の乗り物に乗った小さなパイロットじゃないんだ。" userName="fire_lake" createdAt="2025-04-04T09:21:53" color="#ff33a1">}}

{{<matomeQuote body="＞政治家は票を失うからガソリン税を課税しない”<br>EUのガソリン税率を見たことある？<br>＞Trumpは企業に操られてない。なぜなら彼の貿易政策は酷いから”<br>彼に市場を崩壊させて金持ちがタイミングよく市場に出入りできるための長い策略だと思わない限りね。<br>＞EUは、ルーマニアの選挙がTikTokアカウントとメディアの操作によって操作されたと言っています。”<br>もっと重要なことにルーマニアの裁判所もそう言っている。そしてそれは公然の事実だったから秘密でもなんでもない。" userName="sofixa" createdAt="2025-04-04T08:57:30" color="">}}

{{<matomeQuote body="未来がどうなるにせよ、アメリカ合衆国じゃないと思う。アメリカの個人主義は資本主義的に武器化されてるし、国を前進させるための教育基盤がない。アメリカはもう終わりで、僕らはただ醜い終焉を見てるだけ。未来はアジアのもので、西洋文化は衰退していく。アメリカの自己統治の失敗実験、って感じだね。" userName="bsenftner" createdAt="2025-04-04T09:23:10" color="">}}

{{<matomeQuote body="同意するけど、もっとマシな状況だと思う。西洋文化が全部終わるわけじゃなくて、過去数百年の間にアジア文化が西洋文化に吸収されたのと同じように、アジアが支配する文化に吸収されるんじゃないかな。アジアの文化の方が教育が進んでて進歩できるなら、それは良いことじゃない？少なくともアメリカは、自分たちの終わりを大声で宣言してるよね。" userName="brookst" createdAt="2025-04-04T12:02:41" color="">}}

{{<matomeQuote body="この話は面白いけど、大きな間違いがあるね。進歩って計算能力とかモデルのサイズだけで決まるもんじゃないんだよ。それってほとんど魔法みたいな考え方だよね。一番大事なのはトレーニングデータだよ。<br>GPT-3の時代には、大量のテキストデータがあったからスケールできたけど、すぐに限界が来ちゃった。だから、合成された推論チェーンとか、ただの合成テキストみたいな別のアイデアを試してるんだ。でも、それを完全にコンピューターの中だけでやることはできないんだよね。<br>新しくて価値のあるテキストを作るには、探索と検証が必要なんだ。LLMはアイデアを出すのは得意だから、そっちは問題ない。でも、検証を自動化できるのは数学とコードだけで、他の分野では無理なんだ。<br>だから、現実世界での検証がボトルネックになるんだよね。世界は秘密を守りたがってるから、それを暴くにはものすごい努力が必要になるんだ。美味しいところはもう全部取られちゃったからね。<br>もし俺が正しいなら、進歩のスピードに影響が出てくるはずだ。検証の摩擦が大きくなるほど、計算能力の向上を妨げることになる。この話では、AIが秘密裏に作られる可能性があるって言ってるけど、それって検証の原則に反してるよね。みんなで協力して検証する方が早いんだから。誰かが秘密裏に人類を出し抜くことはできないんだ。それはブロックチェーンみたいに、みんなに頼ってるんだよ。" userName="visarga" createdAt="2025-04-04T15:07:48" color="#ff5733">}}


{{< /details >}}


[記事一覧へ]({{% ref "/posts/" %}})
