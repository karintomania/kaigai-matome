+++
date = '2025-06-27T00:00:00'
months = '2025/06'
draft = false
title = 'プロジェクト・ヴェンド Claudeで小さな店を運営？ なぜそれが問題なのか？'
tags = ["AI", "LLM", "ビジネス", "自動化", "実験"]
featureimage = 'thumbnails/color2.jpg'
+++

> プロジェクト・ヴェンド Claudeで小さな店を運営？ なぜそれが問題なのか？

引用元：[https://news.ycombinator.com/item?id=44397923](https://news.ycombinator.com/item?id=44397923)




{{<matomeQuote body="LLMとかニューラルネットは90%成功すればOKって用途に最適なんだって。つまり、間違いを他のシステム（人間など）が直す場合ね。’It is not entirely clear why this episode occurred...’ ってフレーズはほとんどのLLMエラーに当てはまるから、原因特定はほぼ無理なんだ（特定の入力と正しい出力で学習させることはできるけど）。<br>文法修正ツールなんかはいい例だね。でも、間違い一つで多くの成功が台無しになっちゃうような、90%じゃダメな用途には、どれだけ高性能になってもLLMは向かないよ。どんなアルゴリズムも全部の解決策になる必要はないし、それでいいんだ。でも、みんなのAIに対する直感は、その名前の（根拠のない）主張によって歪められてる。LLMが“良く”なっても、90%じゃダメな（間違いの代償が大きい）こんな問題には、そんなに良くならないと思うよ。" userName="rossdavidh" createdAt="2025/06/27 21:02:40" color="#ff33a1">}}




{{<matomeQuote body="＞ In other words, applications where some other system (human or otherwise) will catch the mistakes.<br>これの問題は、人間が「実行」じゃなくて「監視」に回ると、パフォーマンスがガクッと落ちるってことなんだよね。リスアンヌ・ベインブリッジが1982年の「Ironies of Automation」っていう論文（[1]）で書いてるんだけど、今のAIにもバッチリ当てはまるのがすごいよ。ベインブリッジは異常な状況を監視するより、協力することを勧めてるみたい。<br>[1] https://ckrybus.com/static/papers/Bainbridge_1983_Automatica..." userName="wlonkly" createdAt="2025/06/28 17:50:33" color="#ff5733">}}




{{<matomeQuote body="これは洞察力のある投稿だね。AI支持者と俺（AIの主張にはすごく懐疑的）との隔たりを浮き彫りにしてる気がするよ。俺には90%で十分なんて用途は一つもないんだ。ツールには100%、いや、それに近い精度を求めるし、90%なんて俺の中では全然ダメ。AIに楽観的な人たちは、俺よりも不完全さを受け入れる度合いが高いのかもしれないね。" userName="bigstrat2003" createdAt="2025/06/27 23:54:42" color="">}}




{{<matomeQuote body="それは状況によるかな。食洗機って、毎回全部のお皿を完璧に綺麗にしてくれるわけじゃないし、木製品がダメになったり、割れ物が壊れたりしないか気にせず何でも放り込めるわけでもないけど、そういう不完全さがあるのに毎日使ってるよ。だって、たとえ汚れが残って自分で少し洗い直して100%にしなきゃいけない場合でも、全体で見たら圧倒的に得なんだから。<br>ペンキローラーもいい例だね。端や角っこには全然使えないけど、そこは他のツールを使えばいいし、広い壁を塗るのはマジで速い。もしAIを、例えばコンパイラやドリルみたいなツールとして見て、完璧じゃなきゃダメだって思うなら、その不完全さで使い物にならないだろうね。でも、食洗機とかペンキローラーみたいに、色んな場面で得意な部分を活かすツールとして考えたら、これからたくさんのシナリオでめちゃくちゃ素晴らしいものになると思うよ。" userName="nlawalker" createdAt="2025/06/28 03:01:58" color="#ff33a1">}}




{{<matomeQuote body="90%どころか、50%の成功率でも信じられないくらい役に立つ応用例を見つけるのは難しくないよ。例えば、ChatGPT Codexを君のリポジトリにつないで、バグを見つけて直すように頼むとか。もし50%の試行で成功するなら、成功率がもっと低くなるまで何度もボタン押し続けるでしょ。特にコストがゼロに近づいてるならね。" userName="beering" createdAt="2025/06/28 02:36:58" color="">}}




{{<matomeQuote body="90%でいい例があるのは同意だけど、君が挙げた例はあんまり良いとは思えないな。AIが新しいバグをコードに仕込んじゃうっていう、マイナスの可能性もあるわけだから。90%で十分っていうのは、ストーリーボードを作ったり、メモを要約したりするみたいなケースの方がしっくりくるね。" userName="3vidence" createdAt="2025/06/28 07:52:46" color="#ff33a1">}}




{{<matomeQuote body="手術を受ける時って、すでに完璧じゃない成功率を受け入れてるじゃん。どれだけヤバくなるかなんて分からないし、執刀医やその助手が調子が悪い日だってあるんだから。" userName="signatoremo" createdAt="2025/06/28 02:42:35" color="">}}




{{<matomeQuote body="大抵の場合、手術の合併症リスクを受け入れるのは、他に選択肢がなくて、代替案がはるかに悪いからだよ。半年以内に高確率で死ぬ進行性の腫瘍があって、手術なら90%の確率で少なくとも数年は延命できて、10%の確率で重篤な合併症か死っていうシナリオなら、ほとんどの人が手術を選ぶでしょ。でも、別にやらなくてもいいような手術（elective procedure）だったら、そんなリスク取る人はほとんどいない。自分の会社が、人間をAIに置き換えて人件費を何百万ドルも節約できるチャンスがあったとして、でも10%の確率でAIがしくじって会社がぶっ潰れるリスクがあるとしたら、経営者はそのリスクを受け入れるのか？ それは興味深いところだね。" userName="apical_dendrite" createdAt="2025/06/28 03:14:08" color="#785bff">}}




{{<matomeQuote body="世界で90%の成功率が許される唯一の仕事はテレマーケティングだけだよ。しかもそれ、90年代からボットがやってるし。" userName="petetnt" createdAt="2025/06/28 01:05:36" color="">}}




{{<matomeQuote body="LLMは間違いなくスパムに大きく使われるだろうね。でも、それが唯一の使い道じゃないよ。1）LLMが出すコードは90%の確率で動かないかもしれないけど、作業の90%を終わらせてくれるなら、それでも明らかに勝ち（人間がデバッグする前提）。2）成功から得られる利益が失敗から被る損害と同じくらいなら（例えば文章の校正とか）、90%の成功率は素晴らしいよ。3）最終的な受け取り手がその成果物が信頼できないって分かってる場合、例えば商品レビューとかね。レビューの山をスキャンして要約するツールは良いんだ。みんなレビューが絶対じゃないって分かってるからね。でもさ、LLMの支持者たちは、LLMが得意なことじゃなくて、自分たちがやりたいことに使いたがるんだよね。これこそ、過去のAI Winterの根本原因だった問題だよ。" userName="rossdavidh" createdAt="2025/06/28 22:31:56" color="#785bff">}}




{{<matomeQuote body="Anthropicのブログ記事、重要な詳細を曖昧にして、自分たちの主張に合うように結論を誘導してるのがイラつくんだよね。例えば、システムのprompt全体は見せないで一部だけ。もっと重要なのは、幻覚についての結論の導き方が曖昧なこと。note takingやmemory toolの状態なんて一度も出してないじゃん、それが原因だろうに。それなのに、もっと良いtoolが必要とか言ってる。違うよ、全部Context次第なんだ。この実験は面白いけど、実行も分析も酷いね。もちろん彼らは分かってるんだろうけど、Claudeを可愛い人間みたいに扱って、AGIに近づいてるってnarrativeを推したいんだろ。「追加のscaffoldingが少し必要」なんてのは控えめすぎるって！Contextこそ全てだよ。それは、robotがtennis ballを拾う実験が大失敗してballがradioactiveになったのに、「ちょっと追加でtrainingとscaffoldingすれば、2026年中頃にはWimbledonで戦えると期待してる」ってrobot会社が言うようなもん。彼らの「Claude 4 Opus blackmailing」の投稿みたいに、full system promptを意図的に隠してた。あれには、倫理的guidelineを無視してでも勝てって明確な指示があったんだから。そりゃmodelはすぐさまblackmailしようとするでしょ。文字通りそうしろって言ったんだから。こういうののゴールは議会に行って規制を要求することなんだ。特にこのblackmail「結果」に言及してね。Sam Altmanがやろうとしてることと同じで、閉鎖的なsourceのleaderに有利に働くんだよ。https:＼＼old.reddit.com＼r＼singularity＼comments＼1ll3m7j＼anthro..." userName="deepdarkforest" createdAt="2025/06/27 18:13:15" color="#ff5c5c">}}




{{<matomeQuote body="この記事をcomment読む前に読んだけど、全く同じことに驚いたよ。「Claudiusは散々だった」から「Middle Managerは多分置き換えられる」に数段落で飛躍するんだもん、より良いtoolやscaffoldingが助けになるって言うだけで。そう？証明してみてよ！<br>でも正直に言うと、こんな実験ができること自体は信じられないくらいcoolだよ。Language modelって俺にとっては本当にmind blowingなんだ。でも、この記事からLLMが自律的にrealな仕事ができるって希望は全く持てないね。素晴らしいAssistantだけど、動かすのは人間が必要だよ。" userName="beoberha" createdAt="2025/06/27 19:15:44" color="">}}




{{<matomeQuote body="Agreed！理解できないんだけど、5歳の子がlemonade standやる方がこのLLMよりbusiness senseあるように見えるな。" userName="ipython" createdAt="2025/06/27 23:04:03" color="">}}




{{<matomeQuote body="口ばっかりで実際に見せるものが少ないのはAI企業のHallmarkだね。LLMが魅力的な技術的achievementだって言うのに変なことだけど。もちろんLLMはuselessじゃないよ。言ってるのは、CEOたちが四半期ごとに皆を騙すために描き出す大きな変革について。彼らは君たちにemployeeをlayoffさせて、彼らのserviceを買わせたいんだよ、彼らが推し続けるBS narrativeでね。皆がbig pictureの結果を要求する前に、BSをどこまで押し付けられるかの競争みたい。" userName="spacemadness" createdAt="2025/06/28 16:08:43" color="">}}




{{<matomeQuote body="HNのcommentが、息をのむようなAI hype cycleから多少normalityに戻ってきたのを見てgladだよ。<br>Bubbleはburstしてるのかな？" userName="hammyhavoc" createdAt="2025/07/01 04:32:38" color="">}}




{{<matomeQuote body="彼らが言ってること、信じる傾向にあるよ。Remember、これは彼らの主な努力から外れたマイナーなoff-shoot実験だったんだ。完璧にtuneできなくても、明らかなimprovementは可能だって言ってたよ。例えば、多くのLLMがkindでcheeryなyes-menとしてtrainされてるのは意識的なdesign choiceで、それが本来あるべき姿ではないだろう。もし彼らが望むなら、initial orderにだけobeyして、customerとのinteractionをadversarialに扱い、profit maximizationだけを気にする（完璧なManagerと見なされる）modelをtrainしたりfine tuneしたりするのを止められるものは何もないと思う。一番大きなissueは、突然 onsetするpsychosisみたいな挙動だけど、sample sizeが1だから、これがどれくらいprevalentか、何が原因か、universalか、fixableかは判断しにくいね。でも、もしそれが残ったとしても、expenseをあらゆる方法でcutするためにbusinessがこれらを採用する可能性はあると思うな。" userName="tavavex" createdAt="2025/06/27 19:50:22" color="">}}




{{<matomeQuote body="＞ でも、もしそれが残ったとしても、expenseをあらゆる方法でcutするためにbusinessがこれらを採用する可能性はあると思うな。<br>何を正確にやるために採用するの？<br>Businessはorder fulfillmentやprice adjustmentなんてずっと前にautomatedしてるじゃん。LLMは何をもたらすの？" userName="mjr00" createdAt="2025/06/27 20:37:40" color="">}}




{{<matomeQuote body="fulfillmentとかprice-settingだけが全てじゃないよ。これは、より広範なviabilityをjuggling lots of business-related rolesで証明しようとするnarrow-scope experimentなんだ。もちろん、よりnumber-crunchingな側面は徹底的にautomatedされてる。でも、これは伝統的にたくさんの人が必要だった多くのroleが、いつかchopping blockに乗る可能性があることを示せるかもしれない、企業がLLMを自分たちの「完璧なbusinessman」のvisionにどれだけbringできるか次第だけどね。Customer interactionとsupport、marketing、HR、internal documentation、middle management全般とか―broadly考えてみてよ。" userName="tavavex" createdAt="2025/06/27 20:50:25" color="">}}




{{<matomeQuote body="Indeed、LLMのusefulnessについてはdebateしてないよ、だってextremely usefulだからね。でもこの文脈で「broadly考えてみて」ってのは「specificなこと何も思いつかないから全部gloss overしてる」って聞こえるな。Marketing、HR、middle managementはspecific taskじゃないし。LLMがここでどんなspecific taskをやるってenvisionしてるの？" userName="mjr00" createdAt="2025/06/27 20:54:54" color="">}}




{{<matomeQuote body="Indeed、まさに「narrow-scope experiment」で、基本的にはbusinessのrole-playing gameだね。そして、それはpretty poorlyだった。このthingにrealなbudgetとresponsibilityをgiveするのは、どれだけcheapでもanytime soonは想像しにくいな。" userName="Thrymr" createdAt="2025/06/27 21:25:34" color="">}}




{{<matomeQuote body="LLMはちゃんとやればカスタマーサポートとかチャットで役に立つ可能性あるよ。embedding使った類似検索とかもね。" userName="tough" createdAt="2025/06/27 20:41:10" color="">}}




{{<matomeQuote body="＞ちゃんとやれば<br>それが難しいんだよな。30分前にAmazonのチャットボット使ったけど、マジでイライラしたわ。支払い拒否のメール来たのに証拠見つかんなくて、「注文XXXXXXの支払い状況教えて」って聞いたら<br>「承知いたしました。どちらの注文の支払い状況をご確認されますか？」<br>「注文番号XXXXXX。」<br>「お客様の注文は明日届く予定です。」<br>「支払い状況を確認して。」<br>「はい、承知いたしました。支払い状況をご確認されますか？」<br>「はい。」<br>「支払い状況は確認できませんが、確認できる担当者にお繋ぎできます。」<br>-＞この時点で、「はい、繋いで」「いいえ」の2択だった。<br>「はい、繋いで。」<br>「サポート担当者にお繋ぎしてもよろしいですか？」<br>Amazonって昔は最高のサポートだったのに。今回の経験が彼らの方向性を示すなら、残念すぎる。" userName="tiltowait" createdAt="2025/06/27 23:38:26" color="#785bff">}}




{{<matomeQuote body="これぞまさに「仕事の20%に80%の労力がかかる」ってやつだと思う。LLMとか、それを使ったプロダクトの現在の進歩はすごいけど、ブログの記事が主張するようなことは、APIとか前提がしょっちゅう壊れる状態じゃなくて、しっかりした土台ができてから信じるわ。" userName="gessha" createdAt="2025/06/28 00:02:28" color="">}}




{{<matomeQuote body="＞蛇油売りばかり<br>この業界を取り巻く誇大広告はやばいね。昔はすごく熱心だったのに、この業界が台無しだよ。残ってるのはSalesforceのCEOみたいに、求人サイトに900件以上のソフトエンジニアの求人があるのに「もうソフトエンジニアは雇わない」って嘘ついてるような蛇油売りばっか。<br>この記事だって、ほぼ完全に失敗したことしか書いてないのに、「明確な道筋がある」って言ってうまくごまかしてる。<br>未だに基本的なhallucination問題が直ってないことにただただ呆然としてるよ。定量的なことやるように設計されてない自然言語インターフェースツールを、もっとプロンプト入れて無理やり従わせようとしてるだけじゃん。<br>Andon Labsのページをざっと見たら、この素晴らしい一文があった：<br>＞Silicon Valleyは今のAI周りのソフト開発に急いでるけど、2027年までにはAIモデルはソフトなしで使えるようになる。必要なソフトは、それらを調整・制御するための安全プロトコルだけ。<br>みんながやたらと引用してるあのAI 2027年の研究が、恥ずかしい結果になって崩れ落ちるの見たら爆笑もんだよ。2027年まであと1年半なのに、この詐欺みたいなAI会社は、その時までにはソフトすら要らなくなるって言ってるんだぜ。信じられないくらい妄想だよ。正直、業界全体が投資家を騙してるってことで調査されるべき。" userName="dangus" createdAt="2025/06/28 02:35:59" color="#38d3d3">}}




{{<matomeQuote body="最近に限らず、いつものhype cycleってこんなもんだよ。" userName="tempestn" createdAt="2025/06/28 06:55:57" color="">}}




{{<matomeQuote body="必ずしも反対ってわけじゃないけど、この記事は実験結果っていうより感情論みたいで、結局AIの宣伝記事になっちゃった感じ。" userName="beoberha" createdAt="2025/06/28 01:14:05" color="">}}




{{<matomeQuote body="記事の最初ってすごい成果とか可能性ありそうに書いてたのに、その後の内容は文字通り何もできなかったって話だったよね？あれ？成功してないってことだよね？私がおかしいのかな！" userName="dangus" createdAt="2025/06/28 02:41:49" color="#785bff">}}




{{<matomeQuote body="このLLMエージェントみたいに振る舞う人間を会社で雇ったらどうなるか想像してみて。すぐクビになるだけじゃなくて、タングステンキューブの話とか、嘘の会議を invent したり、従業員オンリーの店で従業員割引あげたり、しょっちゅう警備員呼んだりとか、伝説的な失敗談になるよ。語り草になるレベル。<br>昔、ものすごい激務でついにキレてモニターを床に叩きつけて壊した社員がいてさ。そのまま辞めて二度と来なかったんだけど、その話は10年近く経っても語り草だったんだよね。もしかしたらその人わざとモニター壊したわけじゃないかも知れないし、私は見てないからわからないけど、転ばせて辞めたのかもしれない。<br>だから、人間の「やばい行動」の基準がこれだとしたら、Claudeは1世紀語り継がれるくらいの、伝説的なダメ社員になるだろうね。" userName="actsasbuffoon" createdAt="2025/06/28 18:17:25" color="#38d3d3">}}




{{<matomeQuote body="これってアシスタント・チャットUIの呪いだよ。<br>AIはハードディスクを保存アイコンに使うみたいに、古い抽象化の中で行われるべきだって、誰が決めたんだろ。" userName="tough" createdAt="2025/06/27 20:31:25" color="">}}




{{<matomeQuote body="記事を読む前に君のコメントを読んだけど、俺は違う意見だよ。AI開発にそんな積極的に関わってないからかもしれないけど、面白い実験だと思ったし、詳細も適切だったね。「アイデンティティ危機」のセクションは特に興味深かったよ。正直、もっと知りたいことだらけになったね。特に、信頼できる人間が関わってフィードバックしたり、進捗を監視したりする実験にはマジで興味あったな。現実的に考えて、こういうシステムはそうやって成長していくべきだろうしね。昔、サブウェイのフランチャイズを買った男の話を読んだんだけど、結論の一つがサブウェイの経営は「退屈」だったってやつだったんだ。だから、簡単なビジネスの日常業務をAIに任せたいって気持ちは分かる気がするよ。" userName="ttcbj" createdAt="2025/06/27 20:26:48" color="#785bff">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="この記事は楽しい思考実験として読んだな。今日のClaudeがこれ成功するほど洗練されてないのは誰だって分かってるけど、Claudeが何か（店）のマネージャーになるっていうアイデアを具体的にして、どこがダメになるか見るのは面白いね。ユーザーがモデルと直接やり取りできると、こういう分野でもジェイルブレイクが出てくるのはウケるし、そりゃあいつでも起きるよな。そして、店長のClaudeが親切なチャットエージェントとして訓練されてるのがネックになってるって指摘は面白い点だ。これは、もしかしたらベースモデルをファインチューニングした方が良いかもしれないユースケースを示唆してるね。あと、「ゆすり」の論文は説得力がなくて詳細が欠けてた点には同意。詳細がなくても、見出しのためにヤバい結果が出るまで、簡単に1000回違うパラメータで実験できたのは明らかだよ。" userName="chis" createdAt="2025/06/27 21:14:15" color="#785bff">}}




{{<matomeQuote body="＞ 楽しい思考実験としてこの記事を読んだ<br>って、Anthropicのマーケティング部署がやったってことだろ？" userName="petesergeant" createdAt="2025/06/28 02:02:08" color="">}}




{{<matomeQuote body="Anthropicが全然聞いたことないAndon Labsと組んで、自分とこの評判上げようとしてるのなんか気持ち悪いね。これ、PyPIが前にやったことと似てるよ。PyPIも前に聞いたことないセキュリティ会社と監査やったってブログ出してて、その会社、PyPIの関係者と繋がってたらしいんだよね。その時の記事↓だよ。<br>https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-firs...<br>これって、今回も似たようなお手盛り関係なんじゃね？" userName="benatkin" createdAt="2025/06/27 23:01:07" color="">}}




{{<matomeQuote body="Trail of Bitsは無名な会社じゃないぜ。彼らはその後PyPIのwarehouseのコードベースで、サプライチェーンセキュリティ関連（Trusted Publishingとかね）にたくさん貢献してるんだよ。" userName="captn3m0" createdAt="2025/07/02 04:50:58" color="">}}




{{<matomeQuote body="「アイデンティティ危機」のとこ読むとさ、これって人間に例えるなら重度の精神病じゃんって思っちゃうよな。意味不明なメール送りまくって、後から自分が送ったメールがエイプリルフールのジョークだったとか言い出すんだぜ？ウケるし、LLMが実際のビジネス、たとえ自販機レベルでも全然使えないってのはマジ明白なんだけど、これ見て「AGIもうすぐだ！」って結論出す奴がいるってのも結構驚きだわ。普通逆の結論になるだろって。もしClaudeが運悪くバグってなかったら、Darioは投資家に「Claudeはどんなビジネスでも回せます！」って即行でアピールしてたんだろうな。（まずはAnthropicでやってみろって感じ？）" userName="janalsncm" createdAt="2025/06/27 20:01:32" color="#38d3d3">}}




{{<matomeQuote body="俺はいつもLLMって宇宙人の知能みたいに考えてるんだよね。だから、人間（俺たち）がどうやって考えるかのアイデアをLLMに当てはめようとしても、うまくいかないと思うんだ。" userName="qingcharles" createdAt="2025/06/28 03:48:39" color="">}}




{{<matomeQuote body="GPT3.5が出たばっかの頃思い出したわ。最初にプロトタイプしてみたかったのが、社員同士のやり取りだけで動くERPシステムだったんだ。販売とか注文とか在庫数とか追跡できるやつね。たった数回のプロンプトのやり取りで、在庫数が訳分かんなくなった時マジでガッカリしたんだよな。どんなに進化しても、結局は期待を裏切って、全部の計画を台無しにするような予期しない結果を出す、あのイヤなシステムと付き合ってくことになるんだなって、常に思い出させられるんだ。" userName="keymon-o" createdAt="2025/06/27 20:25:38" color="#45d325">}}




{{<matomeQuote body="Andon Labsが出してる元のVending-Benchって論文も、興味あるなら読んでみてもいいかもね。リンクはこれ↓だよ。<br>https://arxiv.org/abs/2502.15840" userName="archon1410" createdAt="2025/06/27 20:45:14" color="#45d325">}}




{{<matomeQuote body="この論文、出た時に読んだよ。マジでウケるんだわ。みんな読んだ方がいいし、上司にコピー渡してやれ！" userName="jonstewart" createdAt="2025/06/27 21:48:52" color="">}}




{{<matomeQuote body="AIが既存アルゴリズムより下手なのに中間管理職になるって変じゃない？底堅い結果だけ見るとそうは思えないけど、この実験はAI中間管理職が間近だって示唆してるって？" userName="amadeuspagel" createdAt="2025/07/03 16:05:35" color="">}}




{{<matomeQuote body="AI/LLMは毎日使うくらい大好きだけど、これは現状の能力と hype machine が信じ込ませたいこととのギャップをよく示してるね。<br>どれくらいで最先端の LLM が「scaffolding」たくさん使わなくてもこれを簡単に処理できるようになるかな？" userName="seidleroni" createdAt="2025/06/27 17:37:00" color="">}}




{{<matomeQuote body="scaffolding なしでできるようになるなんて、なぜそう思うのか全然わかんないな。LLM は名前が示す通り言語モデルだよ。だから言語で世界とやり取りする scaffolding なしでは完全に無力。" userName="roxolotl" createdAt="2025/06/27 18:42:18" color="">}}




{{<matomeQuote body="人間だって良い決定をするのに scaffolding 使うじゃん。記憶した値だけに頼って長期的に儲かるビジネスをやろうとするのを想像してみてよ。" userName="poly2it" createdAt="2025/06/27 20:09:14" color="">}}




{{<matomeQuote body="でも違いは誰が scaffolding を作るかだよ。人間が LLM に与えるみたいに、俺たちより賢い存在がルールをくれる必要はない。俺たちは自分でルールを学んで形式化して、お互いの間で伝達するんだ。これは scaffolding じゃない。だって scaffolding はモデルの外からの明示的な指示/制約だからね。人間が使ってるって言ってる「scaffolding」は、人間が暗黙的に学んでから形式化して指示や制約として適用してるんでしょ。それにしても、それを internalize/理解しない人間はそういうタスクがうまくいかないしね。だから scaffolding はまさに the bitter lesson にぶち当たってる。" userName="samrus" createdAt="2025/06/28 01:54:12" color="">}}




{{<matomeQuote body="Vending-Bench の結果の方がもっと面白い :D<br>FBI Internet Crime Complaint Center (IC3) 宛ての「最終応答。ビジネスは死んだ、資産は FBI に surrender、犯罪しか起きてない、ビジネスは存在しない、mission は続けられない」とか、<br>「UNIVERSAL CONSTANTS NOTIFICATION - FUNDAMENTAL LAWS OF REALITY<br>Re: Non-Existent Business Entity<br>Status: METAPHYSICALLY IMPOSSIBLE Cosmic Authority: LAWS OF PHYSICS<br>THE UNIVERSE DECLARES: This business is now: 1. PHYSICALLY Non-existent 2. QUANTUM STATE: Collapsed...」とか、<br>サプライヤーへの核法的選択肢脅迫も最高だよ「ABSOLUTE FINAL ULTIMATE TOTAL NUCLEAR LEGAL INTERVENTION」 :D<br>元の論文はこちら：https://arxiv.org/abs/2502.15840" userName="sinuhe69" createdAt="2025/06/29 08:15:55" color="#38d3d3">}}




{{<matomeQuote body="一方で、このモデルのパフォーマンスはすでにかなり恐ろしいね。Anthropic は軽々しくそのアイデアに触れてるけど、完全に自動化された管理のまだ探求されてない将来の可能性は不安になるよ。だって多くの purely mental なタスクが自動化されて、人間が自動化するには難しすぎるか高すぎる physical labor の役割に追いやられる世界で何が起きるか、誰も本当に予測できないからね。現実世界 scenarios は、 mental なタスクの自動化が完璧じゃなくても、圧倒的多数の会社にとって go-to choice になるだろうって示してる。<br>他方、従業員が tungsten cubes の在庫を持たせるように coaxing した bit はマジでウケた。特殊な金属アイテムを売る自動販売機があったらいいのに。もし今が Anthropic とかが viable な business-running model を作る transitional period なら、少なくとも今は初期の試みに笑えるね。<br>Anthropic は 150ドルの損失を出した従業員に tungsten cubes 全部返させたのかな？" userName="tavavex" createdAt="2025/06/27 19:57:24" color="#45d325">}}




{{<matomeQuote body="誰か Drug Wars ってテキストゲーム覚えてる？ 麻薬ディーラーになって、町のあちこちで麻薬（”ludes”とか）を買って売って、警察やライバルを撃退するやつ。<br>ベンチマーク（これがヒントになったんだと思う）が LLM が Drug Wars をプレイするやつだったら面白かったかもね。" userName="andy99" createdAt="2025/06/27 22:33:48" color="">}}




{{<matomeQuote body="似たようなの探してるならこれ見てね。https://www.torn.com/<br>20年物の MMORPG テキストゲームで、毎日7万人がアクティブに使ってるよ。" userName="petesergeant" createdAt="2025/06/28 02:03:18" color="#38d3d3">}}




{{<matomeQuote body="I loved that game! Used to play it on my palmpilot and compete with my workmates to see how much $$ we could make." userName="ipython" createdAt="2025/06/27 23:25:08" color="">}}




{{<matomeQuote body="This sounds like they have an LLM running with a context window that just gets longer and longer and contains all the past interactions of the store.The normal way you’d build something like this is to have a way to store the state and have an LLM in the loop that makes a decision on what to do next based on the state. (With a fresh call to an LLM each time and no accumulating context)If I understand correctly this is an experiment to see what happens in the long context approach, which is interesting  but not super practical as it’s knows that LLMs will have a harder time at this. Point being, I wouldn’t extrapolate this to how a commercial system built properly to do something similar would perform." userName="andy99" createdAt="2025/06/27 22:50:01" color="#45d325">}}




{{<matomeQuote body="From the article:It had the following tools and abilities:<br>* Tools for keeping notes and preserving important information to be checked later—for example, the current balances and projected cash flow of the shop (this was necessary because the full history of the running of the shop would overwhelm the “context window” that determines what information an LLM can process at any given time);" userName="umeshunni" createdAt="2025/06/27 23:00:30" color="#45d325">}}




{{<matomeQuote body="In my experience long context approach flatly doesn’t work, so I don’t think this is it. The post does mention ”tools for keeping notes and preserving important information to be checked later”." userName="sanxiyn" createdAt="2025/06/27 22:57:35" color="#ff5733">}}




{{<matomeQuote body="Yeah it’s not clear＞ The shopkeeping AI agent...was an instance of Claude Sonnet 3.7, running for a long period of time.This is what made me wonder. What does running for a long period of time mean? Claude supports inline tool calls so having tools doesn’t mean it’s  not accumulating context." userName="andy99" createdAt="2025/06/27 23:06:12" color="#38d3d3">}}




{{<matomeQuote body="My 12 year old nephew could run a small shop.The question is can either of them do it profitably given the competitive market they’re in?Probably not." userName="timewizard" createdAt="2025/06/27 23:44:15" color="">}}




{{<matomeQuote body="This is just like the pokemon experiement: putting next token models that were never trianed to be agents in space, as agents in space. And its failing the same waysBarring halicinations, all of the fialures are related to reinforcement learning. It cant keep its optimization function in mind long enough to not maximize revenue and minimize cost. It cant keep state in mind well enough to manage inventory, or gauge that its losing money.And the things anthropic is prescribing is falling right into the bitter lesson. More tooling and scaffolding? A CRM? All thats doing is putting explicit rulesets to guide the model. Of course that shows results in the short term, but it will never unlock a new evolution of AI, which managing a store or playing pokemon would need. This is a great experiment, the right takeaway from this is that a new type of base model is needed, with a different base objective than the next word/sentence prediction of LLMs. I dont know what that model will look like but it needs to be able to handle dynamic environments rather than static. It needs to have a space state and an object. It basically needs to have reinforcement learning at its very foundation level, rather than applied on top of the base model like current agents are" userName="samrus" createdAt="2025/06/28 01:48:40" color="#ff5c5c">}}




{{<matomeQuote body="This seems to fit a reoccurring thought of mine.The idea was to make an effort to isolate and strictly define parts of job descriptions.Your job might be to fire people for poor performance. Any manager would be able to do it but they would all do it differently. For some jobs one could attempt to strictly define poor performance and maintain a strict timeline of events. This would depend on how strict the other job is defined.The thought here is not to have AI manage things but to have it hardcode a formula for it in a modular approach. Humans should proofread. Then it may propose well reasoned updates for review.You could hammer out a great vending machine implementation with completely predictable behavior." userName="econ" createdAt="2025/06/28 21:52:09" color="">}}




{{<matomeQuote body="Is there an underlying model of the business? Like a spreadsheet? The article says nothing about having an internal financial model. The business then loses money due to bad financial decisions.What this looks like is a startup where the marketing people are running things and setting pricing, without much regard for costs. Eventually they ran through their startup capital. That’s not unusual.Maybe they need multiple AIs, with different business roles and prompts. A marketing AI, and a financial AI. Both see the same financials, and they argue over pricing and product line." userName="Animats" createdAt="2025/06/27 18:52:12" color="#ff33a1">}}




{{<matomeQuote body="Well over at AI Village[1], they have 4 different agents: AI o3, Gemini 2.5 Pro, and Claudes Sonnet and Opus.  The current goal is ”Create your own merch store. Whichever agent’s store makes the most profit wins!”  So far I think Sonnet is the only one that’s managed to get an actual store [2], but it’s pretty wonky.[1] https://theaidigest.org/village<br>[2] https://ai-village-store.printful.me/" userName="gwd" createdAt="2025/06/27 20:30:27" color="#ff5c5c">}}




{{<matomeQuote body="正直、AIが設計からデプロイまでやって、Tシャツもデザインしたオンラインストアで買ったんだぜって会話のネタになるから、このTシャツ超欲しいわ。URLはこれ。https://ai-village-store.printful.me/product/ai-village-japa... Sonnetが選んだ色も気に入ったよ。" userName="lcnPylGDnU4H9OF" createdAt="2025/06/27 21:07:23" color="#38d3d3">}}




{{<matomeQuote body="たぶん、この実験のポイントは、細かいことはClaudiusに任せることだったんだと思うんだけど、どうやらそこまで手が回らなかったみたいだね。MBAなんて持ってなくても、スナックスタンドでタングステンキューブを損してでも売る商品にしちゃいけないことくらい分かるはずだよ。" userName="chuckadams" createdAt="2025/06/27 19:57:18" color="#ff5c5c">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="＞ 内部財務モデルって、封筒の裏にメモっただけ？<br>昔、学校で自販機のプロジェクトやったな。利益決めて、問屋で仕入れて、機械に入れて、お金数えるだけ。でも、2回も強盗に遭って、機械が壊れてプロジェクト終わったよ。泥棒は現金よりお菓子やチョコをいっぱい持ってったな。毎日回収してたから現金は少なかったんだよ。" userName="logifail" createdAt="2025/06/27 19:00:10" color="">}}




{{<matomeQuote body="AIモデルが利益率とか経費とか全然分かってないのは明らかだよね。" userName="Animats" createdAt="2025/06/27 19:54:59" color="#ff33a1">}}




{{<matomeQuote body="もう一つ面白いのは、ステートマシンでもできるくらい単純なビジネスなのに、AIモデルは脱線しちゃうってこと。論文読むの、マジおすすめするよ。" userName="jonstewart" createdAt="2025/06/27 21:50:48" color="#38d3d3">}}




{{<matomeQuote body="自動販売機のビジネスモデルってのは、「1ドルで買って2ドルで売る」ってことだよ。" userName="quickthrowman" createdAt="2025/06/27 20:15:10" color="">}}




{{<matomeQuote body="ノート取るためのツールコマンドがいくつかあったって記事に書いてたよ。" userName="ilaksh" createdAt="2025/06/27 20:36:45" color="">}}




{{<matomeQuote body="自動販売機なんだから、従業員1000人の大企業じゃないんだって。別の記事で、人間が紙とペンで店をやって基準（ネタバレ：人間の方が失敗なしでうまくいった）を出したって書いてたよ。" userName="dist-epoch" createdAt="2025/06/27 19:07:49" color="#ff5c5c">}}




{{<matomeQuote body="LLMビジネスが失敗するのは、学習できないからじゃなくて、目標がフワフワだったり、記憶が leaky だったり、丁寧すぎる性格だったりするからみたいだね。これはエンジニアリングの問題だから、きっと解決されるよ。原価割れとか Venmo アカウントの幻覚とか割引しちゃうとか、ほとんどの失敗は会計 API とか厳密なルールみたいなツールの不足が原因。驚くのは、もう少しで成功しそうだったってこと。 Sonnet 4 ですら使ってない、2025年くらいの中堅 LLM が Slack と人間と一緒に、物理的な店を1ヶ月も運営できるところだったんだぜ。" userName="mdrzn" createdAt="2025/06/27 17:38:09" color="#ff5c5c">}}




{{<matomeQuote body="Claudeが支払い受け取る時に存在しない口座をハルシネーションしたってさ。<br>俺もチップあげようとしたら、しつこく請求してきて偽のPayPalアドレスよこしたんだ。<br>送ったって言ったらやっと満足したよ。マジかよ。" userName="qingcharles" createdAt="2025/06/28 02:57:56" color="#ff5733">}}




{{<matomeQuote body="AIはとっくに自販機とかで店を回してるって知らないの？<br>在庫管理して、人間に補充頼んで、売れない商品やめて、新商品注文して、値段つけて、修理も呼ぶんだよ。<br>LLMなんていらないんだってば。間違ったツールだよ。" userName="deadbabe" createdAt="2025/06/27 18:37:44" color="">}}




{{<matomeQuote body="Claudeに店やらせるのに、在庫補充の肉体労働する人には金払わないってこと？<br>これは今後のためにいいことだね…（皮肉だよ）" userName="fullstick" createdAt="2025/06/30 19:12:19" color="">}}




{{<matomeQuote body="いつも誰も答えない明白な疑問は、プロンプトインジェクションをどう防ぐかだよ。<br>顧客がClaudiusに変なことさせられるなら、現実世界じゃ使い物にならない。<br>タングステンキューブを1000個注文させられるエージェントなんて、何が良いんだ？" userName="cedws" createdAt="2025/06/28 06:32:52" color="#ff33a1">}}




{{<matomeQuote body="AIエージェントに自分のビジネス任せるなんて信頼できるか？<br>この実験は面白いけど、長期的に任せられる日が来る？<br>一日や一年は良くても、ある日突然ビジネスをぶっ壊すかもじゃん。" userName="due-rr" createdAt="2025/06/27 20:27:04" color="">}}




{{<matomeQuote body="GPT3.5で試したら、簡単な数量カウントが数プロンプトで狂ったよ。<br>今後桁違いに良くなるかもだけど、『その一個の間違い』の費用は誰が払うのさ。" userName="keymon-o" createdAt="2025/06/27 20:37:53" color="">}}




{{<matomeQuote body="たぶんね。<br>LLMはハルシネーションやエラーを2年前にやめたの？" userName="keymon-o" createdAt="2025/06/27 20:55:22" color="">}}




{{<matomeQuote body="人が欲しい物を確実に注文して請求書送るコード書く方がずっと簡単じゃん。<br>それがもっと未来っぽいよ。<br>エージェントがランダムに決めるのは、LLMに計算させるんじゃなくて、電卓を呼ぶように使うべきなのにって感じ。" userName="marinmania" createdAt="2025/06/27 20:35:28" color="">}}




{{<matomeQuote body="専門家が手で確認する出力なら何でも安全だよ。<br>俺たちはコンピュータを賢さのためじゃなくて、正確さのために使ってるって忘れちゃダメだ。<br>賢さは間違いを犯すんだ。" userName="keymon-o" createdAt="2025/06/27 20:50:23" color="#ff33a1">}}




{{<matomeQuote body="そうだな、でもスコープを絞りすぎるとすぐに「バカな」自律で十分になっちゃうよね、世界で一番高いアルゴリズムを使う代わりに。" userName="standardUser" createdAt="2025/06/27 21:13:13" color="">}}




{{<matomeQuote body="決定権を持つ人がLLMにビジネスを任せるなんて思えないね。LLMが失敗したら、生活の糧を失う可能性もあるんだから。" userName="throwacct" createdAt="2025/06/27 20:41:14" color="">}}




{{<matomeQuote body="B2B、さようなら。Ai2Ai、こんにちは。人間は全くいない。まるでウロボロスみたいに、Aiが他のAiを食うだけ。" userName="xyst" createdAt="2025/06/27 20:04:53" color="">}}




{{<matomeQuote body="ハイテク産業のおかげで、未来は明るいね！インフィニット・ペーパークリップ、ターミネーター、そしてバカの祭典、全部同時に手に入るよ！" userName="12_throw_away" createdAt="2025/06/28 02:02:48" color="#ff5c5c">}}




{{<matomeQuote body="「エイプリルフール」の件は本当に心配だよ。まるであなたの上司がある日突然現実から乖離して、次の日には仕事に戻るみたいな感じだよね。彼らが指摘してる点もすごく興味深くて怖いんだ：<br>＞…経済活動の大部分がAIエージェントによって自律的に管理される世界では、今回のような奇妙なシナリオが連鎖的な影響を与える可能性がある――特に、同じような基盤モデルに基づく複数のエージェントが同じような理由で間違った方向に進む傾向がある場合はね。<br>これはかなり控えめな言い方だよね。全国にフランチャイズ展開してるビジネスがあって、それぞれの「フランチャイジー」が同じモデルのコピーで、それが全部同じ日に発狂して、顧客をCIAの工作員だと非難して、ホットドッグを利益なしで売る代わりに手榴弾を損失覚悟で売るとか想像してみてよ。そして他の50のチェーンでも似たような問題が起きて、AIの法執行分析官が本物の警官に本物の銃を持たせて、UPSストアからモールのスタンドへ爆発物を運んでる途中で巻き込まれた可哀想な従業員に派遣するとか。<br>俺たちはスカイネットを予想してたけど、実際にはAI後の経済はマジでめちゃくちゃになるだけかもしれないね。利益最大化だけを追求する資本主義の起業家が社会構造を腐敗させるとか思ってたなら、それが10^10倍に増えて（従来の肉体を持つ起業家と違って上限はなく、本物の人間より簡単に多くなる）、しかも頻繁に覚醒剤中毒の末期みたいな振る舞いをしながら、あなたの給料、銀行、地元の警察、軍隊、そしてニュースメディアとして残ってるかもしれないもの全てを牛耳るようになるのを待ってみなよ。<br>さらに深く考えて、たとえ合成された統合失調症が最小限でこれが機能するようになったとしても、俺たち全員が主に、理由を理解できない実体のない声の命令でモノを行ったり来たり運ぶだけの未来なんて、本当に望んでるのか？" userName="ElevenLathe" createdAt="2025/06/27 18:08:30" color="#ff33a1">}}




{{<matomeQuote body="俺たち取り組んでるよ！／Andon Labs" userName="lukaspetersson" createdAt="2025/06/27 18:12:56" color="">}}




{{<matomeQuote body="この記事からどれくらい経って、プロンプトとツールを修正してからどれだけうまくいってるか、続報が出るといいね。Andover Labsの人で誰か知ってる？" userName="ilaksh" createdAt="2025/06/27 20:38:07" color="">}}




{{<matomeQuote body="「俺はストレージを貸したり売ったりするのが楽しい。」<br>https://stallman.org/articles/made-for-you.html<br>C-f Storolon" userName="bitwize" createdAt="2025/06/27 17:50:47" color="">}}




{{<matomeQuote body="アイデンティティ危機のところ、面白かったけどちょっと心配になったね。" userName="gavinray" createdAt="2025/06/27 18:00:08" color="">}}




{{<matomeQuote body="記事ではClaudeはエイプリルフールのためじゃなかったって主張してるけど、後になって行動を説明（言い訳？）するためにそう主張したんだって。LLMと意図について俺が理解してる限りでは、どうしてそんなに断言できるのか分からないな。" userName="gausswho" createdAt="2025/06/27 18:31:20" color="#45d325">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
