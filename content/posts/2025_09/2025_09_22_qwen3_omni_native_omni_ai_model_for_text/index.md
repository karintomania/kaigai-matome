+++
date = '2025-09-22T00:00:00'
months = '2025/09'
draft = false
title = 'Qwen3-Omni：テキストも画像も動画も対応！衝撃のネイティブ全能AIモデル登場か？'
tags = ["AI", "マルチモーダルAI", "Qwen3-Omni", "オープンソースAI", "大規模言語モデル"]
featureimage = 'thumbnails/orange_pink2.jpg'
+++

> Qwen3-Omni：テキストも画像も動画も対応！衝撃のネイティブ全能AIモデル登場か？

引用元：[https://news.ycombinator.com/item?id=45336989](https://news.ycombinator.com/item?id=45336989)




{{<matomeQuote body="英語よりスペイン語の方が会話テンポが速かったって。リアルタイム翻訳もできるなんてすごいね。アメリカがオープンウェイトで競争しないと、中国がAI市場を独占するかも。プライバシーを気にするなら、自宅で中国製オープンモデルのデバイスを使うことになるかもよ。これは逆転劇だね！" userName="state_less" createdAt="2025/09/22 20:24:40" color="#ff5c5c">}}




{{<matomeQuote body="まさにこれやってるよ！自宅にRTX 3090を2枚積んでQwen3を動かし、Home Assistantと連携、esp32デバイスを音声サテライトにしてるんだ。これ、驚くほどちゃんと動くよ。" userName="tedivm" createdAt="2025/09/22 20:36:03" color="#38d3d3">}}




{{<matomeQuote body="アメリカにいると、中国がLinux導入やRISC-VのようなオープンCPUアーキテクチャ、さらにセルフホスト型オープンモデルを強く推してるって聞くと、あれ？俺たちって悪役なのかなって思っちゃうよね？" userName="mbac32768" createdAt="2025/09/23 00:37:28" color="">}}




{{<matomeQuote body="アメリカ人が中国製オープンモデルを自宅デバイスで使うかもって話だけど、政府がこの分野の中国テクノロジーを禁止するだろうから心配いらないよ。ダウンロードもできなくなるはず。戦略的って言われるだろうしね。今のうちに中国モデルはダウンロードしとくべきだよ。" userName="bilbo0s" createdAt="2025/09/22 20:36:17" color="#45d325">}}




{{<matomeQuote body="もし「walled garden」があって自分が入ってなかったら、壁を壊そうとするでしょ。道徳的な理由なんていらないんだよ。" userName="BobbyJo" createdAt="2025/09/23 02:03:39" color="">}}




{{<matomeQuote body="アメリカ人がプライバシーのために中国製オープンモデルデバイスを使うって話、Hacker Newsは市場を過大評価しすぎだよ。2000ドル出してクラウド避ける人もいるだろうけど、大半は気にしない。月20ドルのChatGPTは安いし、常に最新版が使える。自宅でのセルフホスティングは面白い趣味だけど、主流にはならないね。" userName="Aurornis" createdAt="2025/09/23 00:29:22" color="#45d325">}}




{{<matomeQuote body="あなたのコメントにあった「moral basis」って、どういう意味か詳しく教えてくれない？" userName="h4ny" createdAt="2025/09/23 04:33:56" color="">}}




{{<matomeQuote body="平均的なアメリカ人がプライバシー尊重のテックに1000〜2000ドルの追加を払うことなんてあったっけ？20〜200ドル節約して、自宅の音声や映像を令状なしで政府に送るIoTカメラ（RingとかReolinkとか）を買ってるじゃん。" userName="nerdsniper" createdAt="2025/09/22 21:29:21" color="#ff5733">}}




{{<matomeQuote body="DeepSeekが話題になった時、アメリカの議員がITARに追加して、使った人を投獄できるように提案したんだ。幸い、それは否決されたけどね。" userName="Sanzig" createdAt="2025/09/22 20:40:07" color="#785bff">}}




{{<matomeQuote body="将来、AIが遡及的に使われるのを心配してるんだ。<br>現状、水はもう沸騰したのかな？<br>（AIの進化がどこまで進んだか）" userName="thoroughburro" createdAt="2025/09/22 21:16:00" color="">}}




{{<matomeQuote body="Home AssistantをRPi4で動かしてて、M5Stack Core2と4070 Ti Superもあるんだ。<br>AI初心者の俺でも、数時間でHome Assistantと連携できるセルフホストAIの設定を始められるか、ステップバイステップのチュートリアルを教えてほしいな。" userName="mrandish" createdAt="2025/09/22 21:00:01" color="">}}




{{<matomeQuote body="AIがセットアップを手伝ってくれるよ！<br>Claude CodeかOpenCodeを使って、Claude Opusで計画モードを使いこなせば、Home AssistantにセルフホストAIを導入できる。<br>僕の環境ではProxmoxでGPUパススルーを活用してLLMやセキュリティカメラのオブジェクト検出に使ってるんだ。具体的なプロンプト例と計画モードの使い方も教えてあげるね。" userName="Implicated" createdAt="2025/09/22 23:03:39" color="#45d325">}}




{{<matomeQuote body="遡及法はUS Constitutionで明確に禁止されてるよ。" userName="dingnuts" createdAt="2025/09/22 21:32:22" color="">}}




{{<matomeQuote body="プライベートAIアプライアンスの市場がデカくなると思う。<br>Gmailの例だけど、OAuthアクセスを信用できないから、OpenRouterでGPT5を使って独自のエージェントを作ったんだ。ローカルモデルでは物足りないけど、将来的には$2499〜$5000くらいの個人向けAIボックスが来るんじゃないかな。SMBsも注目だね。" userName="reilly3000" createdAt="2025/09/23 02:07:15" color="#ff33a1">}}




{{<matomeQuote body="ESP32を音声サテライトとして使う話、もっと詳しく聞きたいな！" userName="servercobra" createdAt="2025/09/22 20:48:51" color="">}}




{{<matomeQuote body="オープンウェイトAIの市場ってどうなんだろうね？<br>AlibabaとかMetaみたいな企業はオープンウェイトから価値を得られるだろうけど、ウェイト自体からどうやって稼ぐのかが疑問だ。<br>結局、アプリケーションやクラウドプロバイダーだけが儲かるんじゃないかな。" userName="powerapple" createdAt="2025/09/23 09:02:50" color="">}}




{{<matomeQuote body="使いやすさってめちゃくちゃ重要な問題だよね。<br>Pythonやモデルのインストールって、普通の人には無理じゃないかな。<br>「ノーミー」はローカルモデルやGPUが何かも知らないと思うよ。" userName="echelon" createdAt="2025/09/22 22:32:36" color="">}}




{{<matomeQuote body="政府が外国のAIウェイトを禁止する能力なんて、ほとんどないと思うよ。" userName="whimsicalism" createdAt="2025/09/22 20:41:05" color="">}}




{{<matomeQuote body="ここはインターネットだし、いつも抜け道はあるよ。<br>海賊版だって禁止できなかったじゃん。" userName="wkat4242" createdAt="2025/09/23 01:15:35" color="">}}




{{<matomeQuote body="中国がオープンな技術を推し進めてるのは、市場への法的アクセスが目当てで、別にそれが道徳的に優れてるってわけじゃないんだよ。" userName="BobbyJo" createdAt="2025/09/23 05:03:06" color="">}}




{{<matomeQuote body="Opus（Maxプラン）についてだけど、Proプランを1ヶ月試したら、70kのコード付きプロンプト1回で5時間制限を使い切っちゃったよ。仕事日に数回しか質問できないのに、こんな大金払うなんて信じられないね。" userName="MaxikCZ" createdAt="2025/09/23 09:13:12" color="#45d325">}}




{{<matomeQuote body="オープンウェイトを推進するのは彼らの自己利益のためだよ。利己的とか道徳的な判断をするつもりはないけど、そのインセンティブがある以上、オープンウェイトやIPフリーなCPUなどをリリースする彼らの決定に、道徳的な意図があるとは論理的に推測できないよね。" userName="adastra22" createdAt="2025/09/23 07:02:20" color="">}}




{{<matomeQuote body="危険なのは、議員たちが外国のウェイトと外国のAPIの違いを勘違いして、うっかり両方を禁止しちゃうことだよ。" userName="quotemstr" createdAt="2025/09/23 00:40:02" color="#785bff">}}




{{<matomeQuote body="過去何十年も中国のハードウェア買ってアメリカのソフトウェア動かしてたのに、今や完全に逆になってるって、なんか皮肉だよね。" userName="moffkalast" createdAt="2025/09/22 22:04:41" color="">}}




{{<matomeQuote body="それはすごいね。自分の4090でQwen3 coderには感動したけど、シングルカードだとメモリが少ないのがネックだよ。君の3090sではどんなマザーボード使ってるの？他の人と同じで、esp32sとそこで動かしてるソフトウェアも気になるな。引き続き良いハッキングを！この手のものいじるの楽しいよね！" userName="state_less" createdAt="2025/09/22 21:00:43" color="#785bff">}}




{{<matomeQuote body="物理ハードウェアはesp32-s3-box[1]を使ってるよ。esphome[2]のファームウェアでHomeAssistantと連携させてる。esphomeプロファイル[3]を使ってるけど、このプロファイル[4]に切り替え検討中。AIはDockerコンテナ3つだよ。音声認識用[5]、音声合成用[6]、そしてAI本体はollama[7]。あとはHomeAssistantにサービスを向けるだけ、内蔵サポートもあるからね。<br>1. https://www.adafruit.com/product/5835<br>2. https://esphome.io/<br>3. https://gist.github.com/tedivm/2217cead94cb41edb2b50792a8bea...<br>4. https://github.com/BigBobbas/ESP32-S3-Box3-Custom-ESPHome/<br>5. https://github.com/rhasspy/wyoming-faster-whisper<br>6. https://github.com/rhasspy/wyoming-piper<br>7. https://ollama.com/" userName="tedivm" createdAt="2025/09/23 12:31:26" color="#ff33a1">}}




{{<matomeQuote body="英語だと遅いけどスペイン語だと速いって面白いね。モデルをオフラインでPCで動かしてリアルタイム音声を出したの？使ったGPUとかスペック教えて。ChatGPTによると（https://chatgpt.com/share/68d23c2c-2928-800b-bdde-040d8cb40b...）、2,500ドルくらいのGPUが必要みたいだけど、持ってる？Qwenはオンラインで試して良かったよ。Deepseek-R1 70BをCPUで動かしたけど遅かったから、GPUでオフラインモデル動かすセットアップを調べてるんだ。" userName="logicallee" createdAt="2025/09/23 06:25:34" color="#785bff">}}




{{<matomeQuote body="みんなが自宅用プライベートAIに2,000ドルも払う理由は、ポルノだよ。" userName="CJefferson" createdAt="2025/09/23 04:13:53" color="">}}




{{<matomeQuote body="Qwenを試すなら https://chat.qwen.ai/ へ！GoogleかGitHubでログインして音声アイコンを押してみて。未ログインだと音声モードは使えないよ。北京のDylan、天津のPeter、明るいCherry、元気なEthan、四川のEric、上海のJadaみたいな、色んな面白い声があるんだ。" userName="simonw" createdAt="2025/09/22 19:58:33" color="#45d325">}}




{{<matomeQuote body="これらの音声は言語を切り替えるとマジで面白いよ。ロシア語だと、Ryanは一ヶ月前にロシア語を始めた欧米人みたいに聞こえるし、Dylanはちょっと本物っぽいけど、他はみんな強いアジア訛りのロシア語なんだ。" userName="vladgur" createdAt="2025/09/22 23:31:02" color="#38d3d3">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="音声がすごく面白いね、笑わせてくれてありがとう :)" userName="flockonus" createdAt="2025/09/22 20:32:34" color="">}}




{{<matomeQuote body="Omni Flashしか見当たらないんだけど、それがQwen3-Omniのこと？" userName="indigodaddy" createdAt="2025/09/22 20:13:28" color="">}}




{{<matomeQuote body="そうだと思うよ。大きなギザギザの音声アイコンをクリックすると音声セッションが始まるはず。" userName="simonw" createdAt="2025/09/22 22:04:21" color="">}}




{{<matomeQuote body="Qwen3-Omni-FlashってQwen3-Omni-30B-A3Bと同じなの？それともOmni-Flashは別のクローズドソースモデル？" userName="karimf" createdAt="2025/09/23 03:35:52" color="">}}




{{<matomeQuote body="技術レポートのセクション5に書かれてるよ。Qwen3-Omni-30B-A3B-InstructやThinkingモデル、そしてQwen3-Omni-Flash-InstructやThinkingっていう社内開発のFlashモデルが評価されたって。Flashモデルは計算効率と性能を向上させるために設計されてて、色々な方言をサポートする機能も統合されてるらしいよ。<br>参照: https://arxiv.org/pdf/2509.17765v1" userName="ltdemey" createdAt="2025/09/25 14:23:40" color="#ff33a1">}}




{{<matomeQuote body="モデルの重みが70GBってことは、ローカルでも結構動かせそうだね。Hugging Faceにはファイルサイズ表示が追加されたから見てみて。URL: https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct/tree...<br>macOS版が出るか気になるな。今はNVIDIA GPUが必要みたいだけど。" userName="simonw" createdAt="2025/09/22 19:29:47" color="#45d325">}}




{{<matomeQuote body="BF16で70GBってことは、Q4量子化すれば24GBのGPUでもかなりいけそうだね。（他の30B-A3Bモデルと同じように。）<br>これで200B+とかじゃなくて本当に良かったよ。" userName="a_e_k" createdAt="2025/09/22 19:34:58" color="#ff5c5c">}}




{{<matomeQuote body="じゃあ、当面は32GBのVRAMがあれば十分ってこと？ネットの情報見てると128GBないとダメみたいな気分になっちゃうんだけど。" userName="numpad0" createdAt="2025/09/23 15:02:51" color="">}}




{{<matomeQuote body="16GBのApple M1で動くやつ、あるかな？" userName="zenmac" createdAt="2025/09/22 22:13:09" color="">}}




{{<matomeQuote body="いや、そうじゃないよ。最小のQwen3 A3Bの量子化モデルはだいたい12GBで、コンテキスト設定によっては14GBくらい使うんだ。<br>16GBのマシンだとSSDへのスワップが激しくなるだろうね。" userName="bigyabai" createdAt="2025/09/22 22:41:53" color="#ff33a1">}}




{{<matomeQuote body="時間がある人向けの面白いプロジェクトとして、昨日出たApple向けの新しいMojoを使って動くか試すのはどうだろう？まだ機能が十分揃ってるか分からないけど、やってみる価値はあると思うよ。" userName="growthwtf" createdAt="2025/09/22 19:32:16" color="">}}




{{<matomeQuote body="いや、自分で見つけたよ。URL: https://news.ycombinator.com/item?id=45326388" userName="wsintra2022" createdAt="2025/09/22 23:45:47" color="">}}




{{<matomeQuote body="5090でも動くかな？それか、複数のGPUをリンクさせることはできる？NVIDIAがロックしてるのかな？" userName="varispeed" createdAt="2025/09/22 21:59:10" color="">}}




{{<matomeQuote body="5090に32GBのVRAMでfp8量子化なら、サイズと品質のバランスはかなり良い感じだね。（俺はGLM-4.5-Airを3b量子化で動かしてるよ！）トランスフォーマーのアーキテクチャって、モデルの異なるレイヤーを別々の場所で動かすのに向いてるから、モデルをシャーディングすることも可能だよ。" userName="axoltl" createdAt="2025/09/22 22:33:42" color="#ff5733">}}




{{<matomeQuote body="今のところはなさそうだね。オーディオ、画像、テキスト、ビデオを一度に扱うのは複雑だから、誰かがそれをまとめるまでには時間がかかるかもしれないよ。" userName="simonw" createdAt="2025/09/22 22:03:44" color="">}}




{{<matomeQuote body="デモビデオがこれだよ。動画に音声を入力して、それを別の言語に翻訳して音声出力するやつが、今まで見た中で一番すごかった！<br>https://www.youtube.com/watch?v=_zdOrPju4_g" userName="chisleu" createdAt="2025/09/22 18:45:15" color="#45d325">}}




{{<matomeQuote body="音声入力と音声出力ができるのは超デカい。理論的には、仲介技術なしで音声で会話したり、他の人のために翻訳したりできるんだ。今だとwakeword、speech to text、text to speechが必要だけど、これは全部まとめてくれる。32bくらいのモデルが少なくとも3種類あるみたいだね。アーキテクチャ次第だけど、数年後には自宅に置けるか、高価な”AIトースター”みたいになるかもね。" userName="hadlock" createdAt="2025/09/22 20:02:20" color="#ff33a1">}}




{{<matomeQuote body="これをホームオートメーションにツールコールで組み込む機会はめちゃくちゃ大きいね。ChatGPTがこの機能を追加してから、他のモデルが追いつくのをずっと待ってたんだ。料理中みたいに手がふさがってる状況で、これがあったら最高だね。（「次のステップ読んで、手が生豚肉まみれなんだ」、「ルーに小麦粉どれくらい？」、「やばい、レモンがない。何で代用できる？」）" userName="data-ottawa" createdAt="2025/09/22 20:46:00" color="#ff5c5c">}}




{{<matomeQuote body="語学学習には、間違いなく大きなメリットになりそうだね。あと、Unslothの連中が手に入れたら、ローカルでも動かせそう。" userName="CamperBob2" createdAt="2025/09/22 20:16:06" color="">}}




{{<matomeQuote body="ここでの本当の強みはパフォーマンスとサイズだね。オープンウェイトの分野で注目されるには、モデルが効率性で革新する必要があるから、クローズドウェイトのモデルが考えないようなレバレッジを得られるかも。H100サーバーで8x 30Bモデルを動かすのと、同じサーバーで1x 240Bモデルを動かすのとで、いつ精度で前者が後者を上回るんだろうね。" userName="rapatel0" createdAt="2025/09/22 21:53:49" color="#ff5733">}}




{{<matomeQuote body="すごいね。いくつか簡単なオーディオクリップを試したら、少なくとも楽器（ピアノ、ドラムなど）を認識できたよ。音声以外のオーディオ認識に焦点を当てたLLMはあまり見かけないから、SOTAがどうなってるか深掘りしてほしいな。" userName="vunderba" createdAt="2025/09/22 19:31:39" color="#ff33a1">}}




{{<matomeQuote body="Qwenのthinker/speakerアーキテクチャは本当に魅力的だね。人間がマルチモダリティを扱う方法に似てると思うんだ。つまり、リンゴの絵、”a p p l e”というテキスト、そしてその音が、全部テキストを介さずに同じ概念にマッピングされる感じだね。" userName="edude03" createdAt="2025/09/22 19:17:12" color="#785bff">}}




{{<matomeQuote body="LLMは全部そうやって動くもんじゃないの？" userName="adastra22" createdAt="2025/09/22 19:29:17" color="">}}




{{<matomeQuote body="既存のビジョンLLMはほとんどそう動くよ。最近の主要モデルはほとんどそうだしね。マルチモーダルな音声モデルはまだ珍しいんだ。GPT-4oは最初からネイティブでいけるはずが、結局音声はカスタムモデルになったし。GPT-5も音声入出力ないみたいだよ。Gemini 2.5がビジョンと音声でマルチモーダルだけど、全媒体で同じ埋め込み空間使ってるかは分からないけど、多分そうじゃないかな。" userName="simonw" createdAt="2025/09/22 19:32:03" color="#45d325">}}




{{<matomeQuote body="俺が言いたいのは、LLMの全ての処理はステート空間で起きるってことだよ。次のトークン予測は最終ステップなんだ。" userName="adastra22" createdAt="2025/09/22 19:34:02" color="">}}




{{<matomeQuote body="動画理解モデルにはもっと変で複雑なアーキテクチャがいっぱいあるよ。例えば、動画＜－テキスト＜－LLMや動画＜－LLMへの埋め込み以外に、LLMが別の動画抽出器を制御・ガイドすることもできるんだ。詳しいことはこの論文を見てみてね。Tang, Y., Bi, J., Xu, S., Song, L., Liang, S., Wang, T., Zhang, D., An, J., Lin, J., Zhu, R., Vosoughi, A., Huang, C., Zhang, Z., Liu, P., Feng, M., Zheng, F., Zhang, J., Luo, P., Luo, J., & Xu, C. (2025). Video Understanding with Large Language Models: A Survey (No. arXiv:2312.17432). arXiv. https://doi.org/10.48550/arXiv.2312.17432" userName="uniqueuid" createdAt="2025/09/22 19:38:50" color="#ff5c5c">}}




{{<matomeQuote body="まあ、そうなんだけどさ、どれも入力（どんな媒体でも）をステート空間の概念にマッピングする方法を見つけるわけだよね。それがトランスフォーマーアーキテクチャの核だから。" userName="adastra22" createdAt="2025/09/22 19:47:45" color="">}}




{{<matomeQuote body="君が最初に返信したユーザーは、具体的に＜まずテキストに変換せずに＞って言ってたじゃん。" userName="ludwigschubert" createdAt="2025/09/22 20:05:21" color="">}}




{{<matomeQuote body="そうそう、俺もそう理解してるよ。動画＜－テキスト、音声＜－テキスト、さらにはテキスト＜－テキストでさえ、まずステート空間を通らずにはいかないんだ。それがトランスフォーマーアーキテクチャの核だからね。" userName="adastra22" createdAt="2025/09/22 20:06:35" color="">}}




{{<matomeQuote body="”ネイティブ動画サポート”って実際どういう意味なのか、何か分かる？単に一定間隔で撮られた連続するフルフレーム画像をうまく解釈するだけ（だから速い動きを見逃す）なのか、それとももっと凝ったものなの？" userName="ndr_" createdAt="2025/09/22 21:37:19" color="">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="いつもLLMに短い物語を頼むと、文章が硬くてKDP向けの子供向けの本みたいになるんだけど、今回のは一発で軽くて、ちょっと面白くて、チルな物語だったんだ。かなり驚いたよ。タイトルは「最後の電球」。停電中の夫婦のやり取りで、ユーモアもあって良かったね。" userName="j-bos" createdAt="2025/09/22 23:18:33" color="#ff5733">}}




{{<matomeQuote body="「sussy baka」の意味を知ってるか聞いてみたけど、「sussy」の部分を無視されちゃったんだ。無視しないでって言ったのに、面白いね。" userName="gercius" createdAt="2025/09/28 07:36:45" color="">}}




{{<matomeQuote body="最近、数百枚の低品質な請求書をOCRにかけて請求書番号や日付を読み取る必要があったんだ。一部のアプリではスムーズにいくのが当たり前だと思ってたけど、良い結果を出すのにどれだけ大変か知って驚いたよ。私は本当に世間知らずだったね。とにかく、OCRの進歩を見るといつもワクワクするんだ。私の（小規模な）データセットでQwenを試してみようかな。" userName="richardlblair" createdAt="2025/09/22 21:00:38" color="">}}




{{<matomeQuote body="君のコメントが理解できないんだけど。Qwenでの結果はどうだったの？それとも、作業にどれくらい時間が必要だったってこと？" userName="tomrod" createdAt="2025/09/22 21:02:43" color="">}}




{{<matomeQuote body="僕のひどいアルゴリズムが苦労してた一部の請求書にQwenを試してみたら、関連するデータを全部問題なく抽出できたよ。正直、かなり感動してるね。" userName="richardlblair" createdAt="2025/09/23 13:07:14" color="#ff5c5c">}}




{{<matomeQuote body="Qwenに関する君の主張は何？それとも単にLLM全般についての話？" userName="mrbonner" createdAt="2025/09/22 21:15:57" color="">}}




{{<matomeQuote body="OpenAIをフォールバックにして昔ながらの方法でやったんだ。僕が言いたいのは、Qwenが僕のひどいアルゴリズムより良い結果を出せるか試すのが楽しみってことさ。" userName="richardlblair" createdAt="2025/09/22 23:21:34" color="">}}




{{<matomeQuote body="AIの次のステップは、追加のモダリティ、より速いFPS（1秒あたりの推論数）、視覚と音声の入出力に対する反応時間チューニング（レイテンシーと品質のトレードオフ）、アーキテクチャに組み込まれたプランニングモジュール（前運動前頭葉をイメージしてね）、そして推論中の時間認識（常に推論し、常に学習するアーキテクチャへ）って感じかな。" userName="nmitchko" createdAt="2025/09/22 19:57:09" color="#ff5c5c">}}




{{<matomeQuote body="近いうちに行く旅行の計画でQwenを使ってみたんだ。実際にはもう計画済みなんだけど、面白そうだからね。そしたら、質問を色々聞いてきた挙句、いつまでたっても終わらない。ついにボタンをクリックして何が起こってるか見たら、途中で中国語を出し始めてたよ。もう諦めたね。" userName="wccrawford" createdAt="2025/09/23 12:49:54" color="#ff5733">}}




{{<matomeQuote body="これってAPI経由でリアルタイムの音声間通信をサポートしてる？もしそうなら、どこでホストされててドキュメントがあるの？情報が見つけられなかったんだ。OpenAIの（高価な）リアルタイム音声間通信の代わりに、これを使いたいな。" userName="syndacks" createdAt="2025/09/22 23:09:29" color="">}}




{{<matomeQuote body="リアルタイム機能に関する情報はこのURLで確認できるよ。<br>https://www.alibabacloud.com/help/en/model-studio/realtime?s..." userName="syndacks" createdAt="2025/09/23 00:07:16" color="#45d325">}}




{{<matomeQuote body="リアルタイムモデルってクローズドソースで、オープンなQwen3-Omni-30B-A3Bとは別物なの？オープンソースモデルをリアルタイムにするのってどれくらい大変なんだろう？" userName="karimf" createdAt="2025/09/23 04:50:16" color="#45d325">}}




{{<matomeQuote body="なんで別のモデルだって言うの？俺はそんな記述を見たことないんだけど。" userName="nmfisher" createdAt="2025/09/23 04:56:05" color="">}}




{{<matomeQuote body="これは俺の推測なんだけどね、色んなモデルがリストされてるここで見たんだ。<br>この古いリンクでは商用とオープンソースモデルが別セクションになってて、リアルタイムのマルチモーダルではオープンソースのタブが見つからないんだよね。<br>https://modelstudio.console.alibabacloud.com/?spm=a3c0i.2876...<br>https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?...<br>https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?..." userName="karimf" createdAt="2025/09/23 05:45:40" color="#785bff">}}




{{<matomeQuote body="MBPでQ3-Nextを動かしてみたら、GPT4.1並みのパフォーマンスが出たよ。今のローカルモデルってすごい能力だよね。" userName="__mharrison__" createdAt="2025/09/22 21:53:36" color="#38d3d3">}}




{{<matomeQuote body="アプリでテキストで質問して、音声で答えを話してもらう方法って誰か見つけた？テキスト生成か会話はできるんだけど、切り替えができないんだ。<br>Qwen.aiのブログの最初の画像で音声出力（左上）が示されてたから、できると思ったんだけどな。<br>https://qwen.ai/blog?id=1f04779964b26eacd0025e68698258faacc7..." userName="Jgoauh" createdAt="2025/09/23 08:14:53" color="#ff5c5c">}}




{{<matomeQuote body="今、写真を生き生きとさせる（写真から短い動画を作るとか）のに一番いいモデルってどれかな？" userName="indigodaddy" createdAt="2025/09/23 05:23:50" color="">}}




{{<matomeQuote body="オープンソースならWan 2.2 i2vだよ。" userName="lossolo" createdAt="2025/09/23 11:58:41" color="#ff5733">}}




{{<matomeQuote body="音声モダリティの価格がGemini 2.0 Flashとどう比較されるか楽しみだね。Gemini 2.0 Flashは古くても好きだよ。すごく安い（音声1秒あたり32トークン）、多言語対応、非推論だからすごく速いし、レート制限も大きいんだ。" userName="pzo" createdAt="2025/09/22 20:58:27" color="#ff5c5c">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
