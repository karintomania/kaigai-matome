+++
date = '2025-07-20T00:00:00'
months = '2025/07'
draft = false
title = '2025年夏、LLMを使ったコーディングはどうなる？最新レポート'
tags = ["LLM", "AI", "プログラミング", "開発ツール", "未来"]
featureimage = 'thumbnails/blue_green3.jpg'
+++

> 2025年夏、LLMを使ったコーディングはどうなる？最新レポート

引用元：[https://news.ycombinator.com/item?id=44623953](https://news.ycombinator.com/item?id=44623953)




{{<matomeQuote body="Gemini 2.5 PROとかClaude Opus 4とか、有料LLMモデルが当たり前になってんのが悲しいわ。LLMはすごいツールだけど、プログラマーが第三者に強く依存するようになるのが理解できん。昔はオープンでフリーなツールでプログラミングできたのに、数年後には有料LLMなしじゃIDEやVimなしみたいになっちゃうかもって不安なんだ。“六桁稼ぐのに月200ドルくらい安いだろ？”って言い訳は問題の本質を捉えてないよな。" userName="dakiol" createdAt="2025/07/20 14:22:43" color="#ff5c5c">}}




{{<matomeQuote body="ローカルで動かせるモデルはまだ有料のほど良くないし、運用コストもめっちゃ高いんだよ。Claude 4レベルのモデルをローカルで安く動かせるようになれば、みんなそうするだろうな。今一番近いのはKimi K2を512GB Mac Studios2台で動かすくらいで、2万ドルくらいかかるらしいぜ。" userName="simonw" createdAt="2025/07/20 16:11:15" color="#ff33a1">}}




{{<matomeQuote body="“プログラミングはオープンでフリーなツールでできる活動だった”って言うけど、JetBrainsは俺の同僚より長くビジネスやってるし、MicrosoftのVisual Basic、C++、StudioもWindows向けソフト開発をめちゃくちゃ楽にしたけど、タダじゃなかったろ。" userName="muglug" createdAt="2025/07/20 14:31:04" color="">}}




{{<matomeQuote body="決定的な違いがあるんだ。JetBrains IDEは使うけど、いつでもVimやVS Codeに切り替えられるんだよ。でも有料LLMの問題は、オープンソースのに簡単に切り替えられないことなんだ。有料のほど良くないからな。つまり、避けられない依存関係ってわけだ。これは見過ごすべきじゃないって思うぜ。" userName="dakiol" createdAt="2025/07/20 14:37:39" color="#ff33a1">}}




{{<matomeQuote body="“六桁稼ぐのに月200ドルくらい安いだろ？”って言い訳は、全然問題の本質じゃないんだよ。Black MirrorのエピソードCommon Peopleに出てきたような、他のサブスクモデルと全く同じさ。最初は値段の割に価値が高すぎるように見えるけど、結局は長期的に見て値上がりしたり、品質が落ちたりして、囚われの身になるんだから。" userName="ozgung" createdAt="2025/07/20 15:53:19" color="#ff5c5c">}}




{{<matomeQuote body="プログラミングが‘死ぬ’のが待ちきれないぜ。俺の人生から少なくとも10年を盗んでったからな。獣医がペットを助ける訓練を受けても、仕事の大部分が殺すことだと知るようなもんさ。言語論争とか、何千もの開発者との意見の相違とか、レガシーコードとか、手入れされてないライブラリとか、そんなことに10年も費やすなんて知らされてなかったわ。過去のプログラミングとはおさらばだ。新しい、違う現実を喜んで歓迎するぜ。" userName="webappguy" createdAt="2025/07/20 16:02:16" color="#38d3d3">}}




{{<matomeQuote body="有料モデルの方が、はるかに、はるかに良いぜ。" userName="azan_" createdAt="2025/07/20 14:25:45" color="">}}




{{<matomeQuote body="Framework Desktopのセットアップってどう思う？マーケティングの口上か、それとも何かメリットあるのか？Ryzen AI Max+ 395と128GBメモリで1999ドルからって、AIワークロードにはすごい価値だってさ。Llama 3.3 70B Q6とか、DeepSeek R1 671Bみたいなデカいモデルも動かせるって。でもVRAM 384GB、メモリ512GBで1万ドル～1万2千ドルって試算は怪しいな。クラウドのコストと性能は常に動くから、損益分岐点の分析はさらに難しい。もしこういうセットアップが動くなら、費用対効果は謎だぜ。 https://frame.work/be/en/blog/introducing-the-framework-desk..." userName="QRY" createdAt="2025/07/20 17:59:58" color="#ff5c5c">}}




{{<matomeQuote body="10年前はVim使いだったけど、今はPyCharmで仕事してるぜ。俺は問題を解決するためにお金をもらってるのであって、Vimの設定いじるためじゃないんだ。Vimで同じようにできるかもしれないけど、設定に何時間も費やしたくないし、そう考えるとPyCharmのライセンスなんて安いもんさ。LLMも全く同じだよ。AIに汚染されてない手作りの綺麗なコードが欲しいなら、そうすればいい。でも俺は問題を解決するためにお金を稼いでるんだ。早く解決したり、多く解決できれば、もっとお金もらえるしな。" userName="rolisz" createdAt="2025/07/20 15:36:08" color="#ff5c5c">}}




{{<matomeQuote body="プログラマってLLMには金払うのに、広告なし検索には払わないの変だよな。" userName="righthand" createdAt="2025/07/20 15:04:24" color="">}}




{{<matomeQuote body="昔はVimガチ勢だったけど、今はPyCharm使ってるよ。問題解決のためにお金もらってるんだからね。ツール最適化と問題解決は別で、VimもPyCharmも好み次第。俺はEmacsとVSCode両方使ってるよ。" userName="skydhash" createdAt="2025/07/20 16:59:09" color="#38d3d3">}}




{{<matomeQuote body="LLMの依存はコストが高すぎるよ。良いエンジニアは高レベルの抽象化を使うんだから、LLMもオープンソースとかプライベートLLMに切り替えられるツールが必要だろ。そうじゃないと会社が暴走した時に対処できない。まるでLinuxプロセスやGitコミットにお金を払うみたいだ。" userName="dakiol" createdAt="2025/07/20 14:33:51" color="#ff5c5c">}}




{{<matomeQuote body="Claude 4クラスのモデルをローカルで動かすのは、Mooreの法則が終わった今、かなり厳しいね。コンシューマーPCの性能は限界で、電力や冷却の問題もある。Facebook、MS、Googleは市場を独占するためにお金を使い、イノベーターを締め出してる。Googleのプロジェクト中止やMetaverseの失敗を見ても、AIの未来にはあまり期待できないよ。" userName="zer00eyz" createdAt="2025/07/20 18:07:42" color="#ff5c5c">}}




{{<matomeQuote body="プログラミングっていつからオープンでフリーなツールでできるようになったの？昔から有料ツールはあったし、無料の代替品もあっただろ。LLMだって同じだよ。「高給取りなんだから月200ドルくらい払えよ」ってのは的外れだ。結局、LLMを使うか、オープンソースか有料か、自分で選べるんだから。" userName="jstummbillig" createdAt="2025/07/20 15:14:39" color="">}}




{{<matomeQuote body="ソフトウェアはOllamaやvLLMがあるし、モデルもQwen3-30B-A3BとかDevstral-23Bみたいにそこそこ使えるようになってきた。でも、ハードウェアが全く追いついてないんだよ。開発用ノートPCじゃ厳しいし、Nvidia L4カードでも性能は限界がある。GitHub CopilotはOllama接続できるけどね。MoEモデルとか訓練の進化で希望はあるけど、Intel Arc Pro B60みたいなNvidiaの代替品が早く出てきてほしいな。" userName="KronisLV" createdAt="2025/07/21 10:57:53" color="#ff5c5c">}}




{{<matomeQuote body="いくつもプロバイダーがあるから、簡単に乗り換えられるよ。俺は数ヶ月ごとに変えてるしね。ロックインとか心配してるのは、抽象的に考えてる人たちだけじゃないかな。競争が激しいから、みんなどんどん良くなってるしね。" userName="Aurornis" createdAt="2025/07/21 00:46:21" color="#ff5c5c">}}




{{<matomeQuote body="彼、正直なだけだよ、失礼じゃないってば。" userName="__loam" createdAt="2025/07/20 20:04:28" color="">}}




{{<matomeQuote body="検索サービスにお金払ってるし、共同作業者にも何人かそうするよう説得したよ。" userName="haiku2077" createdAt="2025/07/20 15:33:28" color="">}}




{{<matomeQuote body="問題ないよ。LLMの進化が停滞するなら、高くて意地悪なサービスにこだわる理由はないし、オープンモデルも多いからね。もし停滞しないなら、どうせ僕らは時代遅れになるから、別のことに移るしかない。だから心配するだけ無駄さ。たぶん進化は停滞するし、大手企業の株は過大評価されてると思うよ。" userName="rapind" createdAt="2025/07/20 18:27:14" color="#ff5733">}}




{{<matomeQuote body="’失せろ’とか’お前はいらない’なんてのは正直さじゃなくて失礼なだけ。正直なら’プログラミングは好きだし、もし消えたら悲しい。君はもっと好きな仕事を見つけるべきだ’って言うべきだろ。あと、元コメントの’メンテされてないライブラリやツール’って指摘は超的を得てる。商業コードはインセンティブのズレでダメなのばかり。Open Sourceも完璧じゃないけどね。" userName="oblio" createdAt="2025/07/20 20:37:22" color="#45d325">}}




{{<matomeQuote body="ツール最適化と問題解決を一緒にするのは違うって意見だけど、ツール最適化ってのは長期的に楽になるためじゃん？俺は10年以上Emacs使って、その後10年以上PyCharm使ってるけど、PyCharmは最初からEmacsで10年かけて設定した機能がほぼ全部揃ってたんだよね。時間かけてEmacsを最適化する”必要”はなかったけど、やった方がインテリセンスとかコードジャンプとかできて便利だった。" userName="LeafItAlone" createdAt="2025/07/20 18:01:39" color="#38d3d3">}}




{{<matomeQuote body="多分ほとんどのデベロッパーは無料検索使ってると思うよ。だって、まだ誰にも’Kagi使え’って言われてないし。" userName="righthand" createdAt="2025/07/20 16:16:37" color="">}}




{{<matomeQuote body="LLMの進化が停滞しないなら、僕らプログラマは時代遅れになるって話に、もっとニュアンスがあるかもね。急な停滞じゃなくて、収穫逓減になる可能性もある。それだと大手LLMプレイヤーが有利になるだろうな。大きなブレイクスルーの合間は、そのシナリオが一番ありそう。" userName="worldsayshi" createdAt="2025/07/21 00:02:53" color="#ff5c5c">}}




{{<matomeQuote body="プログラマがプログラミングするためにサードパーティに強く依存するのを気にしないってのが理解できないし、自分たちのコードベースを大手テック企業に無料で公開するのも気にしないのが不思議だね。" userName="Fervicus" createdAt="2025/07/20 16:55:04" color="">}}




{{<matomeQuote body="ロックインを避けるためにローカルでLLMを動かす必要はないよ。いくつかのクラウドプロバイダーがDeepSeek R1とかKimi K2を100万出力トークンあたり2〜3ドルで提供してるし。" userName="smallerize" createdAt="2025/07/20 18:24:31" color="">}}




{{<matomeQuote body="まあ、収穫逓減は結局、停滞と同じ効果を生むさ。もし君が（はるかに安価な中国の）競合他社と一緒に進化の丸太に乗ってるなら、君の優位性はあっという間にちっぽけになるだろうね。" userName="rapind" createdAt="2025/07/21 03:39:12" color="#45d325">}}




{{<matomeQuote body="フェイシャルティッシュが欲しい時、箱がPuffsって書いてあってもKleenexって言うよね。だって、「Puffs取って」って言う人いる？LLMを使ったコーディングツールも、特定のツール名が代名詞みたいになるかもって話かな。" userName="haiku2077" createdAt="2025/07/20 16:24:26" color="">}}




{{<matomeQuote body="LLMツールって、他の開発ツールと全然変わんないよね。無料のオープンなツールはたくさんあるけど、有料版よりちょっと遅れてる。JetBrainsとかmacOSとか、Google Adsにもみんな金払うじゃん。強力なオープンモデルもあるけど、最先端じゃないし、ローカルで動かすにも結局金がかかる。競争が進んで、各社が最高のモデルを作ろうと頑張ってて、価格も下がって選択肢も増えるのはマジ最高。資本主義が機能してるって感じだね。" userName="cafp12" createdAt="2025/07/20 15:21:12" color="#785bff">}}




{{<matomeQuote body="ちょっと話がそれるけど、筆者の「PhD-level knowledge」って表現には反対だな。antirezにはすごく尊敬してるんだけど。この言い方は誤解を招くし、博士課程の本質を間違って捉えてる。AIラボのマーケティングとか誇大広告に影響されてる感じ。明確な「PhD-level knowledge」なんて主張は意味ないよ。PhDの主な目的は、既存の膨大な知識を覚えることじゃなくて、研究の仕方を学ぶことだからね。" userName="quantumHazer" createdAt="2025/07/20 13:42:09" color="">}}




{{<matomeQuote body="その意見に同意。あれはLLMが人間ほど得意じゃない他の部分を除いた、エキスパートレベルの知識って読むべきだよ。LLMの知識表現方法は独特で、ちょっと異質だから、やっぱり単純化しすぎてるんだね。例えば、LLMはトップレベルの人間コーダーほど上手くコードは書けないけど、反復なしで最初から最後まで非自明なプログラムを書ききれるんだよね。" userName="antirez" createdAt="2025/07/20 13:44:21" color="#ff5c5c">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="antirezさん、Geminiがプロダクションリリース前にバグを見つけてるって話がすごく気になったよ。もう少し詳しく教えてくれると嬉しいな。だって、普通AIはバグを作る側で、人間がそれを見つけるって考えるじゃん。でも、Geminiがテストを書くだけじゃなく、QAみたいにバグを見つけてるなら、それってすごい興味深いよね。" userName="spyckie2" createdAt="2025/07/20 14:17:24" color="#785bff">}}




{{<matomeQuote body="うちのチーム、LLMを自動コードレビューにガンガン使い始めてるよ。PRを見てコメントしてくれるんだ。APIガイドラインとか、”あなたはエキスパートのソフトウェアエンジニアでQAプロフェッショナルです。このPRをレビューしてバグや技術的リスクを指摘し、改善提案を簡潔にしてください”みたいなプロンプトを与えると、めちゃくちゃたくさんの問題を見つけてくれる。あと、ビルド失敗時に人間が見る前に、LLMが根本原因のレポートを作成するのにも使ってるから時間節約になるね。アラームのトリアージも試作中だよ。" userName="jacobr1" createdAt="2025/07/20 18:13:57" color="#ff33a1">}}




{{<matomeQuote body="LLMを活用したツールが有料製品としてもっと出てないのが意外だね。僕はCursorsのbugbotみたいなシステムをめちゃくちゃ使ってるよ。ノイズはあるけど信号は多くて、いつも正しいわけじゃないけど、自分が絶対見逃してたであろうバグをたくさん見つけてくれるんだ。" userName="infecto" createdAt="2025/07/21 00:17:07" color="#ff5c5c">}}




{{<matomeQuote body="いくつかあるよ、Greptile、Ellipsis、GH Copilotとかね。多くの人が”自動レビューと修正”を試そうとしてる気がする。生成されたコメントを別エージェントに渡して適用させるのは魅力的だもんね。でも、それって新たな問題を引き起こすし、すぐにただのコードアシスタントサービスになっちゃうだけだよ。" userName="senko" createdAt="2025/07/21 06:58:14" color="#785bff">}}




{{<matomeQuote body="チーム固有の知識とか基準に基づいた、具体的なプロンプトを使えば本当にうまくいくんだよ。「このコードでバグを探して」みたいな、ほとんどのコードレビューアがやってるような一般的なプロンプトじゃダメなんだ。" userName="dearilos" createdAt="2025/07/22 23:30:41" color="#785bff">}}




{{<matomeQuote body="LLMにPRレビューさせる時って、精度どうしてる？「このPR見て」ってだけだと、大したレビューしてくれないよね。" userName="dearilos" createdAt="2025/07/22 23:27:02" color="">}}




{{<matomeQuote body="確かにそれじゃダメだよね。うちはガイドラインとかPRのベストプラクティスをコンテキストに入れてるよ。LLMでそのデータを整形して、ルールを絞るのも効果あったね。" userName="jacobr1" createdAt="2025/07/23 15:59:01" color="#ff5c5c">}}




{{<matomeQuote body="僕も数ヶ月前からコードレビューのルールディレクトリ作ってるんだ！何がうまくいって、何がダメか、話して情報交換しない？メールはilya (at) wispbit.comだよ。" userName="dearilos" createdAt="2025/07/24 00:22:27" color="">}}




{{<matomeQuote body="PhDレベルの知識って、正しい問いを立てることだよね。LLMは言われないと自分で仮説を探らないから、「ultra-think」みたいな指示で深掘りさせないとダメなんだ。前、Claude Codeがコミット後にリフレッシュされてないORMオブジェクトのバグを勝手にコメントアウトしちゃってさ。" userName="ghm2180" createdAt="2025/07/20 15:39:53" color="#38d3d3">}}




{{<matomeQuote body="「ultrathink」は一語だよ（下に証拠あり）。Claude CodeのOpusで使ってるけど、同じ経験したな。複雑なテストさせようとしたら、コメントにテストプラン書かれた空の関数返ってきたし（笑）。Gemini 2.5 Pro試したいけど、Claude Codeみたいな体験ができるツール知らないんだよね。Cursorとかどうなのかな？" userName="vl" createdAt="2025/07/21 02:37:14" color="#ff5733">}}




{{<matomeQuote body="Googleのgemini-cliってClaude Codeにかなり近い使い心地で、無料枠も結構あるよ。個人的にはClaude Codeの方が上だけど、自動修正を鵜呑みにするとすぐに話がズレることも。でも、大きなコンテキストウィンドウあるからコードレビューやプランニングには便利だね。" userName="andrew_k" createdAt="2025/07/21 03:27:21" color="#ff5c5c">}}




{{<matomeQuote body="https://github.com/sst/opencode" userName="elyase" createdAt="2025/07/21 07:55:29" color="">}}




{{<matomeQuote body="PhDって知識だけじゃなく、その知識に簡単にアクセスできることがすごく大事って分かってたら、LLMの価値ってめっちゃ高いよ。前の仕事でPhDレベルの人が答えるような質問がよくあったけど、LLMは結構うまくやってくれるんだ。" userName="chis" createdAt="2025/07/20 15:50:34" color="">}}




{{<matomeQuote body="LLMの回答、実験的に検証したの？それにこれって「PhDレベルの知識」っていうより、単に仕事で出てくる電気工学の質問じゃない？" userName="quantumHazer" createdAt="2025/07/20 15:55:42" color="">}}




{{<matomeQuote body="全くその通り。「PhDレベルの知識」ってのは、博士論文の序論だよね。PhDの目的は、既に知られてる知識を超えて、LLMには知りえない新しい知識を生み出すことなんだよ。" userName="pcrh" createdAt="2025/07/21 10:30:46" color="">}}




{{<matomeQuote body="2015年のデータサイエンスブーム以外で、博士号だけで「博士レベルの仕事」が得られる状況なんて一度もなかったんだよな。お前が博士号で学ぶことをどんなにえらそうに言おうと、博士号採用する奴は誰も同意しないよ。むしろ、ほとんどのPhD教授は博士号取得者を、お前が研究した超特定のテーマの器としか見てないんだから。自分の専門外のトップラボでポスドク取ろうとしてみろよ。俺は試して、そして諦めた！" userName="ramraj07" createdAt="2025/07/21 08:12:16" color="">}}




{{<matomeQuote body="＞博士号の主目的は、既存の膨大な知識を得ることではなく、研究のやり方を学ぶことである。一度博士号を取ったら、誰もその分野には関心がない、ってこと？大事なのは研究のやり方を学んだってことだけ、ってか？" userName="kgwgk" createdAt="2025/07/20 15:23:20" color="">}}




{{<matomeQuote body="LLMを使ったコーディングとか、Vibe Codingとかの議論では、ドメインとプログラミング言語の選択をちゃんと考慮すべきだ。この二つの変数の方が、どんなVibe Codingのやり方よりも10倍（いや100倍かも）説明力があるんだから。LLMでのコーディングが好きか嫌いかで混乱してる奴は、どんな問題に取り組んでるか聞けばいい。そうすれば相手の視点がよく分かる。そうしないと、「使い方が悪い」「最悪だ」みたいなメッセージだらけで、ノイズにしかならないよ。" userName="airstrike" createdAt="2025/07/20 14:45:30" color="#ff5c5c">}}




{{<matomeQuote body="プロンプトとか、出力チェックにかかった労力とかも共有すべきだよな。記事でも「LLMと協調するために必要なやり取りを受け入れられ、問題を明確に記述できるなら…LLMに大量の情報を提供する必要があるんだ。論文やターゲットコードベースの大部分…そして、何をすべきかに関する自分の理解の脳内ダンプ。その脳内ダンプには特に以下を含まねばならない：」って、どんだけ手間かかってるか示唆してるし。これだけ努力して、やっと生成されたコードが使えるレベルになったらさ、結局自分で書けばよくない？って思っちゃうよな。タイピングにかかる時間なんて大したことなくて、問題記述に費やす認知労力こそがプログラミングの本質なんだから。" userName="cratermoon" createdAt="2025/07/20 15:05:18" color="#38d3d3">}}




{{<matomeQuote body="＞結局、自分で書けばよくない？<br>いやいや、それでもLLMの方が断然、断然、断然速くて簡単だよ。解決策を見つけるのが難しいってのは全くその通り。でもタイピングに費やす時間は決して些細でも、認知的に単純でもない、特に複雑なタスクではね。プロンプト一つで数秒で5〜10倍ものコードを簡単に生成できる上に、さらに以下のメリットがあるんだ：<br>a) ほとんど全ての中間データ構造、クラス、アルゴリズム、データベースクエリを考えてくれる。<br>b) 全てのボイラープレートやドキュメントを処理してくれる。<br>c) 自分が考慮しなかったエッジケースまで考慮してくれて、計り知れない量の将来のデバッグ時間を節約できる。<br>d) 単に頼めばテストも含まれる。<br>実際、最近は解決策が分かったら、手動だと頭の中の設計をコードに十分速く落とし込めないことにイライラするんだ。AIが大量のコードを吐き出してくれて、すぐに実行（またはテスト）して期待通りの結果を見られるのはすごく満足感がある。もちろん、コミットする前に差分はしっかりレビューするけど、それは自分のコードでもそうするしね。" userName="keeda" createdAt="2025/07/21 00:10:55" color="#38d3d3">}}




{{<matomeQuote body="＞頻繁に自分が考慮しなかったエッジケースまで考慮してくれて、計り知れない量の将来のデバッグ時間を節約できる；<br>そして、お前が今まで考えもしなかった新しいバグも生み出して、同じか、それ以上のデバッグ時間を将来に生み出すんだよ :D" userName="gf000" createdAt="2025/07/21 01:31:16" color="">}}




{{<matomeQuote body="それはコードレビューでしょ :-) でも前のコメントに同意すると、俺は一年半以上、生成されたコードの中に微妙なエッジケースやバグを見つけたことがないよ。間違いや失敗モードは確かにあるけど、それはすごく分かりやすいから、そういうコードは捨ててやり直すだけ。とはいえ、俺はAIと仕事する上で、自分の状況に非常に効果的な特定のやり方を採用してるけどね（コメント履歴にも書いたけど、TFAの助言と似てるよ）。" userName="keeda" createdAt="2025/07/21 21:36:41" color="#38d3d3">}}




{{<matomeQuote body="Claude Codeみたいなツールを使うと、こういう微妙なバグが実際ほとんど出てこないことに驚いてるんだ。たいてい出てくるバグは、プロンプトの誤解によるもので、コードの作りが悪いわけじゃないから、すごく分かりやすいんだよ。AI生成コードのレビューは、実際には思ってたよりずっと簡単だったから、これは驚きだったな。多分、俺がLLMを使うのは簡単な説明ができるコードだけで、だから複雑じゃないからかもしれないね。もっと複雑なコードなら自分で書くし。" userName="sothatsit" createdAt="2025/07/21 02:06:24" color="#ff5c5c">}}




{{<matomeQuote body="この話にはもううんざりだ。「LLMが膨大な量のバグを出す」ってのは、GPT-4の2023年あたりの見方だよ。みんな真剣にAIツールを学ぶべきで、こんなくだらないことを繰り返すのはやめろ。最も寛大に言うとすれば、1時間あたり20～50倍のコードを生成するから、お前が慣れてる時間枠でバグの『生の数』が増えるのはあるかもしれない。でもAIコーダーが人間よりバグが出やすいっていう考えは全くのナンセンスだ。それが当てはまらないのは、ごくニッチなシステムか、人間が事前に徹底的にコードを概念化して計画している場合だけだよ。それでもAIが次のステップとしては優れてるだろうね。" userName="jatora" createdAt="2025/07/21 07:06:46" color="#45d325">}}




{{<matomeQuote body="LLMは英語から高水準言語への翻訳みたいなニッチな作業は得意だけど、並行処理みたいな複雑な推論は全然ダメだよ。統計的なデタラメを返してるだけ。LLMは役立つツールだけど、過大評価も過小評価もあるね。" userName="gf000" createdAt="2025/07/21 07:55:03" color="#45d325">}}




{{<matomeQuote body="自分でコーディングすると、知ってるバグしか直せないけど、AIは考えつかなかったエッジケースを教えてくれることがあるよ。" userName="IshKebab" createdAt="2025/07/21 06:23:43" color="#38d3d3">}}




{{<matomeQuote body="＞1つのプロンプトで数秒で5～10倍のコードを生成できる<br>それって良くない結果だね。コードの行が増えるほどバグの可能性も負債も増えるからね。プログラミングで一番どうでもいいスキルはタイピングの速さだよ。" userName="cratermoon" createdAt="2025/07/22 15:25:34" color="#45d325">}}




{{<matomeQuote body="＞彼らもプロンプトを共有すべきだよ<br>最近のShowHN投稿にOneDriveの写真マップビューがあるんだけど、これにはLLMプロンプトの記録が全部載ってるよ。<br>https://news.ycombinator.com/item?id=44584335" userName="datastoat" createdAt="2025/07/20 17:18:41" color="#ff5c5c">}}




{{<matomeQuote body="「生成コードを受け入れるまでに自分で書く方がマシじゃない？」ってよく思うんだけどね。<br>でも、アホなロボットがプロジェクト設計に役立つって分かったんだ。設計書を作らせて、それについて議論して修正して、新しいアイデアを探って、最後にコードを書かせるんだ。まるで剃ろうとしてるヤクと話せるみたいで、最高だよ！" userName="UncleEntity" createdAt="2025/07/20 16:41:22" color="#ff5733">}}




{{<matomeQuote body="たぶん、LLMには一回大量の情報を渡せば（あるいは集めさえすれば）、それをいろんなタスクで使えるから、後は楽できるって言いたいんじゃないかな。" userName="tines" createdAt="2025/07/20 16:35:35" color="#45d325">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="コードの再利用を考えるなら、そんなに大量のコードを書く人なんていないよ。ほとんどの時間は、情報収集（特にステークホルダーとのやり取り）や、書いたコードで何も壊れてないかの確認に使われるんだ。" userName="skydhash" createdAt="2025/07/20 17:08:52" color="#ff33a1">}}




{{<matomeQuote body="AIでクリエイティブなビジュアルマーケティングキャンペーンを立ち上げたばかりなんだけど、プロンプトのスキルでアウトプットにめちゃくちゃ差が出るってマジだよ。プロンプト次第で印刷できるレベルの画像も、ゴミみたいな画像もできちゃうからね。" userName="spaceman_2020" createdAt="2025/07/21 13:17:38" color="#45d325">}}




{{<matomeQuote body="ClaudeのGitHub actionを結構使ってるけど、ヒットとミスがあるから、LLM単独よりは「強化されたコーディング」が良いね。小さい変更やリファクタリングはテストがあれば安全で得意だよ。でも中規模以上は動いてるように見えても動かないことが多い。もっと大きいのは全然ダメ。PRレビューは小さいタスクには良いよ。まだ単独で何でもできるレベルには程遠いけど、僕の生産性はかなり上がった。" userName="skippyboxedhero" createdAt="2025/07/20 14:52:07" color="#785bff">}}




{{<matomeQuote body="問題は、コード変更をユーザーに見せるためのインフラだよ。コードをプルしてデータを設定して、意図通りに動くか確認する手間があるし、生成コードが期待通りに動かないと、差分をチェックするだけで時間を無駄にしちゃうんだ。" userName="milofeynman" createdAt="2025/07/21 04:02:15" color="#45d325">}}




{{<matomeQuote body="LLMにコードを書かせる前に、まずやりたいことを説明させると、その後に生成されるコードの質がめっちゃ上がるって気づいたんだ。詳細な説明を求めて、何度かフィードバックしたら、あとは実装させればOKだよ。" userName="1024core" createdAt="2025/07/20 16:58:30" color="#ff5c5c">}}




{{<matomeQuote body="同感！良い戦略だよね。LLMの強みを活かしつつ、暴走させないのがミソだよ。" userName="hyperadvanced" createdAt="2025/07/21 05:54:43" color="#38d3d3">}}




{{<matomeQuote body="元の記事の人とは違って、僕の経験ではGemini 2.5 PROとOpus 4はアーキテクチャとか抽象的なレベルで良い感じだったな。Sonnetはコーディング向きだね。GeminiとOpusは回りくどいことあったけど、Sonnetはサッと本題に入るんだ。" userName="Keyframe" createdAt="2025/07/20 13:55:14" color="#ff5c5c">}}




{{<matomeQuote body="僕もそう思う！大きなデザインアイデアはGemini 2.5 Pro、コーディングはClaude Codeが最高だね。Gemini CLIはClaude Codeに全然敵わないよ。Claude Codeはデバッグも優秀なんだ。あとはDeepSeek R1もアイデア出しにはいいけど、ちょっと遅いかな。" userName="khaledh" createdAt="2025/07/20 14:28:31" color="#785bff">}}




{{<matomeQuote body="それ、全然あり得るね。Sonnet/Opus 4は最高の出力はすごいけど、安定性や一貫性ではSonnet 3.5v2（3.6）や3.7より劣ることもあると思うな。モデルって複雑だし、使い方で性能も変わるからね。" userName="antirez" createdAt="2025/07/20 16:02:29" color="#ff33a1">}}




{{<matomeQuote body="社内の統計でも、OpusとGemini 2.5 proが実世界シナリオでSonnet 4より性能が悪いって確認されてるよ。<br>https://x.com/pashmerepat/status/1946392456456732758/photo/1" userName="jpdus" createdAt="2025/07/20 21:20:48" color="#ff5733">}}




{{<matomeQuote body="Gemini 2.5 PROはコスパ最強で推論も良いね！Cursorだと1リクエストだし、コードスタイルもシンプルで気に入ってるよ。無人島に一つだけ持っていくならコレだね。大仕事にはOpus 4を使うけどさ。" userName="headcanon" createdAt="2025/07/21 15:54:49" color="#785bff">}}




{{<matomeQuote body="数ヶ月エージェントコーディングしてみて、この投稿に完全に同意するよ。今一番使いやすいのはフロンティアLLMだけど、オープンモデルもすぐ追いつくはず。LLMから学べるし、アプローチも相談できるんだ。Github Copilotも簡単な機能にはすごく良いよ。みんな、この新しい旅を楽しんで、学んだことを共有しよう！" userName="cadamsdotcom" createdAt="2025/07/20 23:28:39" color="#ff33a1">}}




{{<matomeQuote body="AI/LLMがいかに非効率なコードを書くか、良い例があるよ。見てみて。<br>https://nullonerror.org/2025/07/12/ai-will-replace-programme..." userName="delduca" createdAt="2025/07/20 18:31:38" color="#785bff">}}




{{<matomeQuote body="同じく、AIはコードゴルフが苦手なんだ。秘密を全部知ってるって思うだろうけど、そうじゃないんだよね。冗長なコードじゃないとダメなんだ。" userName="dawnerd" createdAt="2025/07/20 18:49:06" color="#45d325">}}




{{<matomeQuote body="LLMをコードゴルフの例でファインチューニングしたら、どんなのができるかな？面白そう！" userName="verbify" createdAt="2025/07/20 19:08:40" color="">}}




{{<matomeQuote body="Claude codeはすごく進化してるよ。うちのPythonコードベースで複雑なアダプターパターンがあるんだけど、適切なプロンプトを与えれば、Claudeは新しいアダプターをほとんど完璧に実装できるんだ。すごいよね。" userName="lysecret" createdAt="2025/07/20 16:59:34" color="#785bff">}}




{{<matomeQuote body="要するに、彼の会社は資金調達やValkeyとの競争のためにAI製品を出すってことだろ。AIがなくても十分生産的だった人たちが、今さらAIのちっちゃな証拠を探し回るのはすごく悲しいと思うよ。" userName="bgwalter" createdAt="2025/07/20 13:59:49" color="#45d325">}}




{{<matomeQuote body="LLMに関する投稿があるたびに、うちのポジティブなLLM体験は気のせいだとか、まだLLMのダメさに気づいてないだけだとか言ってくる人が湧いてくるのは、もっと悲しいよ。" userName="brokencode" createdAt="2025/07/20 14:18:17" color="">}}




{{<matomeQuote body="俺たちは「想像だ」って言うだけじゃないんだ。証拠も提供できるんだぜ。ほら、これ見てくれよ→ https://metr.org/blog/2025-07-10-early-2025-ai-experienced-o..." userName="cratermoon" createdAt="2025/07/20 14:56:34" color="#ff5733">}}




{{<matomeQuote body="もしLLMが本当に役立つなら、そこらじゅうで叫び回る必要なんてないだろ。むしろ、極秘にするはずだ。" userName="on_the_train" createdAt="2025/07/20 14:24:39" color="">}}




{{<matomeQuote body="CursorなんてLLMの古い使い方だよ。それに、その研究では、調査前にCursorを使ったことのある人が半分以下だったんだぜ。" userName="nojito" createdAt="2025/07/20 15:08:38" color="">}}




{{<matomeQuote body="AIツールの変化がめちゃくちゃ速いから、どんなツールを使って研究が出ても、みんな「あー、あれは古いツール使ってたね」って言えるようになるだろうね。" userName="roywiggins" createdAt="2025/07/20 15:15:45" color="#ff33a1">}}




{{<matomeQuote body="そうでもないよ。LLMとのチャットは3年間も最先端だったんだ。Claude codeやGemini CLIが出てきたこの8～10ヶ月で、LLMとの関わり方に次の大きな変化があっただけさ。" userName="nojito" createdAt="2025/07/20 15:23:55" color="#ff5733">}}




{{<matomeQuote body="俺の投稿読んだ？読んでないといいんだけど。この投稿はRedisとは全く関係ないし、会社に戻る前に書いた投稿の続きなんだ。" userName="antirez" createdAt="2025/07/20 14:32:02" color="">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
