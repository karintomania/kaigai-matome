+++
date = '2025-08-04T00:00:00'
months = '2025/08'
draft = false
title = 'Qwen-Image：驚異の文字描写！新たな画像生成AIの幕開けか'
tags = ["AI", "画像生成", "深層学習", "自然言語処理", "オープンソース"]
featureimage = 'thumbnails/color4.jpg'
+++

> Qwen-Image：驚異の文字描写！新たな画像生成AIの幕開けか

引用元：[https://news.ycombinator.com/item?id=44787631](https://news.ycombinator.com/item?id=44787631)




{{<matomeQuote body="これ、なんでそんなに話題になってないんだろう？オープンソースモデルなのに、gpt-image-1をあらゆる面で超えてて、Flux Kontextの編集能力も上回ってるみたいだし。これはかなりすごいことだと思うよ。" userName="rushingcreek" createdAt="2025/08/04 18:51:22" color="#45d325">}}




{{<matomeQuote body="40GBのVRAMが必要ってのが、みんなの熱意を冷ましちゃってる原因じゃないかな。LLMではマルチカードの技術が成熟してるのに、画像モデルだとGGUF使ってもそうじゃないのが不思議。画像モデルがもっと大きくなれば、そういう機能も実装されるかもね。" userName="tetraodonpuffer" createdAt="2025/08/04 20:03:46" color="">}}




{{<matomeQuote body="gpt-image-1以外、AI画像生成の話題が減ってるよね。AIエージェントとかvibe codingに注目が集まってるのと、AI画像生成への社会的なスティグマが増えてるからかな。Flux Kontextは画像編集のすごいやつだけど、まだあんまり知られてない。Qwen-Imageはライセンスが緩いから、編集モデルが出たらもっと進化しそうだね。" userName="minimaxir" createdAt="2025/08/04 20:57:05" color="#45d325">}}




{{<matomeQuote body="40GBなんて小さいよ。MacBook ProとかM3 Ultra Mac Studioで動くし。家で使うならNvidiaはいらないね、Apple Siliconの方がコスパいいよ。M3 Ultraは4090と同じくらいのメモリ帯域幅で、5090には敵わないけど遅くはない。20Bモデルなら20GB VRAMで十分だし、FP8でも品質はほとんど変わらないから、Mac Studioならどれでも、低スペックのMacBook Proでもいけるよ。" userName="reissbaker" createdAt="2025/08/04 21:17:29" color="#ff5733">}}




{{<matomeQuote body="1時間くらい触ってみたけど、やっぱりgpt-image-1（Imagen 3/4も）には及ばないな。複雑なプロンプトにどれだけ忠実かって点では、Qwen-Imageは50%くらい、gpt-image-1は75%くらいだった。迷路とかシュレディンガー方程式は無理だったね。<br>https://genai-showdown.specr.net" userName="vunderba" createdAt="2025/08/04 23:19:46" color="#38d3d3">}}




{{<matomeQuote body="AI画像生成に社会的なスティグマなんてないよ。あれはただの”いじめキャンペーン”みたいなもん。シンセサイザーとかカメラができた時と同じ。怒ってる人たち以外は誰も本気にしてないし。実際、AI画像生成はもうどこにでもあるし、AI画像編集は主要なスマホに全部入ってるもんね。" userName="ants_everywhere" createdAt="2025/08/05 00:11:50" color="">}}




{{<matomeQuote body="MacBook Proで画像一枚作るのに20分待てるなら、動かせるよ。" userName="42lux" createdAt="2025/08/05 09:17:15" color="">}}




{{<matomeQuote body="gpt-image-1よりできること多いんじゃない？スタイル変換、オブジェクトの追加削除、テキスト編集、ポーズ操作に加えて、物体検出、セマンティックセグメンテーション、深度とかエッジの推定、超解像度、新しい視点合成までサポートしてるんだよ。マジで盛りだくさん！最初はgpt-image-1の方がシャープネスとかクリアさがあったけど、OpenAIが後処理で何かしてるんじゃないかって疑ってるんだよね。このモデルもほぼ同じくらい良いってのはすごいよ。OpenAIがしばらくリードすると思ってたのに。Flux Kreaも発表から4日しか経ってないんだし、このモデルが本当にgpt-image-1と同じ品質ならマジでやばいね。" userName="jug" createdAt="2025/08/04 20:36:50" color="#ff5733">}}




{{<matomeQuote body="40GBのVRAMって？24GBのGPUを2枚ってこと？それって最新のQwen coder（SOTAに近いし、プロプライエタリモデルにもベンチマークで勝ってるやつ）を動かすマシンと比べたら、かなり妥当な方だよ。" userName="TacticalCoder" createdAt="2025/08/04 20:07:56" color="#ff5733">}}




{{<matomeQuote body="それにしても、fluxモデルは非商用利用のみだって話だよ。" userName="jacooper" createdAt="2025/08/04 20:56:12" color="">}}




{{<matomeQuote body="これ、クオリティが今よりちょっと上がるだけでかなりお得になるって。<br>人間だと出来上がるまで何日もかかるのが普通だからね。" userName="roenxi" createdAt="2025/08/05 10:30:17" color="">}}




{{<matomeQuote body="役立たずなAIアート（ほとんどがそうだけどさ）は、カメラとかシンセサイザーとは違うよ。<br>50～60代のママたちがFacebookでMinionのミームをシェアしてた時の感じに近いね。マジで無理。<br>良くなっても受け入れられないし、むしろ本物の作品が疑われるようになって、誰もチャンスをくれなくなるだけだよ。" userName="debugnik" createdAt="2025/08/05 06:25:25" color="#ff5733">}}




{{<matomeQuote body="fluxモデルのライセンスは月1,000ドルだよ。本格的な商用利用にとっては大した障害じゃないね。" userName="doctorpangloss" createdAt="2025/08/04 21:09:23" color="">}}




{{<matomeQuote body="OpenAIのタコは‘本物’って呼んでいいのかな？" userName="supermatt" createdAt="2025/08/05 03:19:44" color="">}}




{{<matomeQuote body="どんな経験を期待してるんだ？<br>AIの良い作品に匹敵するようなものを20分で描けるアーティストの配信なんて見たことないよ。<br>彼らの優位性は、今んとこ作品のクオリティの上限が高いってだけだ。<br>分単位で見れば、AIの方がずっと優れてる。<br>ただ、今のモデルだとAIにGPUで時間をかけさせても、一貫して自分の作品を改善できないから無意味なだけさ。" userName="roenxi" createdAt="2025/08/05 12:58:48" color="#ff5733">}}




{{<matomeQuote body="LLMみたいに、画像モデルを2つのGPUに分割することはできないんだよ。" userName="AuryGlenz" createdAt="2025/08/05 06:50:34" color="#ff33a1">}}




{{<matomeQuote body="10万画像あたり、だね。それに画像一枚あたり0.01ドルが追加でかかる。<br>H100が1時間あたり1.5ドルで、5秒で画像一枚できると考えると、ベアメタルコストが画像一枚あたり約0.002ドル＋ライセンスコスト0.01ドルって話だね。" userName="liuliu" createdAt="2025/08/04 21:35:36" color="#45d325">}}




{{<matomeQuote body="絶対にあるよ。誰かがプレゼン資料や記事でAI画像をポイントの図解に使った時なんて、みんな呆れてるもん。<br>個人的には、低品質なAI画像よりストックフォトか、何もない方がマシだと思うね。" userName="torginus" createdAt="2025/08/05 01:16:19" color="">}}




{{<matomeQuote body="AIへの偏見というより、お前はアート全般が嫌いなんじゃないか？と主張してるね。<br>もし職人技のアートとAI生成作品に差がなく、品質が収束するなら両方捨てられるって言うなら、そもそもアートの価値って何だったんだ？って話だよな。" userName="roenxi" createdAt="2025/08/05 10:35:53" color="#785bff">}}




{{<matomeQuote body="「みんな」って誰のこと？お前はどうしてそれを知ってるんだ？それって個人的な意見を一般的なことみたいに言ってるだけじゃないのか？" userName="orbital-decay" createdAt="2025/08/05 03:04:37" color="">}}




{{<matomeQuote body="まだ数時間しか経ってないのに、デモがエラーだらけじゃん。興奮する前に、もっとみんながちゃんと触れる時間が必要だろ。<br>ローカルで動かすなら量子化されたGGUFとかComfyUIのワークフローが重要になるけど、このモデルは他のよりかなりデカいぞ。<br>面白かったのは、AlibabaとAlibabaを比較することになった点だね。Wan 2.2での画像生成は超人気だから、みんなQwen-ImageがFluxよりどれだけ進化したかじゃなくて、Wan 2.2からどれだけ飛躍したかを知りたいんだよ。<br>新しい画像モデルの本当の良し悪しを判断するのに最適なのは、リリースから約1週間後みたいだね。その頃には、みんながモデルをいじくり倒して、第三者による長所／短所が出てくるだろうからな。でも、これは期待できそうだ！" userName="zamadatix" createdAt="2025/08/04 19:54:36" color="#785bff">}}




{{<matomeQuote body="奴らが画像やエディタのウェイトを公開してないのに、出してるグラフだけでFlux Kontextより優れてるって結論をどうやって出したんだ？<br>もちろん、そんなことしないよな？<br>グラフのスケールちゃんと見たか？" userName="SV_BubbleTime" createdAt="2025/08/05 06:24:51" color="">}}




{{<matomeQuote body="これ、革命的だと思うぜ。俺のユースケースはVDMXのワークフローで使うビジュアルを作ることだったんだ。<br>クールな技を見つけたんだけど、グリーンバックのスタート画像を生成して、それをローカルのLTXビデオ作成ワークフローに入れて、VDMXでグリーンバックのビデオでクロマレイヤーを作って、そこから進めるんだ。めちゃくちゃクリエイティブで楽しいぜ。だからAIアートは無駄じゃない！" userName="wsintra2022" createdAt="2025/08/05 12:58:02" color="#ff5733">}}




{{<matomeQuote body="商業的に成功しないとダメになるSOTAクラスのモデルにしては、価格は妥当だと思うぜ。" userName="Mtinie" createdAt="2025/08/05 00:12:15" color="">}}




{{<matomeQuote body="モデルの推論サーバーもリリースされてるぜ。WanとQwen-Imageは問題なく分割できる。<br>https://github.com/modelscope/DiffSynth-Engine" userName="42lux" createdAt="2025/08/05 09:18:39" color="#45d325">}}




{{<matomeQuote body="それは間違った画像だよ。CDNが古いメディアをキャッシュしてたんだ。もうパージしたから、正しいのが表示されるはずだ。<br>指摘してくれてありがとう！" userName="vunderba" createdAt="2025/08/05 03:55:48" color="#ff5c5c">}}




{{<matomeQuote body="M3 Ultra以降のCPUコアってFP8のハードウェアサポートあるのかな？" userName="RossBencina" createdAt="2025/08/04 21:31:31" color="">}}




{{<matomeQuote body="40GBあれば、軽く量子化すれば5090に載せられるね。" userName="cma" createdAt="2025/08/04 20:15:43" color="#45d325">}}




{{<matomeQuote body="いいリリースだね！GenAI Showdownサイトに追加したよ。全体的に40％くらいのスコアで、コンシューマー向けGPUで動かせるSOTAモデルだね（量子化すればさらに）。でも、txt2imgのプロンプト順守ではOpenAIのgpt-image-1にはまだ遠いかな。ただ、このモデルは編集とか色々なことができるってスレッドで言われてるね。<br>https://genai-showdown.specr.net" userName="vunderba" createdAt="2025/08/04 21:37:09" color="#ff5c5c">}}




{{<matomeQuote body="余談だけど、Imagen 3と4を混ぜるのは適切じゃないと思うな。全然違うモデルだから。" userName="cubefox" createdAt="2025/08/05 00:35:23" color="#ff33a1">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="Imagen3からの改善は大きくないと思ったけど、確かにその通りだね。最初はページがごちゃごちゃしてたんだけど、”Show/Hide Models”トグルを追加したから、その変更をするよ。" userName="vunderba" createdAt="2025/08/05 01:33:20" color="#ff5c5c">}}




{{<matomeQuote body="うん。”Imagen 4 Ultra”もあるんだよね（Gemini APIだと50%高いけど）。どれくらい違いがあるか分からないけど。" userName="cubefox" createdAt="2025/08/05 17:51:09" color="">}}




{{<matomeQuote body="4oの画像生成みたいに、画像を勝手に変えちゃわないのがすごいね。4oで誰かの服を修正しようとすると、顔まで変わっちゃうことがよくあるんだ。これは編集が必要な要素だけに、認識できるAIの痕跡が適用されるみたいだね。" userName="nickandbro" createdAt="2025/08/04 17:36:49" color="#38d3d3">}}




{{<matomeQuote body="だからFlux Kontextはすごいんだよね。手動でコンテンツをマスクしなくても、img2imgのインペイント機能が使えるんだ。<br>https://mordenstar.com/blog/edits-with-kontext" userName="vunderba" createdAt="2025/08/04 20:06:25" color="#ff33a1">}}




{{<matomeQuote body="みんなが自分で再現してみたいなら、プロンプト自体を含めないのは変だよね。" userName="diggan" createdAt="2025/08/05 09:48:16" color="#38d3d3">}}




{{<matomeQuote body="あー…それ良いアイデアだね！探してみるよ！" userName="vunderba" createdAt="2025/08/05 14:40:34" color="">}}




{{<matomeQuote body="4oなら編集したい範囲を選べて、それ以外はそのままにできるよ。" userName="herval" createdAt="2025/08/04 19:42:38" color="">}}




{{<matomeQuote body="gptはマスクを無視するんだよ。" userName="barefootford" createdAt="2025/08/04 20:28:28" color="#785bff">}}




{{<matomeQuote body="そうなんだよな。OpenAIが言ってるけど、試しても全然うまくいかなかったよ。" userName="icelancer" createdAt="2025/08/04 20:48:00" color="#ff33a1">}}




{{<matomeQuote body="普段やってる人には当たり前かもだけど、これ動かすにはどんなマシンがいるの？Linux機で16GB GPUと64GB RAMあるんだけど、SDは楽勝なのにQwen-imageはGPUもCPUもメモリ不足だったよ。どれくらい足りないんだろう？すごいハードウェアがいるの？" userName="rwmj" createdAt="2025/08/04 19:04:40" color="#45d325">}}




{{<matomeQuote body="普段やってる人にも分かりにくいよ。VRAM使用量の計算は超難しい。オンラインの計算ツールも使い物にならないし。とにかくこのモデルには40GB以上のVRAMが必要。システムRAMじゃ無理、Apple Siliconの統合RAMでも速度は出ないよ。" userName="icelancer" createdAt="2025/08/04 20:49:37" color="#ff33a1">}}




{{<matomeQuote body="あと、VRAMが40GBじゃなくて、40GBの”カード”が必要だと思うよ。前に書いたけど、1枚のカードがいるんじゃないかな。複数GPUの連結は無理なんじゃないかな。" userName="cellis" createdAt="2025/08/04 21:37:50" color="#ff5c5c">}}




{{<matomeQuote body="ああ、そうだね、一部のDiffusionモデルはレイヤー分割できないのを忘れてた。画像生成モデルはあまり使わないから、LLMの知識だけで話してたよ。誤解させてたらごめんね。" userName="icelancer" createdAt="2025/08/04 23:34:15" color="">}}




{{<matomeQuote body="意味がわからないか、LLMに詳しくないのかな？でもRTX 3090を2枚使えば動くし、GGUF化されたらRTX 3060みたいなローエンドカードでもいけるよ。" userName="rapfaria" createdAt="2025/08/04 22:04:55" color="#38d3d3">}}




{{<matomeQuote body="これはTransformerじゃなくてDiffusionモデルだよ。Diffusionモデルは計算ノード間で分割できないんだ。" userName="axoltl" createdAt="2025/08/04 23:55:16" color="#38d3d3">}}




{{<matomeQuote body="https://github.com/pollockjj/ComfyUI-MultiGPUのこと？1つのGPUが計算して、他のGPUがVRAM拡張で協力するってこと？（このノードは使ったことないけど）" userName="karolist" createdAt="2025/08/04 22:09:43" color="#ff33a1">}}




{{<matomeQuote body="Nah, RAMでレイヤーを入れ替える方がはるかにいいぜ。テキストエンコーダもRAMに置いとけば特にマイナスもないし、それ以外に大して得るものもないよ。" userName="AuryGlenz" createdAt="2025/08/05 06:54:38" color="">}}




{{<matomeQuote body="たとえ容量が足りたとしても、Nvidia以外のGPUだと画像生成が遅すぎるから、やる価値ないよ。" userName="AuryGlenz" createdAt="2025/08/05 06:52:54" color="">}}




{{<matomeQuote body="モデルファイルと大体同じサイズだと思うよ。transformersフォルダを見たら、5GBくらいのファイルが9個あるから、GPUのVRAMは45GBくらい必要そうだな。後々、VRAMが少なくても動く量子化バージョンが出るだろうけど、ちょっと品質は落ちるかもね。" userName="mortsnort" createdAt="2025/08/04 19:30:27" color="#ff5733">}}




{{<matomeQuote body="これについてはずっと彼らに言い続けてるんだよね。一つのリポジトリに複数のモデルウェイトがある場合があって、ファイルサイズを合計するだけじゃダメなんだ。でも「リポジトリサイズ」の表示はやっぱり便利だと思うよ。だから自分でツール作ったんだ。→ https://tools.simonwillison.net/huggingface-storage" userName="simonw" createdAt="2025/08/04 21:08:54" color="#ff5733">}}




{{<matomeQuote body="Hugging FaceはGGUFモデルについては、選択したGPUでどの量子化バージョンが動くか表示してくれるんだ。この機能がもっと多くのモデルタイプに対応してくれるといいんだけどね。" userName="Gracana" createdAt="2025/08/05 16:30:00" color="">}}




{{<matomeQuote body="Hugging FaceはただのGitホスティングサービスで、GitHubと同じようなもんだよ。ディレクトリ内の全ファイルサイズは自分で合計できるじゃん。" userName="matcha-video" createdAt="2025/08/04 20:23:44" color="">}}




{{<matomeQuote body="モデルサイズはVRAMに直結するって話だよ。FP16だと40GB、FP4に量子化すれば10GBくらいで動くかもね。" userName="halJordan" createdAt="2025/08/04 21:11:36" color="#ff5733">}}




{{<matomeQuote body="4bitに量子化されたバージョンが出るまで数日待つことになりそうだよ。これ、20Bパラメータもあるからね。" userName="zippothrowaway" createdAt="2025/08/04 19:27:32" color="#ff33a1">}}




{{<matomeQuote body="NF4量子化の設定例だよ。<br>こうするとVRAMは17GBくらい使うみたいだけど、あんまりうまく動かないね。このアプローチが推奨されてるらしいよ: https://github.com/QwenLM/Qwen-Image/pull/6/files" userName="pollinations" createdAt="2025/08/04 21:33:22" color="#ff33a1">}}




{{<matomeQuote body="Qwen-Imageのフルモデルだと少なくとも24GBのVRAMが必要だけど、4bit量子化版ならAutoGPTQとか使って約8GBのVRAMで動かせるよ。" userName="ethan_smith" createdAt="2025/08/04 23:26:18" color="#38d3d3">}}




{{<matomeQuote body="8bit量子化なら16GiBのRAMでいけるよ。これはSD3 Largeモデルを少しスケールアップしたものなんだってさ（38層→60層）。" userName="liuliu" createdAt="2025/08/04 21:07:44" color="#ff5733">}}




{{<matomeQuote body="プロダクションでの推論なら、H100一枚で十分動くみたいだよ。" userName="philipkiely" createdAt="2025/08/04 22:52:04" color="">}}




{{<matomeQuote body="P40カード2枚合わせれば300ドル以下で、これで動かせるらしいよ。" userName="cjtrowbridge" createdAt="2025/08/05 01:58:14" color="">}}




{{<matomeQuote body="40GBのVRAMが必要って事実が、たぶんみんなの熱意を冷やしてるんじゃないかな。<br>PCならPCIe 4.0 x16以上のスロットが2つあるマザボに、24GB VRAMのGPUを2枚挿せばいけるよ。友達のPCも「ぶっ飛んだ」マシンじゃないけど動いてるみたいだし。" userName="TacticalCoder" createdAt="2025/08/04 20:12:07" color="#785bff">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="「ぶっ飛んだ」マシンじゃなくても、安くはないよ。RTX 3090を複数枚使うならたぶん4,000ドルくらいかかるんじゃないかな。AI画像のためにそんな大金はちょっとね。" userName="ticulatedspline" createdAt="2025/08/05 00:44:12" color="#785bff">}}




{{<matomeQuote body="Diffusionモデルって、そんな風に分割しては動かせないんだよ。" userName="AuryGlenz" createdAt="2025/08/05 06:56:29" color="">}}




{{<matomeQuote body="画像生成AIって、ピクセルと同時にテキストのベクトル情報も出せば良くない？文字をピクセルで描くより、フォントとかサイズみたいな高レベルな情報で生成すれば、ビジネス資料とかでめちゃくちゃ綺麗になると思うんだけど。なんでそうしないの？" userName="pradn" createdAt="2025/08/05 14:10:21" color="#ff33a1">}}




{{<matomeQuote body="Qwen-Imageのデモ、英語の文字がおかしいよ。「The silent patient」が大文字になったり、「When stars are scattered」がスペース開いちゃったり。これで「すごい！」って言うのは、正直期待値が低すぎない？改善はしてるけど、まだまだだね。" userName="james_a_craig" createdAt="2025/08/05 09:32:52" color="">}}




{{<matomeQuote body="数ヶ月前までは文字すらまともに生成できなかったのに、こんなに進化してるのに、みんなの期待値ってどんどん上がっていくもんだね。すごい進歩なのに、なんか評価が厳しくなる一方だ。" userName="sixhobbits" createdAt="2025/08/05 13:06:40" color="">}}




{{<matomeQuote body="文字のレンダリング、どうやって学習させてるんだろうね？なんか文字だけ不自然で、影とか反射が画像と合ってないアーティファクトが共通してある気がする。OpenAIもFluxも同じ問題抱えてるから、もしかして同じ方法使ってるのかな？" userName="oceanplexian" createdAt="2025/08/04 20:29:28" color="#ff33a1">}}




{{<matomeQuote body="テクニカルレポート14ページに書いてあるんだけど、合成データを作る時に元の照明を無視して文字を重ねてるんだって。だからモデルもその不自然な見た目を再現しちゃうんだよ。まさに「ゴミを入れればゴミが出る」ってやつだね。いつかリアルな文字生成のためのデータ作る方法が出てくるといいな。" userName="yorwba" createdAt="2025/08/04 20:39:20" color="#ff33a1">}}




{{<matomeQuote body="それならレンダリングした画像を使うのが理にかなってるんじゃない？" userName="Maken" createdAt="2025/08/04 22:39:33" color="">}}




{{<matomeQuote body="そこまで「ゴミ」って言うほどじゃないんじゃない？合成データって、汎化のためにはむしろ役立つはずだよ。自己教師ありモデルのいいところってそういうことじゃないの？" userName="doctorpangloss" createdAt="2025/08/04 21:11:10" color="">}}




{{<matomeQuote body="ノイズからこんなに読める、正確な文字を生成できるのを「ゴミ」なんて言うなんて、IT系の人間の傲慢さには本当に驚くね。ここで何してるんだよ？" userName="halJordan" createdAt="2025/08/04 21:13:56" color="">}}




{{<matomeQuote body="彼らが「ゴミ」って言ってるのは、訓練データのことだよ。拡散プロセスについてじゃないからね。" userName="bavell" createdAt="2025/08/05 04:24:49" color="">}}




{{<matomeQuote body="中国からこんなにたくさんの良いオープンソースモデルが出てくるなんて、マジで希望が持てるね。すごいことだよ。" userName="artninja1988" createdAt="2025/08/04 17:45:39" color="">}}




{{<matomeQuote body="これってAIバブルを後押しする戦略みたいだね。今の大手テック企業の設備投資は、もはや失敗できないレベルだからな。" userName="owebmaster" createdAt="2025/08/05 02:42:09" color="#ff33a1">}}




{{<matomeQuote body="LLMの世界で具体的なリードを取るのは、中国にとって国家的な大勝利になるだろうね。" userName="tokioyoyo" createdAt="2025/08/05 07:56:41" color="#785bff">}}




{{<matomeQuote body="セクション3.2のデータフィルタリングを見てみてよ。ここだよ：https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Q..." userName="djoldman" createdAt="2025/08/04 17:06:46" color="#45d325">}}




{{<matomeQuote body="英語と中国語以外の言語が言及されてないし、表示もされてないのはちょっと面白いよね…" userName="numpad0" createdAt="2025/08/04 20:44:36" color="">}}




{{<matomeQuote body="記事は読んでないけど、最初のプロンプトをドイツ語で入れたら（HF-Demoで）ちゃんと生成してくれたよ。" userName="entropie" createdAt="2025/08/05 11:52:47" color="#45d325">}}




{{<matomeQuote body="天安門広場で戦車の列の前に一人で立っている人の画像を生成できるかな？" userName="doubtfuluser" createdAt="2025/08/05 16:33:42" color="#38d3d3">}}




{{<matomeQuote body="モデルの潜在的な欠点をオープンに議論せずに使ってるのが本当に心配になってきたよ。どこかでモデルとその問題点のリストを持つべきだね。" userName="doubtfuluser" createdAt="2025/08/05 16:34:43" color="#785bff">}}




{{<matomeQuote body="AIから”コンテンツセキュリティ警告：入力テキストデータに不適切なコンテンツが含まれている可能性があります”って言われたよ。" userName="qingcharles" createdAt="2025/08/06 06:08:55" color="#38d3d3">}}




{{<matomeQuote body="試してみたけど、すごく印象的な結果だったよ。Qwenチームがどうやってこれをこんなにうまく機能させたのか不思議だね。ここにアクセスしてみて：https://chat.qwen.ai/ （画像生成を選んで、Qwen3-235Bモデルを使うようにしてね。Coderも試したけどエラーになったよ。）" userName="metadat" createdAt="2025/08/05 09:18:03" color="#38d3d3">}}




{{<matomeQuote body="どの画像モデルも、例えば午後3時15分を示す時計を生成するみたいに、時間を表示するのは苦手みたいだね。" userName="android521" createdAt="2025/08/05 09:34:37" color="#38d3d3">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
