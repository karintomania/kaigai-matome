+++
date = '2025-04-06T00:00:00'
months = '2025/04'
draft = false
title = 'AIの進歩って結局ハリボテ！？業界関係者がぶっちゃけトーク'
tags = ["AI", "LLM", "機械学習", "自然言語処理", "誇大広告"]
featureimage = 'thumbnails/cyan2.jpg'
+++

> AIの進歩って結局ハリボテ！？業界関係者がぶっちゃけトーク

引用元：[https://news.ycombinator.com/item?id=43603453](https://news.ycombinator.com/item?id=43603453)

{{<matomeQuote body="AIの話題で一番大きいニュースは数週間前に出たのに全然注目されてないよね。USAMOでSOTAモデルの平均スコアが5%だったらしいよ（うろ覚えだけど、マジで低い数字）。IMOの問題で50%とか60%とか取れてるはずなのにさ。これってAIモデルが過去の結果をただ覚えてるだけで、問題を実際に解いてないってことじゃん？誰もこのこと言わないのが信じられないんだけど。企業がトレーニングデータからテストデータ（IMO、ICPCとか）を除外するためにどんな努力をしたのか教えてくれないのもおかしいよね。" userName="InkCanon" createdAt="2025-04-06T20:03:33" color="#38d3d3">}}

{{<matomeQuote body="＞Yes、リンクはこちら：<br>＞”https://arxiv.org/abs/2503.21934v1”<br>個人的には、学部レベルの数学の問題でo3-miniを試してるんだけど、GPT-4よりも「plug-and-chug」な証明が得意みたい。でも、そういう問題って本質的に面白くなくて、教育的なものなんだよね。何か洞察が必要な問題だと、LLMがその問題を前に見たことあるのがバレバレな答えか、一見正しそうだけど反証するのがめっちゃ大変な答えが出てくる。後者がLLMの誇大広告の秘密のソースってわけ。学部でSTEM専攻の学生がこれ使い始めたらどうなるか心配だよ。回転と球面幾何学の問題を出したら、高度な幾何代数が出てきて、球面の三角形を描いてほしかっただけなのに…答えを知らなかったらマジで混乱してたと思う。LLMがレクリエーション数学者を迷わせた実例もあるよ。" userName="AIPedant" createdAt="2025-04-06T20:52:07" color="">}}

{{<matomeQuote body="LLMって基本的な西洋音楽理論すら理解できないんだから、それより難しいことに使えるわけないじゃん。" userName="apercu" createdAt="2025-04-07T14:22:39" color="">}}

{{<matomeQuote body="うちの息子の学校じゃ、宿題は全部LLM対策されてるらしいよ。だからLLMは教育には使えないね。（あと、最近の若い子たちはLLMのことマジでつまんないと思ってる。）" userName="otabdeveloper4" createdAt="2025-04-07T10:36:53" color="">}}

{{<matomeQuote body="どうやって宿題をLLM対策するの？もし本当にできるなら、すごいビジネスチャンスになるかも。LLMが教育を急速に破壊してるからね。" userName="bambax" createdAt="2025-04-07T10:47:56" color="">}}

{{<matomeQuote body="紙とペンで試験をして、宿題を自分でやるのが唯一の有効な準備方法だって教えるしかないんじゃない？" userName="hyperbovine" createdAt="2025-04-07T14:58:23" color="">}}

{{<matomeQuote body="甘いね。昔はそう思ってたけど、今じゃ試験中にスマホを隠し持ってるのが当たり前だよ。先生が生徒を身体検査するわけにもいかないし。スマホを回収しようとしても、古いスマホを提出して、現役のスマホは隠し持ってるんだよ。提出されたスマホがちゃんと動くか電話をかけて確認するのも時間がかかるし。AIが教育をどれだけダメにしてるか、マジで誰もわかってないよ。" userName="bambax" createdAt="2025-04-07T17:02:25" color="#45d325">}}

{{<matomeQuote body="（笑）標準的じゃない問題を出して、答えと一緒に理由や説明を求めればいいんだよ。LLMも「推論」できるけど、LLMの出力だってすぐわかるし。（先生にとっては大変な作業だけどね。レポートを宿題に出すだけでよかった時代は終わったんだ。）" userName="otabdeveloper4" createdAt="2025-04-07T11:04:16" color="#45d325">}}

{{<matomeQuote body="LLM初期の頃に見つけた例だけど、「2ポンドの鉄と1ポンドの羽、どっちが重い？」って聞くと、「両方とも同じ」って答えに誘導できたんだ。有名な「1ポンドの鉄と1ポンドの羽」の問題をたくさん学習してたからね。今のモデルでも同じことができるけど、もっと工夫が必要だよ。よく知られた問題と「近い」けど、重要な点で異なる質問をすると、間違った答えを引き出せる。でも、LLMを騙せる問題と人間を騙せる問題の差はどんどん小さくなってるんだよね。人間だって入力を完璧に解析するわけじゃないし…" userName="jerf" createdAt="2025-04-07T14:31:49" color="#ff33a1">}}

{{<matomeQuote body="Googleに「Boeing 737の客室にゴルフボール何個入る？」って聞いたら、AIが4段階で答えてきたんだけど…<br>1) 客室は約3000立方メートル（間違い。300立方メートルくらい）<br>2) ゴルフボールは約0.000004立方メートル（間違い。0.00004立方メートルくらい）<br>3) 3000 / 0.000004 = 750,000（間違い。750,000,000）<br>4) 席とかあるから調整して、完璧には詰められないから…150万～200万個くらいかな！（減らすべきだろ！）<br>マジでAIの現状に懐疑的になっちゃった。たまに凄いアウトプット出すくせに、こういう失敗で印象がマジで変わるわ。" userName="billforsternz" createdAt="2025-04-07T03:12:04" color="#785bff">}}

{{<matomeQuote body="＞AIのアウトプットで凄いものを見たことがあるから、こういう失敗で印象がマジで変わるっての、マジで共感。<br>初めてマジシャンが”本物”の魔法じゃなくて、手品と心理トリックだって知った時みたい。もうどんなに凄いトリック見ても、”本物”だって思えないんだよね。タネが分からなくても、ニセモノだって分かる。" userName="aezart" createdAt="2025-04-07T07:46:11" color="#785bff">}}

{{<matomeQuote body="ここには大きな隔たりがあると思うな。大人はみんな魔法が”ニセモノ”だって知ってるけど、それでも驚いて楽しめる人もいれば、つまんないって人もいる。<br>俺は後者だけど、分かってても楽しめる人が羨ましいと思う時もある。" userName="bambax" createdAt="2025-04-07T10:54:51" color="">}}

{{<matomeQuote body="魔法、特にクロースアップマジックはマジで面白いと思うけど、タネに興味を持つことを嫌う風潮は嫌い。<br>俺的には、観客に見せたいトリックと、そのタネは表裏一体だと思うんだよね。どっちかだけじゃ面白くない。" userName="tshaddox" createdAt="2025-04-07T15:11:16" color="#ff5733">}}

{{<matomeQuote body="会社のAIチャットボットに同じ質問してみた。<br>ゴルフボールの体積は正しかった(0.00004068立方メートル)けど、客室の体積は1000立方メートルって過大評価してた。<br>最終的な計算は24,582,115個で、まあまあ正確。でも、電卓としては微妙…<br>座席とか考慮してないけど、球体のパッキング効率の悪さは認めてて、実際には「もう少し少ない」って言ってた。でも、概算は出してくれなかった。<br>詰め込んだ時の密度74%で見積もったら18,191,766個だって。計算ミスってるけど、文脈的には誤差かな。<br>座席とか考慮したらどうなるか聞いたら、客室の体積を30%減らして計算し直してた。でも、計算が全然ダメ。700 ÷ 0.00004068 = 17,201,480って言ってたし(約6千個違う)。17,201,480 × 0.74 = 12,728,096 (約1千個違う)。<br>計算が間違ってるって言っても、同じ数字が出てくる。正しい答え教えたら、それを元に最後の計算だけやり直してた。<br>AIチャットボットが「推論」できるって言うけど、まさかこんな基本的な計算でコケるとは思わなかった。ゴルフボールの数を概算する程度ならいいけど、もっと深刻な質問には不安になるわ。" userName="CivBase" createdAt="2025-04-07T14:51:53" color="">}}

{{<matomeQuote body="面白いことに、Google AI Studioだと、最新のGemini 2.5Proから軽量版のGemma 2まで、ほぼ正しい答えを出すんだよね。球体のパッキング効率も認識してるし。<br>でも、Google検索は同じようなデタラメな答えを出してきた。検索に使われてるモデルは、マジでクソ安いやつなんだろうな。全然最先端じゃない。" userName="greenmartian" createdAt="2025-04-07T05:04:37" color="#38d3d3">}}

{{<matomeQuote body="検索は要約に特化した小さくて高速なモデルを使うのは当然だよね。Google検索は1日に140億回も検索されるんだから。もっと大きなモデルを使うには、計算リソースが足りなさすぎる。" userName="aurareturn" createdAt="2025-04-07T05:22:02" color="#ff5c5c">}}

{{<matomeQuote body="検索って重複が多いじゃん？ゴルフボールの数みたいな質問は、キャッシュできると思うけどな。" userName="fire_lake" createdAt="2025-04-07T06:17:57" color="">}}

{{<matomeQuote body="IMO（国際数学オリンピック）で50～60%のパフォーマンスを出したLLM（大規模言語モデル）は報告されてないよ。USAMO（米国数学オリンピック）でSOTA（最先端）のLLMが5%のスコアを出すのは予想通り。IMOで50～60%のパフォーマンスを出したのはAlphaProofだけど、AlphaProofはLLMじゃない。論文はまだ出てないけど、AlphaProofはAlphaFoldみたいに、LLMの上に色んな要素を重ねたシステムだってのは明らか。" userName="sanxiyn" createdAt="2025-04-07T01:04:22" color="">}}

{{<matomeQuote body="計算生物学の研究者だけど、細胞の挙動を推論する新しい機械学習のアプローチに取り組んでるんだ。今、アルゴリズムが収束しなくて困ってる。<br>だから、ChatGPT-o3-mini-highに数式を説明して、何が問題なのか推論してもらおうとしたんだけど、マジで役に立たなかった。ブログみたいな「ML入門」レベルの解決策しか出してこない。数式的な文脈を無視して、「収束しない」ってことだけに注目して、学習率を下げろとか言ってくる。３週間前に試したわ！どんなに言っても、問題について意味のある「推論」をしてくれない。たぶん、似たような問題を見たことがないんだよね。潜在空間で一番近いのが、Adamに関する大量のMediumの記事なんだろうな。だから、それらの統計的な平均値が出てくる。<br>Terence Taoみたいな人が、これらのモデルを「平凡な大学院生」みたいだって言うけど、マジでイライラする。マジでTaoの言う「平凡な大学院生」に見て欲しいけど、無理っぽい。代わりに、三流MLブログの作者が出てくる。<br>PS：ここまで読んだ人いるか分からないけど、密度推定に詳しい人で手伝ってくれる人がいたら、bglazer1@gmail.comまでメールしてね！マジで面白い数学パズルだし、生物学もかなり面白いよ。" userName="bglazer" createdAt="2025-04-06T22:20:27" color="">}}

{{<matomeQuote body="個人的にはChatGPTよりClaudeの方が好きだから、最新モデルを試してみてほしいなー。ただ、3.7は前の3.5よりちょっと劣ってる気がするんだよね。" userName="airstrike" createdAt="2025-04-07T04:22:04" color="">}}

{{< details summary="もっとコメントを表示（1）">}}
{{<matomeQuote body="3.7って3.5と比べてどこがイマイチなの？最近Claude使い始めたから、比較対象がなくて。" userName="pdimitar" createdAt="2025-04-08T10:45:49" color="">}}

{{<matomeQuote body="Gemini 2.5がテストで25%のスコアを出したらしいよ。つまりAIはどんどん進化してるってこと。あと、LLMがちゃんとした数式証明を書くのが苦手って話だけど、それは事実だよね。" userName="usaar333" createdAt="2025-04-06T21:34:39" color="">}}

{{<matomeQuote body="＞within a week<br>Gemini 2.5が新しい問題で特別に学習されたり、fine-tuningされたりしてないってどうしてわかるの？新しいモデルが急に前のモデルより5倍も良いスコアを出すなんて信じられないんだけど。" userName="selcuka" createdAt="2025-04-07T02:33:41" color="#ff5c5c">}}

{{<matomeQuote body="たった1週間で、特定の評価指標のためにモデルを再学習させるなんてありえないでしょ。モデルの性能が5倍になることなんてよくあるし。Winograd schemaみたいな昔は無理だった問題も、今は簡単に解けるようになってるじゃん。Rs in strawberryとか、動物を川に運ぶ問題とかもそう。" userName="levocardia" createdAt="2025-04-07T02:44:58" color="#ff5c5c">}}

{{<matomeQuote body="動物を川に運ぶ問題は全然解決されてないよ。全然理解してないし、既製品のソリューション使ってるだけで、ちゃんと推論できてないんだよね。<br>例えば、すごく簡単なパターンで失敗してる例がこれ:<br>https://xcancel.com/colin_fraser/status/1864787124320387202<br>Claude 3.7も意味不明:<br>https://xcancel.com/colin_fraser/status/1898158943962271876<br>DeepSeek:<br>https://xcancel.com/colin_fraser/status/1882510886163943443#...<br>ワイングラスから水が溢れる問題もちゃんと解決されてないよ！ワイングラス自体はなんとなく解決されてるけど（見た目が不自然で泡立ってるけど）、花瓶に水を注ぐと昔と同じ問題が起きるんだよね。OpenAIがワイングラスをRLHFで学習させたんだろうけど、結局はモグラ叩きで、根本的な理解には繋がってないんだよ。" userName="AIPedant" createdAt="2025-04-07T03:24:22" color="#ff5733">}}

{{<matomeQuote body="Gemini 2.5 Proは農夫の問題のバリエーションを正しく解いたらしいよ:<br>https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%..." userName="leonidasv" createdAt="2025-04-07T04:42:35" color="">}}

{{<matomeQuote body="これって結構当たり前のことだよね。もしちゃんと推論できるなら、チェスみたいな複雑なゲームも（下手でも）できるはずじゃん。なのに、ランダムに動くbotに勝つのがやっとってどういうこと？https://maxim-saplin.github.io/llm_chess/" userName="AstroBen" createdAt="2025-04-06T21:45:29" color="#38d3d3">}}

{{<matomeQuote body="LLMはチェスできるし、3.5 turbo instructは人間レベル（ELO1800）で結構上手いよ。ってことは、推論できるってこと？https://github.com/adamkarvonen/chess_gpt_eval" userName="og_kalu" createdAt="2025-04-06T23:45:45" color="">}}

{{<matomeQuote body="チェスの話じゃなくて、LLMが訓練されてないけど、推論能力で解けるはずの問題があるってことが言いたかったんだよね。人間がルールを覚えただけで戦略を知らなくても滅多に負けないような相手に負けるってどういうこと？企業は難しい試験に合格したり、博士レベルの問題を解いたり、人間を代替できるって言ってるのに、ランダムbotに負けるの？AGI目前なのに、訓練されてないことには推論能力が全然ないってどういうこと？" userName="AstroBen" createdAt="2025-04-07T02:22:39" color="#785bff">}}

{{<matomeQuote body="もしLLMがチェスのために訓練されたら、そのパフォーマンスは単なる暗記によるもので、「推論」なんかじゃないってことだよね。そんなレベルで、あんなにたくさんのゲームや手を暗記だけでこなせると思ってるなら、もう何も言うことないよ。ありえないから、それはまずハッキリさせておこう。<br>＞これらの企業は、自社製品が超難関試験に合格したり、博士レベルの問題を解決したり、人間を代替する寸前だと騒いでいるのに、ランダムな戦略のチェスボットすら倒せないってどういうことなの？<br>別に不思議じゃないよ。実際にゲームを見たことある？LLMは推論が下手なわけじゃなくて、ゲームのルールを知らない機械みたいなんだよね。LLMは予測して失敗して、少しずつ改善していくってのを繰り返して学習するんだよ。複雑なゲームのルールを学ばせたいなら、そうやって予測させるしかない。チェスの本で学習させても、チェスについて会話する方法を学ぶだけだよ。<br>人間には、知能とは裏腹な変な失敗パターンがあるよね。それを面白い名前で呼んで笑い飛ばしたりするけど、機械にも同じようなものがあるってだけだよ。一番上のコメントでGemini-2.5-proが5日も経たずにベンチマークで25%に達したって書いてあったけど、あれは特に笑えた。" userName="og_kalu" createdAt="2025-04-07T03:21:28" color="">}}

{{<matomeQuote body="3.5 turbo instructはマジで飛び抜けてるよね。<br>https://dynomight.substack.com/p/chess<br>ここでも議論されてるよ：<br>https://news.ycombinator.com/item?id=42138289" userName="hatefulmoron" createdAt="2025-04-07T00:34:48" color="">}}

{{<matomeQuote body="それ、ちょっと言い過ぎかも。少なくとも、再現不可能な偉業って意味ならね。<br>Eleuther discordでは、1200から1300くらいのモデルが訓練されてるし、グランドマスターレベルのtransformerもあるよ。<br>https://arxiv.org/html/2402.04494v1<br>OpenAIとかAnthropicとかは、LLMにチェスをさせることにあんまり興味ないんだと思う。それか、post trainingが邪魔してるのかもね。" userName="og_kalu" createdAt="2025-04-07T02:14:18" color="">}}

{{<matomeQuote body="＞それ、ちょっと言い過ぎかも。少なくとも、再現不可能な偉業って意味ならね。<br>いやいや、3.5 turbo instructをわざわざ持ち出して、他のモデル、例えば3.5 turboとか、それ以降のモデルを挙げないのはなんで？明らかに飛び抜けてるじゃん。少なくとも、「LLM」を最近のモデルの広い範囲で考えた場合はね。<br>もしLLM/transformerモデルがチェスデータで訓練すればチェスをプレイできるようになるって言うなら、それは同意だよ。<br>AstroBenが指摘したのは、LLMは数学やプログラミングのタスクを解けるのに、チェスのような分野には推論能力を応用できないってことだと思うんだよね。それって不思議じゃない？" userName="hatefulmoron" createdAt="2025-04-07T02:31:48" color="#785bff">}}

{{<matomeQuote body="最高の例だから挙げたんだよ。反例は一つあれば「できない」を否定できるからね。他の例もあるし。<br>＞AstroBenが指摘したのは、LLMは数学やプログラミングのタスクを解けるのに、チェスのような分野には推論能力を応用できないってことだと思うんだよね。それって不思議じゃない？<br>別に。LLMはチェスのルールを知らないみたいにプレイするんだよ。推論が下手なんじゃなくてね。予測して失敗するのが学習方法なんだから。チェスみたいなゲームを学ばせたかったら、チェスの手を予測させるしかない。チェスの本で訓練しても、チェスについて会話する方法を教えるだけだよ。" userName="og_kalu" createdAt="2025-04-07T03:13:48" color="">}}

{{<matomeQuote body="＞反例は一つあれば「できない」ってナンセンスを否定できるからね。他の例もあるし。<br>了解。チェスデータを大量に投入すれば、チェスが上手くなるのは間違いないよね。<br>次の段落で言いたいことがよくわからないんだけど。LLMはチェスのルールをよく知るための訓練データはたくさん持ってるはずだよ。それに、その知識を使って dots を繋げて実際にプレイできる推論スキルもあるはずだよね。チェスのゲーム訓練データを大量に投入すればこの問題を隠せるのは当たり前だけど、そのやり方が成功したからって、推論能力が高いってことにはならないよね。" userName="hatefulmoron" createdAt="2025-04-07T03:33:03" color="">}}

{{<matomeQuote body="勾配降下法はアホな最適化アルゴリズムだよ。LLMの訓練は人間が本を読むのとは全然違って、進化が何世紀もかけて適応を調整するのに近いんだ。どちらのプロセスも、何に向かって収束しているのかを意識しているとは期待できないよね。だから、チェスのことがたくさん書かれた本で訓練しても、チェスについて上手に話せるモデルになるだけだよ。ルールについて話せるけど、プレイは下手でも驚かないな。<br>それに、その記事にはフォローアップがあったよ。Post-trainingがうまくいってないのが問題なのかも。ちょっと例を増やしたり、反芻させたりするだけでも影響があるみたいだし。<br>https://dynomight.net/more-chess/" userName="og_kalu" createdAt="2025-04-07T03:49:54" color="#785bff">}}

{{<matomeQuote body="USAMO（ましてやIMO）での平均的な人間のスコアはゼロだよ。ソースは、韓国数学オリンピックでメダルを取った俺。" userName="sanxiyn" createdAt="2025-04-07T08:17:36" color="">}}

{{<matomeQuote body="これらのモデルが実際にどのように動作するかを考えれば、そんなに驚くことかな？次のトークン予測 != 推論だってことを指摘して、過去3年間で容赦なくdownvoteされてきたコメンターたちを代表して、溜飲が下がったよ。" userName="hyperbovine" createdAt="2025-04-07T14:56:22" color="#ff33a1">}}

{{<matomeQuote body="この記事のポイントは、LLMってやつが何か報告したがるせいで、大げさに言っちゃうってことだよね。プログラマーが期待するほど「いや」って言えないんだよ。質問すると、基本イエスって言っちゃうし。<br>だから、LLMの開発競争でベンチマークのスコアが上がっても、それはまやかしなんだって。LLMの根本的な問題は、とにかく人に合わせようとしちゃうことで、そこが全然改善されてないんだよね。だから、モデルが正解できる数学の問題が5/100増えたとしても、ChatGPTみたいなお決まりの質問より複雑なプロンプトだと、体感的にはあんまり変わらないんだって。業界はツールが足りないって分かってるけど、それが何かはまだ分かってないって感じかな。Cursorみたいなマジで自律的なパフォーマンスは上がってきてるけど、まだ進化の途中だよね。俺も、評価すべきは個別の応答じゃなくて、自律的なシナリオでのモデルの評価だってことにめっちゃ同意だわ。" userName="iambateman" createdAt="2025-04-06T19:34:31" color="#ff5733">}}

{{<matomeQuote body="＞LLMは根本的に何かを望んでいるわけではない<br>LLMが何かを望むことはないよ。でも、LLMをトレーニングして、仕事で使えるモデルを作ってる会社は、LLMが人に合わせようとするように見せたいんだよね。" userName="bluefirebrand" createdAt="2025-04-06T21:38:17" color="">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（2）">}}
{{<matomeQuote body="＞LLMは根本的に何かを望んでいるわけではない<br>確かにLLMは何も望んでないよね。でも、強化学習では、モデルは報酬を最大化するように訓練されるから、何かを望んでるって表現することが多いんだ。これはただの言い方で、本当に自律性があるって言ってるわけじゃないよ。" userName="JohnKemeny" createdAt="2025-04-07T06:55:38" color="">}}

{{<matomeQuote body="強化学習で報酬を最大化するって？それってウサギがニンジン好きなのと同じ理屈？LLMは何を望んでるの？そもそも、強化学習を使って報酬を求めてるって言う時点で、根本的な間違いを犯してない？" userName="squiggleblaz" createdAt="2025-04-07T13:56:33" color="">}}

{{<matomeQuote body="それ、理にかなってると思うけど、企業は「人に合わせる」ってのが色々あるのを忘れてるんだよね。LLMのアプローチは、.NETについての質問には何でも答えるけど、コーディングで自分で墓穴を掘るのを止めない同僚みたいなもん。それに対して、「ちょっと座って、何をやろうとしてるのか見直そうよ。だって、質問がバラバラすぎるんだもん」って言う人もいるじゃん。<br>政治的な信念と、仕事で特に役に立たないって理由で、LLMを使うのをやめたんだ。過去に色々なモデルをソフトウェア開発で使ってみたけど、LLMが設計ミスとか考え方の間違いに気づかないのが問題だと思ったんだよね。ほとんどの場合、設計ミスとか考え方が間違ってるのが原因なんだ。LLMは、やろうとしてることが間違った設計の兆候だって教えてくれないんだよね。人に合わせつつも、過去の決定の問題点を指摘する方法もあるはずなのに。" userName="mrweasel" createdAt="2025-04-07T12:21:26" color="#785bff">}}

{{<matomeQuote body="LLMをコントロールするのはあんたの責任だよ。たまに、自分が墓穴を掘り始めてるんじゃないかって心配になるから、「これって今まで聞いた中で一番バカなアイデア？」って聞くと、「もっといい方法があるかも」って言ってくれるんだ。疑わしい時は、最初にその質問をすることもあるよ。（でも、マジで的外れなことをしてると、幻覚を見始めるんだよね。それに気づくのは、たいていそれが最初。）" userName="squiggleblaz" createdAt="2025-04-07T13:53:21" color="">}}

{{<matomeQuote body="＞LLMをコントロールするのはあんたの責任だよ。<br>そうそう。問題はコントロールで、NLPはコンピューターをコントロールするインターフェースとしてはイマイチなんだよね。コードは最高。それがソフトウェア開発におけるLLMに対する懐疑的な見方のポイントだよ。" userName="namaria" createdAt="2025-04-09T08:45:09" color="">}}

{{<matomeQuote body="たしかに、データセットには「人に合わせる」系のコンテンツが多いのかもね。だって、めっちゃ不愉快な内容って、短いか、炎上につながる前触れの場合が多いし。" userName="Terr_" createdAt="2025-04-07T01:09:46" color="">}}

{{<matomeQuote body="それマジであるある。Claudeにコードを書かせれば書かせるほど、でたらめなことを言い出すんだよね。だいたい50～60％は削除してるわ。それに、'テストだけ書いて'って頼むと、50％の確率で実行しようとして、どうでもいい問題で失敗して、テストコードの90％を削除して、幻覚のラビットホールにどんどんハマっていくんだよね。<br>…単に俺のプロンプトが下手なだけかも、hehe" userName="boesboes" createdAt="2025-04-07T15:27:40" color="">}}

{{<matomeQuote body="もしかして、俺がプロンプト下手なだけかも(笑)<br>誰かが、ソフトウェア開発でLLMの有用性を主張するために、プロンプトをもっと上手く書けとか、リポジトリにLLM用のルールを追加しろとか言うたびに、NLPを使うのは逆効果だって言ってるようなもんだよね。<br>コードのいいところって、コンピュータの動きをめっちゃ具体的にコントロールできることじゃん。LLMを使うメリットは、そんなに細かく指定しなくてもいいから楽ってことなのに。プロンプトをもっと具体的に書けって言うなら、それってつまり、コーディングにNLPを使うのはやめた方がいいってことにならない？" userName="namaria" createdAt="2025-04-09T08:49:08" color="">}}

{{<matomeQuote body="LLMって、仕事でイエスマンばっかり抱えてるのと同じ問題があると思うんだよね。中途半端なイエスしか返ってこなくて、本当は「ノー」とか「条件付きイエス」って答えてほしかったのに、結局みんな困るみたいな。<br>もしかしたら、経営幹部がLLM/GenAIに夢中なのって、雇わなくてもいいイエスマンだからかもね。それに、いつものことながら、専門知識がないから、デタラメ言われてることに気づかないんだよ。" userName="tristor" createdAt="2025-04-07T14:46:49" color="#45d325">}}

{{<matomeQuote body="＞この記事のポイントは、LLMが何か報告したがるってことで、だから大げさになりがちなんだよね。プログラマーが期待するほど「ノー」って言えないんだよ。<br>うーん、これってつまりこういうことじゃない？(記事から引用):<br>＞社内ベンチマークとか、同僚とか僕自身の感覚からすると、AI企業が発表してる成果って、経済的な有用性とか汎用性とはかけ離れてると思うんだよね。<br><br>その数行後にはこう書いてある:<br>＞もしかしたら謎でもなんでもないのかも。AI研究機関は嘘をついていて、ベンチマークの結果が良くなってるのは、答えを事前に見て覚えてるからなのかもね。" userName="signa11" createdAt="2025-04-07T07:09:36" color="#38d3d3">}}

{{<matomeQuote body="何か聞いても絶対に「ノー」って言わないんだよ。イエスって言いまくって、金だけ巻き上げる。" userName="malingo" createdAt="2025-04-07T03:50:25" color="">}}

{{<matomeQuote body="メタコメントだけど、こういう投稿への反応って、(a)めちゃくちゃ意見が分かれてて、(b)完全に個人的な話に基づいているのが面白いよね。<br>俺にも意見はあるけど、それも結局は個人的な話とか、経験則に基づいた判断だって言えると思う。<br>でも、いつか誰かが正しくて、誰かが間違ってるってことになるんだよね。AIに関して「より良い選択」をする能力って、どんな特徴でわかるんだろう？たとえ「より良い」って何なのか証明できなくても。" userName="lukev" createdAt="2025-04-06T19:34:47" color="#ff33a1">}}

{{<matomeQuote body="個人的な経験を共有するのは全然悪いことじゃないよ。ここで経験談を読むことで、自分の経験がみんなと共通してるのかどうかわかるし。それに、もし自分が失敗してるなら、他の人がどうやって解決したのか知りたいじゃん。<br>それに、この記事のテーマであるLLMの実際の影響について話してるんだから、ベンチマークの結果よりも、経験談の方が役に立つこともあると思う。それに、LLMの使い道や状況は人それぞれ違うから、他の人と同じ結果が出なくても、誰かが間違ってるとは限らないし。ウェブ開発者が「LLMに懐疑的な人が理解できない」とか言って、LLMの使い方を説明し始めると笑っちゃう。" userName="freehorse" createdAt="2025-04-06T21:14:42" color="#785bff">}}

{{<matomeQuote body="確かに、経験談を共有するのは全然悪いことじゃないよね。問題は、個人的な経験だけで broad assumptions をして conclusion を出してしまうこと。残念ながら、そういうことが多すぎるんだよね。それは人間の本能だから、意識して抑えないといけない。" userName="otterley" createdAt="2025-04-06T23:43:05" color="">}}

{{<matomeQuote body="人は日々 decision をしないといけないから、 conclusion を出すんだよ。完璧な証拠が出るまで待ってられないじゃん。データは参考になるけど、もし完璧な objective benchmark がある X llm が自分には使いこなせなくて、Y llm の方が results が良かったら、自分の経験に基づいて decision するのが賢明でしょ。もしくは、LLM を使った workflow が上手くいってるなら、他の人が LLM は使えないって言っても、やめる意味ないし。<br>実際的な証拠がない場合は、経験談が一番良い情報源になることもあると思う。重要なのは、経験談が食い違う理由を理解すること。それは主に状況的な要因によるもので、状況が変われば conclusion も変える柔軟性を持つべき。" userName="freehorse" createdAt="2025-04-07T12:51:09" color="#38d3d3">}}

{{<matomeQuote body="100%同意。データが足りない場合は、類推とか、個人的な観察、又聞きなどの他の情報源に頼るしかないよね。でも、自分の限られた経験が真実だって主張する人が多すぎる。本当は、反証する evidence とデータが簡単に入手できるのに。" userName="otterley" createdAt="2025-04-07T17:44:40" color="">}}

{{<matomeQuote body=" рационалист delusionってやつにハマってるんじゃない？人は自分の経験だけで結論出すじゃん。客観的な証拠なんてマジで稀だし。しかも、そういう証拠って歪められやすいし。「本能に逆らって」 рациональноになるって、結局どの怪しい証拠を信じるかって話になるだけじゃね？" userName="droopyEyelids" createdAt="2025-04-07T04:23:33" color="">}}

{{<matomeQuote body="ตอบกลับมัน anekdotischになっちゃうのは当然じゃね？手っ取り早く伝えるには短くするしかないし。「良い AI」ってのは、結局 slope-gradient アルゴリズムがどれだけ local maximaに辿り着けるかって話でしょ。 generative modelがマジで「意思決定」できるようになるまでは、ただの凄い линейная алгебра solverだよ。Generative machine learningはエンドユーザーを喜ばせるのが目的で、人間の意思決定レベルには全然届かない。" userName="FiniteIntegral" createdAt="2025-04-06T20:00:58" color="">}}

{{<matomeQuote body="ウザかったらごめん。でも、人間の賢い意思決定みたいなのって超嬉しいじゃん？ image generatorが変な手を描いちゃうのも、そうしたいんじゃなくて、頑張ってるけどまだ下手なだけだよ。GPT 4の初期版(gpt-4-0314)で感動するような refactoringしてくれたのに、最近同じような質問をしたら OpenAIのモデルが酷くて hallucinatedしまくり。Gemini 2.5だけまともだった。 groundingすべきだったかもだけど、学習データに入ってると思ってたんだよね。" userName="code_biologist" createdAt="2025-04-07T05:45:00" color="#ff5733">}}

{{<matomeQuote body="激しく同意！しかも evalで色々仕込みがあるから、しばらくは anekdotischな話ばっかりになりそう。モデルはリリースごとに良くなってる気はするけど、均等じゃないよね。vertical integrationとか groundingとかが進んで、モデルをコロコロ変えなくて良くなることを願うよ。" userName="lherron" createdAt="2025-04-06T19:52:09" color="">}}


{{< /details >}}
{{< details summary="もっとコメントを表示（3）">}}
{{<matomeQuote body="evalの裏話（ほとんど報道されないけど）は、裏工作が横行してるってこと。最近の USAMO 2025では、SOTAモデルがたった5%しか正解できなかったのに、IMOでは銀メダル/金メダル級だって主張してるんだぜ。ARC-AGIも、基本的なルールを真似して大量の synthetic examples作って trainすれば簡単に”解決”できる。" userName="InkCanon" createdAt="2025-04-06T20:07:45" color="#45d325">}}

{{<matomeQuote body="subjectivityを排除したいなら、数式を書けばいいじゃん。考えるべきは3つの質問：a)疑う余地なく AI開発は壁にぶつかった？ b)人類には AIを開発し続ける動機がある？ c)AIは今後も改善される？ boolean値として考えると、(a), (b), (c)には8通りの組み合わせがあって、一番面白いのは偽, 真, 真: “(not a) and b => c”。色んな変数を追加できるけど、やめとく。多くの人が示唆する (偽, 真, 偽)は、ただの恐怖と拒否だと思う。恐怖は当然だけど、拒否は意味ない。" userName="dsign" createdAt="2025-04-07T08:24:46" color="">}}

{{<matomeQuote body="それマジで的を射てるかも。訂正するわ。" userName="dsign" createdAt="2025-04-08T17:15:28" color="">}}

{{<matomeQuote body="めっちゃ同意だわ…この分野はまだマジで新しくて予測不可能だから、みんなノリとか勘とか、個人的なエピソードとかで動いてる感じ。<br>暗闇の中で手探りで懐中電灯をリバースエンジニアリングしようとしてるみたい。" userName="KolibriFly" createdAt="2025-04-07T07:07:22" color="#ff5c5c">}}

{{<matomeQuote body="＞AIで「より良い選択」をする能力を示す特徴って何だろう？ってめっちゃ気になる。<br>俺もそう思う。未来にタイムトラベルして見つけたら教えてくれるって約束するなら、俺も同じように約束するよ。" userName="throwanem" createdAt="2025-04-07T01:10:43" color="">}}

{{<matomeQuote body="良い指摘だけど、ちょっと当たり前なことでもあるよね。俺たちは全知全能の神じゃないし、結局すべての意見や判断は、自分自身の限られた経験に基づかざるを得ないんだから。" userName="nialv7" createdAt="2025-04-06T21:00:18" color="">}}

{{<matomeQuote body="良い指摘だね。コメント欄ってすごく個人的な話が多いよね。これがよくあることなのか、この話題特有のことなのかを示すデータってある？" userName="aunty_helen" createdAt="2025-04-06T23:26:40" color="">}}

{{<matomeQuote body="＞“これはメタコメントだけど、こういう投稿への反応を読んでると、この件に対する集団的な反応が(a)めちゃくちゃ多様で、(b)完全に個人的な話に基づいているってことがよくわかる。”<br>AIに対する意見が大きく異なるのは、単純にtoken usageの問題だと思う。何百万ものtokenを日常的に使っているなら、今いる革命的な地点を完全に理解できるはず。ちょっとチャットする程度じゃ、絶対にわからないよ。" userName="ramesh31" createdAt="2025-04-07T14:45:52" color="#ff5c5c">}}

{{<matomeQuote body="道具なんだから、使い方次第だし、得意なことと苦手なことがあるのは当然。<br>経験とかスキルとか、結果を評価する能力がない人がツールを使って、良い結果が出ないとツールのせいにするのもよくある話。<br>とは言え、LLMに対するhypeは明らかに能力を誇張してるよね。" userName="antonvs" createdAt="2025-04-07T15:07:28" color="#785bff">}}

{{<matomeQuote body="なるほど、これは個人的な意見（あなたは革命的だと信じているhigh-token userだと推測するけど）だけど、原則的には測定可能で反証可能な仮説なんだよね。<br>主要なLLM API providerが、LLMの利用額（またはtoken数）と将来の変革への楽観度を相関させる調査を見てみたい。<br>現在の有用性”との相関はトートロジーになるだろうけどね。<br>実はあなたとは逆の直感を持っていて、一番tokenを使っている人は、entity extractionとかclassificationとか、今得意なことのために使っていて、将来の可能性については無相関な立場なんじゃないかと思ってる。一応言っておくと、俺もそっちの陣営。" userName="lukev" createdAt="2025-04-07T21:49:06" color="#38d3d3">}}

{{<matomeQuote body="Token usageっていうのは、agentic processのことね。ここ数年のLLMに対する不満（hallucinationとか、リアルタイムデータの欠如とか）は、基本的にsingle shot promptingをモデルに直接行った結果だった。今はもう誰もそんなこと真面目にやってないよ。確かに、タスクにかかる費用は10倍になるし、時間もかかる。でも、最終的な結果は有意義で役に立つし、それに基づいてシステムを構築できるようになるんだ。" userName="ramesh31" createdAt="2025-04-08T01:13:48" color="#45d325">}}

{{<matomeQuote body="多くの人と違って、私は作者の不満に完全に同意するわ。<br>AI batch startupがsubscriptionを売り切って、外部の企業が確率モデルに賭けたくないから、市場の成長が止まったら、AI bubbleは2026年末か2027年までに崩壊すると思う。なぜなら、彼らはほとんど何も理解していないし、見たコンテンツの巧妙な模倣機にすぎないから。" userName="wg0" createdAt="2025-04-07T09:37:11" color="#785bff">}}

{{<matomeQuote body="マジでそれな。個人的にはWindsurf毎日使ってて、Sonnet 3.5が一番使いやすいんだよね。3.7は余計なことしてくるし、マジで勘弁って感じ。他のモデルでもそうだけど、3.7になってから特にひどくなった気がする。" userName="consumer451" createdAt="2025-04-07T15:42:06" color="#ff5733">}}

{{<matomeQuote body="わかるー、3.7で同じこと経験したわ。毎回じゃないけどね。助かることの方が多いけど、3.5の方が“なんか良い”って感じするんだよね。3.5には期待してなかったから、接し方が違ったのかも。みんなモデルの使い方違うだろうし、結果も変わるよね。個人の成功率とか生産性を測る方法が出てきたら面白いかも。誰がどのモデルと相性良いか分かれば、理由もわかるかもね。" userName="cootsnuck" createdAt="2025-04-07T16:00:06" color="#ff33a1">}}

{{<matomeQuote body="＞個人の成功率とか生産性を測る方法が出てきたら面白いかも。誰がどのモデルと相性良いか分かれば、理由もわかるかもね。“<br>マジそれな。ずっとそれ思ってた。CursorとかWindsurfみたいなツールは、モデルごとにシステムプロンプトが違うだろうから、ツールごとにテストする必要があるよね。Playwrightみたいなテストツール使えば簡単そうじゃん？最初に成功した人が勝ち組だね。" userName="consumer451" createdAt="2025-04-07T18:15:05" color="#ff33a1">}}

{{<matomeQuote body="3.7はまるで野生の馬みたい。ちゃんと指示しないと暴れちゃう。自動で分かってくれないのは残念だけど、乗りこなせばイケる。" userName="behnamoh" createdAt="2025-04-07T15:52:07" color="">}}

{{<matomeQuote body="これ使ってるよ：今のコードを大幅に変えたくないし、新しいファイルとか関数をいっぱい作るのもイヤ。ちゃんと考えて、タスクに集中して、暴走しないで！一番シンプルでエレガントで直感的で、マジで安定してるやつが良い。" userName="behnamoh" createdAt="2025-04-07T18:03:48" color="#ff5c5c">}}

{{<matomeQuote body="注意点として、最小限のコードで作ってって言ったら、めっちゃ重要な部分を飛ばされた。「最小限」って言ったからだってさ。Claude 3.7、マジありえない。" userName="pdimitar" createdAt="2025-04-08T11:39:34" color="">}}

{{<matomeQuote body="モデルの予測不能な挙動を回避するためにプロンプトを修正するのって、結局プラスになってるのか疑問に思わない？" userName="namaria" createdAt="2025-04-09T10:00:38" color="">}}

{{<matomeQuote body="LLMでコード書くメリットって、自然言語でやりたいことを説明するだけで済むってことだと思ってたんだけど。でも、プロンプト書くのが大変で、フォーラムで情報交換したり、予測不能な挙動に悩まされたりするなら、結局コード書いた方が早いんじゃないかって思う。" userName="namaria" createdAt="2025-04-09T11:16:48" color="#ff33a1">}}


{{< /details >}}


[記事一覧へ]({{% ref "/posts/" %}})
