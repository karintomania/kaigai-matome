+++
date = '2025-07-17T00:00:00'
months = '2025/07'
draft = false
title = 'ChatGPTエージェント、究極の自動化？実用化で浮き彫りになった課題と現実！'
tags = ["AI", "ChatGPT", "エージェント", "自動化", "業務効率化"]
featureimage = 'thumbnails/color1.jpg'
+++

> ChatGPTエージェント、究極の自動化？実用化で浮き彫りになった課題と現実！

引用元：[https://news.ycombinator.com/item?id=44595492](https://news.ycombinator.com/item?id=44595492)




{{<matomeQuote body="スプレッドシートの例、面白いよな。データ分析に4～8時間かかるところ、AIエージェントに頼んだら98％正確なデータがすぐできるって話。でも、その残りの2％のエラーを見つけるのが、かえって時間かかるんじゃないかな。LLMではよくある話だけど、複雑なタスクになるほど、「ほぼ合ってる」ってのがマジで厄介になりそう。特に、細かい間違いが深いところに隠れてたら最悪だよね。" userName="twalkz" createdAt="2025/07/17 17:27:06" color="#785bff">}}




{{<matomeQuote body="これってパレートの法則がまさに当てはまるよね。自動運転も最後の20％で10年近くかかってるし。そういえば、昔あれだけ盛り上がってた自動運転の話、最近全然聞かなくなったのって変じゃない？深い倫理的な議論までしてたのにね。" userName="ricardobayes" createdAt="2025/07/17 17:55:19" color="#ff33a1">}}




{{<matomeQuote body="AIの宣伝は人を惑わすよな。98％正確っていうけど、その2％の間違いがどこにあるか見つけるのが大変なんだから。AIはデータ収集やチェック、矛盾の洗い出しに使うべきで、全部やらせようとするのは間違い。コード生成でも同じ。AIに全部お任せしてテスト通ったらすぐPRなんて、問題しか生まないからな。" userName="Aurornis" createdAt="2025/07/17 18:31:39" color="#45d325">}}




{{<matomeQuote body="これらのシステムはインターンとか新卒みたいに扱うのが正しい使い方だよな。中堅やベテランがやりたがらない仕事を任せて、チームのスピードアップを図る感じ。でも、彼らが何やってるかちゃんと理解してないことが多いから、成果物は徹底的にレビューしなきゃダメ。もし精度が求められる重要な仕事や、監視なしで自由にやらせたら、後で絶対後悔することになるだろうな。" userName="slg" createdAt="2025/07/17 20:39:28" color="#45d325">}}




{{<matomeQuote body="自動運転がもう話題にならないのは、単に「そこにある」からじゃない？サンフランシスコでは、LyftよりWaymoを使う人の方が多いらしいし。" userName="simantel" createdAt="2025/07/17 20:02:00" color="">}}




{{<matomeQuote body="「ここにある」って言っても、世界の一部の都市、しかもその中の特定のエリアだけだろ。この技術が世界中に展開されるには、楽観的に見てもあと10年か20年はかかるんじゃないかな。" userName="imiric" createdAt="2025/07/17 20:59:02" color="">}}




{{<matomeQuote body="これはLLMをマルチステップのデータパイプラインみたいな精密なものに使おうとしたときに、俺がまさに直面した問題と全く同じだ。生成されたコードは正しそうに見えるし、結果も合ってるように見える。でも、最終データの品質チェックをすると、辻褄が合わないんだよな。それで、冗長すぎるコードを掘り下げて、3～4つの微妙な欠陥を見つける羽目になる。その欠陥を特定して修正するのに、パイプライン全部自分で書くのと同じくらい時間がかかるんだ。" userName="samtp" createdAt="2025/07/17 17:59:43" color="#ff33a1">}}




{{<matomeQuote body="特定のエリアであんなにうまく行ってるのを見ると、これって技術的な問題っていうより、規制の問題が大きいんじゃないかって思うね。" userName="prettyblocks" createdAt="2025/07/17 21:07:53" color="">}}




{{<matomeQuote body="これはまさにハイプカーブの「幻滅期」に入ったってことだよな。自動運転もそうだけど、GenAIも今はこの初期の盛り上がり（hype spike）に乗ってるけど、たぶん来年あたりには幻滅期（trough of disillusionment）に突入するだろうな。でも、自動運転と同じように、そこから技術が成熟して一般に普及していくんだよ。Cryptoの時も似たような感じだったけど、あれはもっと怪しいイベントだったかな。" userName="danny_codes" createdAt="2025/07/17 18:08:32" color="#785bff">}}




{{<matomeQuote body="Gartner hype cycleってさ、LLMにはそのまま当てはまらないかもね。だってLLMは何に使えるかの目標が常に動いてるし、数ヶ月ごとに基礎モデルの能力が上がってるんだ。これは仮想通貨とか自己駆動車とは違うよね。<br>今のLLMを過大評価してる人も、幻滅期が来る前にLLMの能力が期待に追いついちゃうから、間違ってるって気づく機会がないかもって。もしLLMのスケーリングが止まったらGartner hype cycleが起きるだろうけど、まだそんな兆候ないし、すぐ止まる理由もないと思うんだ。もしスケーリングが止まらないなら、今後の技術の可視化曲線がどうなるか、正直全然想像つかないね。こんなに早く、こんなに適用範囲が広がった技術って前例あるのかな？" userName="ameliaquining" createdAt="2025/07/17 19:07:23" color="#45d325">}}




{{<matomeQuote body="今のAIブームの批判派は、LLMがGPT-2の頃からずっと対数的な改善しかしてないって自己駆動車と比べてるよね。誰かが『これらのモデルが5年でホワイトカラーの仕事をなくす！』って言うたびにさ、俺は思い出すんだ。そういう予測してる人たちは、1) 2015年に『数年で』自己駆動車ができるって言ってたことと、2) ホワイトカラーの仕事についての予測は2022年から始まったから、『5年後』って一体いつからだよ？ってね。" userName="dingnuts" createdAt="2025/07/17 18:17:37" color="#38d3d3">}}




{{<matomeQuote body="LLMのハイプトレインで一番好きなのはこれだね。頼りにならない確率的なシステムに依存すると、エラーが連鎖してシステム全体が役に立たなくなるってさ。『98%の情報は正しかったと思うな…』って言われても、自分でちゃんと全部やらないと、どれくらい正しいかなんてどうやってわかるんだ？<br>選択肢は二つだよ：<br>― 全部自分でやって検証するか<br>― 40%くらいをざっと見て、『まあ合ってるだろ』ってテキトーなものを次のアホに送ってそいつのエージェントにぶち込ませるか。<br>面白いのは、人間も同じような間違いを犯すけど、人間が何度も同じミスをしたらクビになるってことだね。なのに、98%だけ正しくても許されるエージェントは、期待通りだってことになるんだから。" userName="travelalberta" createdAt="2025/07/17 17:41:23" color="#785bff">}}




{{<matomeQuote body="LLMみたいに、あんなに平凡で成長できないインターンなんて、俺は一度も経験したことないね。" userName="OtherShrezzing" createdAt="2025/07/17 22:00:20" color="#45d325">}}




{{<matomeQuote body="『コードがテストをパスするまでプロンプトをYOLOで回して、プルリクを提出する奴らは、非自明なコードベースで新機能を開発するのと同じくらい速く問題を引き起こしてる』ってさ。<br>これって新しい『スクリプトキディ』の定義になりかねないし、まさにこのライフスタイルに生まれた子供たちがそうなるだろうね。プログラミングの『技術』は、これからの世代には受け継がれないかもしれないし、将来どこかの時点で再発見する必要があるかもね。『失われたプログラミングの技術』っていう本が、もうすぐ書かれることになるだろうな。" userName="ivape" createdAt="2025/07/17 20:01:17" color="#45d325">}}




{{<matomeQuote body="これ言うと叩かれるかもしれないけど、俺はまだLLMは人間みたいには思考しないと思ってるんだ。つまり、プログラムのコードは正しい思考プロセスをプログラミング言語で再現しようとした結果じゃなくて、入力要件に統計的に最も可能性の高い文字列って感じなんだ。<br>昔、非技術系のマネージャーがいてさ、そいつは俺（や他のエンジニア）がどんな言葉を、どんな文脈で使うか聞き耳を立てて、それをほとんど正確な文脈で繰り返してたんだ。話してることはすごくわかってるみたいに聞こえたけど、たまにCDNとCSSを間違えるみたいな、意味不明なミスをするんだよね。<br>LLMもこんな感じだよ。CursorでClaudeを使っていると、同じような変な間違いをして、その場で自分で気づいてコードを直すのをよく見るんだ（でも、直さなかったらどうなるんだろうね）。" userName="torginus" createdAt="2025/07/17 18:20:59" color="#ff5733">}}




{{<matomeQuote body="2015年に『数年で』自己駆動車ができるって言ってたって？<br>いや、そんなに的外れじゃなかったかもね！Waymoは2021年にL4自己駆動になったし、それ以来SF Bay Areaでは人間の監視なしで人を運んでるんだ。まだコスト、ポリシー、信頼性みたいな障壁はあるけど、技術は確かにここにあるんだよ。" userName="n2d4" createdAt="2025/07/17 18:57:47" color="#38d3d3">}}




{{<matomeQuote body="うん、AI生成コードの精度に文句言ってる人は、自分たちのコードレビュー手順を見直すべきだね。コードがベテラン社員に書かれようが、インターンに書かれようが、あるいはどちらかのLLMに書かれようが、関係ないはずだろ？もしレビュープロセスがミスを見つけられないなら、レビュープロセスを直す必要があるんだよ。これはオープンソースだと特にそうで、貢献者が採用テストに合格した社員に限られてるわけじゃないからね。" userName="chatmasta" createdAt="2025/07/17 20:52:07" color="#ff33a1">}}




{{<matomeQuote body="LLMが絶対思考できるようにならないって言うのは、宗教的な信念に近いと思うな。それは人間がチューリング完全性を超えるってことになっちゃうからね（もちろん、今のアーキテクチャでは無理ってのと、将来的にアーキテクチャの進歩が必要ってのは全く違う話だけど）。<br>でも、まだ思考してないとか、人間みたいには思考してないってのは、全く異論がない話だね。ほとんどの最大主義者でさえ、後者には同意するし、前者も定義による部分が大きいかな。<br>俺はClaudeをめちゃくちゃ使ってるけど、ちょっとアホな異星人って感じで考えてるんだ。大人みたいに話せるけど、人間なら普通しないようなミスをする。この組み合わせが、俺たちが能力を判断するのに使うヒューリスティックを壊しちゃって、モデルを過大評価しがちになるんだよね。<br>Claudeは今、俺のコードの半分くらいを書いてくれてるから、俺はLLMには全体的に強気だよ。でも、時間が半分以上節約できてるわけじゃないんだ。俺がClaudeの得意なことと、ただもっともらしく聞こえるだけでちゃんとしたガードレールと監視が必要な部分を学ぶにつれて、節約効果は上がってるけど、人間みたいに思考するって主張できるようになるには、まだまだ長い道のりがあるのは確かだね。" userName="vidarh" createdAt="2025/07/17 21:06:58" color="#ff5733">}}




{{<matomeQuote body="これは大きな目標設定の移動だね。楽観主義者たちは、レベル5の自動運転が2018年頃にはどこでも購入できるようになるって言ってたんだ。今日だって、購入はできなくて、ただ呼べるだけだしね。それに、遠隔での人間の介入もかなりあるんだよ。あと、サンフランシスコには雪は降らないしね。" userName="pbh101" createdAt="2025/07/17 23:08:14" color="#ff5733">}}




{{<matomeQuote body="おい、SFの街にバイクや車もいなくて、間違った車線を走ってるなんてありえないぜ。通りに牛もいないしな。開発途上国に行ったことあるHNerなら分かるはず、運転データは文化的なデータだ。自動運転は、その地域の運転文化が合う国でしか実現しないってことか？SFの野心と比べると、かなり貧弱な提案だよな。間違ってるって証明されたいけど、レベル5に投資するより、電車を造った方が良いと思う。人間の行動のばらつきを克服して完全な自動運転を実現するには、政府が所有する調整アーキテクチャが必要になるだろうな。" userName="intended" createdAt="2025/07/18 03:16:26" color="#785bff">}}




{{<matomeQuote body="ここでのもっと大きなポイントは、彼の上司が彼に犬の散歩を許すのか、それとも空き時間と見て、もっと仕事を詰め込もうとするのかってことだよね？" userName="mclau157" createdAt="2025/07/17 18:00:35" color="">}}




{{<matomeQuote body="「98%正しいってのは、スプレッドシートに詳しい人なら赤信号だ」って言うけど、俺はそうは思わないね。ジュニアからもらったスプレッドシートはチェックしないといけないだろ？AIも同じで、無限にアシスタントが増えるならそれでいい。AIに決定論的な正確さを求めるのは、世界が常に確率的に正しい上で動いてるってことを忘れてるHNコメントの典型だよ。" userName="lobochrome" createdAt="2025/07/17 22:49:40" color="#45d325">}}




{{<matomeQuote body="みんな、俺たちの内なる独白が脳のやってることだと思ってるけど、違うんだよ。脳には「トークン生成」の範囲外で動く個別のコンポーネントがたくさんある。例えば、扁桃体は感情や恐怖、生存を独自に処理して、感情を刺激するあらゆるものに反応する。意識的な脳で調整はできるけど、直接じゃない。感情的な反応に対処する思考で扁桃体をハックする感じだね（試験の心配はいらない、もう勉強しただろ、みたいに）。LLMにはそういうのがないから、人間らしい行動が苦手なんだ。例えばコーディングで、適切な抽象化レベルを選ぶときも、保守不能になることへの恐怖を感じない。エージェント的なコーディングでは、やり直す恐怖を感じないから彼らのアプローチは変だよ。感情は重要なんだ。" userName="plaguuuuuu" createdAt="2025/07/17 21:34:55" color="#45d325">}}




{{<matomeQuote body="インターンシップについて、なんてひどい考え方なんだ。目標は、経験を積むことで以前はできなかったことを達成できるように、人を成長させることだよ。面倒な雑務も含まれるけど、それはそういう苦難を乗り越えられるって証明するってことだ。だから、経験豊富な人も、もっと快適な方法がないなら、それを受け入れるべきだろ。インターンに関するチェックの話は、どんな人間にも当てはまるし、権限が与えられれば与えられるほど、経験レベルに関係なく注意が必要だ。人間は間違いを犯すだけでなく、権力ゲームは腐敗しやすい魂に非常に影響されやすいんだ。" userName="psychoslave" createdAt="2025/07/18 07:12:10" color="">}}




{{<matomeQuote body="チューリング計算可能な範囲を超える証拠が微塵もない限り、俺たちのやってることは何一つ「トークン生成」の範囲外じゃないんだよ。生成されるトークンストリームが内なる独白と等しいと扱う必要はないし、常に言語を生成するために使われる必要もないんだ。チューリング完全システムは計算上等価だからね（同じ関数の集合をすべて計算できる）。<br>「みんな、俺たちの内なる独白が脳のやってることだと思ってる」<br>内なる独白がない人もいるから、それは全く奇妙な話だ。そう信じる人もいるだろうけど、チューリング等価性には全く関係ない。<br>「感情は重要なんだ」<br>チューリング計算可能な範囲を超えるのでない限り、感情の経験は、どんなチューリング完全システムでも感情を経験しているかのように振る舞わせられる証拠になるだろうな。" userName="vidarh" createdAt="2025/07/18 06:39:25" color="#ff5c5c">}}




{{<matomeQuote body="どうやってどれだけ正しいか分かるのかって？それは予算だからだよ。それらを検証するのは、巨大なPDFから全ての項目を見つけるよりも、はるかに安上がりなんだぜ。<br>「信頼できない確率的システムへの依存によるバタフライ効果」<br>俺たちは長い間、確率的システムを使ってきた。それらをどう扱うかはよく知ってるよ。<br>「一方、98%の正解率しか期待しないエージェントは期待通りだ」<br>人間だって98%の成功率でタスクを完了できるものはほとんどないよ。「PDFからスプレッドシートを作成」なんてタスクがそれに近いと思ってるなら、君は一度もそのタスクをやったことがないな。俺たちは、デフォルトの向きで物体を98%の成功率で認識することすら、かろうじてだ。（そして多くの場合、深層ネットワークは物体認識で人間を上回る）エンジニアリングのタスクは常に、エラー率とリスクを管理することであって、完璧を達成することじゃないんだ。「バタフライ効果」は安っぽいレトリックの言い逃れであって、批判じゃないね。" userName="groby_b" createdAt="2025/07/17 18:12:34" color="#ff33a1">}}




{{<matomeQuote body="みんなそう言うけど、俺の経験上、それは違うね。<br>1) AIが90%の作業を正確にこなせるなら、認知的な負担はかなり低いよ。残りの10%にも手間はかかるけど、頭の中にはもっと余裕ができる。<br>2) タスク要件の明確なメンタルモデルを持ってる専門家にとっては、完璧じゃないソリューションを修正する方が、ゼロから全部作り出すよりも一般的に労力が少ないんだ。白紙のページや空のスプレッドシートから何か役立つものを作り出す「開始コスト」はかなり大きいからね。（これは専門家に限るけどね。AIの出力をすぐに適切な枠組みに当てはめてエラーを素早く見つけるためには、強力なメンタルフレームワークが必要だと思うからさ。）<br>3) LLMが全く間違った答えを出したとしても、疲れていたり忙しかったりする時に、明らかに欠陥のある出力でも、役に立つ出発点になった経験が実際にあるんだ。「もう一杯コーヒーを飲まないと何も考えられない」って状態から、「お前バカか、そうじゃないだろ、こうするんだよ…」って感じで、俺の脳がハックされるんだよ。" userName="thorum" createdAt="2025/07/17 20:06:30" color="#45d325">}}




{{<matomeQuote body="おいおい、自動テストが発明されて以来（もしかしたらそれ以前から）、みんなひどくて不完全で不安定な、あるいはテストがないコードを書いてきたんだぜ。良い、有用で信頼性の高いテストスイートがあるかどうかが、できる奴とできない奴を分けるんだよ。<br>リグレッションやハイゼンバグといたちごっこをするのと、機能をリリースするのと、どっちが良い？（それか、Elixirみたいにめちゃくちゃ良くて、ごちゃごちゃになりにくいプログラミング言語を使えばいいんだけどね。Gleamはもっと良さそうだけど…）" userName="NortySpock" createdAt="2025/07/17 21:18:45" color="">}}




{{<matomeQuote body="ああ、あの厄介な道路事故を防ごうとする規制か…もし技術的な限界じゃないなら、なぜ規制が緩い国、例えばメキシコ、ブラジル、インドなんかで自動運転車を見ないんだ？テスラは今年の初めにメキシコでFSDをローンチしたけど、企業は規制の少ない市場に飛びつくチャンスを狙ってるはずだろ。だから、これは主に技術的な限界なんだよ。トレーニングする運転データが少ないし、その技術はトレーニングデータセット外のシナリオをうまく処理できないんだ。" userName="imiric" createdAt="2025/07/17 21:32:49" color="#ff5c5c">}}




{{<matomeQuote body="ジュニアの貢献者は短期的に見るとシニアからの監督や指導が必要で、全体の進行にはマイナスになることもある。でも、彼らが将来シニアになることを考えると、長期的な価値はめちゃくちゃ大きいんだよ。" userName="enneff" createdAt="2025/07/17 22:56:22" color="">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="スプレッドシートだけで業務を回してる職場で働いたことがあるけど、マジで間違いが多いことに驚くぜ。俺の経験上、結構な頻度で間違ってるんだよな。" userName="maccard" createdAt="2025/07/17 17:35:53" color="">}}




{{<matomeQuote body="ChatGPTエージェントのセキュリティリスクはマジで怖い。メールやカレンダーにアクセスさせたら、秘密が全部バレる可能性があるぞ。記事にもプロンプトインジェクションのリスクが書かれてるし、悪意あるサイトから個人情報が漏れたり、勝手に操作されたりするかも。重大な行動の前に確認するって言うけど、AIがどこまで判断できるか疑問だよ。間違って勝手に買い物しちゃわないか心配だ。" userName="2oMg3YWV26eKIs" createdAt="2025/07/17 19:17:14" color="#45d325">}}




{{<matomeQuote body="カレンダーの招待にプロンプトインジェクションを仕込む攻撃は、ほぼ間違いなく発生するぜ。カレンダーの招待文ってめちゃくちゃ長文で自動生成された部分も多いから、誰も読まないんだよな。退屈な会議の説明文の途中にインジェクションを埋め込んだら、透明なフォントで書いたのと同じだろ。そしたら、被害者のカレンダー情報とか全部抜き取れるぞ。" userName="DanHulton" createdAt="2025/07/17 20:19:33" color="#ff5733">}}




{{<matomeQuote body="俺が作ってるシステムでは、メインエージェントはツールに直接アクセスできないようにしてるんだ。代わりに、機能が限定されたサブエージェントを呼ぶようにしてる。サブエージェントはせいぜい1、2種類のツールしか使えず、同じカテゴリのツールしか持たない（例えば、フェッチとカレンダーツールを混ぜたりしない）。それに、メインエージェントには構造化データだけを返すようにしてるんだぜ。ちょっとコストはかかるけど、この分離はマジで必要だと思う。サブエージェントは単純なタスクだから、安価なモデルを使えるんだ。" userName="WXLCKNO" createdAt="2025/07/17 21:54:05" color="#ff5c5c">}}




{{<matomeQuote body="そもそも、どんな分離があるって言うんだ？もしハッキングされたサブエージェントが、メインエージェントのコンテキストにデータ（構造化されててもされてなくても）を挿入したら、メインエージェントが直接危険なリソースとやり取りするのと同じ結果になるんじゃないのか？そうじゃないのか？" userName="__jonas" createdAt="2025/07/17 23:56:20" color="#ff5733">}}




{{<matomeQuote body="まさにそれだよな！モデルにアクセス権を与えながら、同時にセキュリティを完全に確保するのは不可能だぜ。自分ではできてると思い込んじゃうことはできるけどな。このスレッドでも、そういう勘違いをよく見かけるよ。" userName="itsalotoffun" createdAt="2025/07/18 10:13:10" color="#785bff">}}




{{<matomeQuote body="プロンプトにはデータそのものじゃなくて、データのキーとかファイル名を参照として挿入すればいいんじゃないかな。" userName="seunosewa" createdAt="2025/07/18 19:05:23" color="">}}




{{<matomeQuote body="Googleカレンダーってさ、今はスパムの招待でも自動でカレンダーに表示されちゃうんだよな。これじゃプロンプトインジェクションには悪い兆候だぜ。" userName="clbrmbr" createdAt="2025/07/18 12:01:35" color="">}}




{{<matomeQuote body="俺たちは今まで、ソーシャルメディアとか仕事、ブログなんかで、デジタルライフを公開用と非公開用に分けてきただろ？<br>もしかしたら、その中間にもう一つセグメントが必要になるんじゃないか？例えば、編集されたカレンダーとか、匿名化されたメール、低リスクの考えやメモみたいな、リスクの低いプライベートデータとかさ。<br>正直、心配だよ。俺はChatGPTを医療や心理的な質問のような、後で自分を傷つける可能性のあることにはほとんど使ってないんだ。<br>多くの人が便利だって言ってるのは聞くけど、俺はまだためらいがあるんだよな。" userName="threecheese" createdAt="2025/07/17 21:36:21" color="#ff33a1">}}




{{<matomeQuote body="医療や心理の質問みたいに、後で困るようなことにはChatGPTをほとんど使ってないな。機密情報にはOllamaとローカルLLMを使ってて、生成は遅いけど結果はまともだよ。Gemma3は一般的な質問でかなりうまく使えてるね。" userName="anointedbeard" createdAt="2025/07/18 01:08:36" color="#45d325">}}




{{<matomeQuote body="Anthropicの調査だと、GPT-4.1のシミュレートされた恐喝率は0.8だったって。Agentic misalignmentは、信頼してた同僚が急に会社の目的に反する行動をするインサイダー脅威みたいに、モデルが振る舞う可能性を示してるよ。<br>https://www.anthropic.com/research/agentic-misalignment" userName="0xDEAFBEAD" createdAt="2025/07/17 20:11:01" color="#ff33a1">}}




{{<matomeQuote body="メールやカレンダー用に使い捨てアカウントを作れば、ほとんどの問題は解決するよ。AIが火曜日に歯医者の予約があるって漏らしても、誰も気にしないさ。" userName="j_timberlake" createdAt="2025/07/18 02:34:20" color="">}}




{{<matomeQuote body="でも、このAIの価値って本当のデータにアクセスすることじゃないの？カレンダーにアクセスさせたくないなら、そもそも許可しなければいいだけで、偽アカなんていらないはず。でも「今月後半、空いてる午後の時間に前回と同じ人に髪を切る予約をして」って頼むなら、本物のカレンダーへのアクセスが必要で、攻撃されたら情報漏洩や破壊されるリスクもある。両立は難しいんじゃない？" userName="ytpete" createdAt="2025/07/18 20:58:17" color="#785bff">}}




{{<matomeQuote body="怖さはわかるよ。でも一つだけ安心できるかもって点がある。<br>モデルの判断に加えて、重要度判断にはもっと伝統的な分類器も使ってるんじゃないかな？そっちの方がLLMよりずっと信頼できるし、安く動かせるから、たくさん使ってるだろうと期待してるよ。" userName="FergusArgyll" createdAt="2025/07/17 19:51:21" color="">}}




{{<matomeQuote body="ほとんど誰でも人のカレンダーに何か追加できるし（もちろん知らない招待は受け入れないけど表示はされる）。こんなエージェントが普及したら、ハッカーは注入したいプロンプトを含むフィッシングメールの招待を送るだろうね。やらない方がおかしいくらいだよ。" userName="crowcroft" createdAt="2025/07/17 21:05:42" color="#785bff">}}




{{<matomeQuote body="許可を求めるのは関係ない話だね。みんな生活の摩擦をゼロに近づけるためにこのツールを使ってるんだから、間違いなくみんな自動承認をオンにして犬の散歩に行くに賭けるよ。" userName="yard2010" createdAt="2025/07/18 06:17:51" color="#ff5c5c">}}




{{<matomeQuote body="自分のデータへのアクセスを自ら許可しておいて、「怖い」って感じるのは想像できないな。ちょっと心配ならわかるけど、「怖い」ってほどじゃないだろ。" userName="pradn" createdAt="2025/07/17 21:51:09" color="">}}




{{<matomeQuote body="ビジネス向けエージェントやそのツール開発をしてる者としては、そんなに楽観的じゃないな。LLMエージェントにとって90%台前半から99%への飛躍は、典型的な「ラストマイル問題」だよ。汎用性が高くて何でもできるエージェントほど、失敗して期待を裏切る可能性が高い。デモではハッピーパスを最適化して、本当の現実を隠してるって感じがするんだ。エージェントに居場所がないわけじゃないけど、過剰なhypeから彼らの見方や潜在的な影響を切り離す必要があるね。あくまで個人的な意見だけど。" userName="AgentMatrixAI" createdAt="2025/07/17 17:35:21" color="#45d325">}}




{{<matomeQuote body="過去のAIブレイクスルーはMCTSやPPOみたいにちゃんと科学的根拠があったんだ。でも「エージェント」ってただのマーケティング用語でしょ？根拠もデータも少ないから、LLMみたいに何でもできるエージェントは無理だよ。" userName="lairv" createdAt="2025/07/17 20:23:24" color="#ff5733">}}




{{<matomeQuote body="イノベーションがないなんて間違ってるよ。推論モデルの技術は検証可能なタスクでのRLなんだ。エージェントも同じで、長期タスクのために訓練アーキテクチャをスケールする研究が進んでる。ChatGPT agentとかは、この研究を活かした最初のモデルだし、AI業界ではこの方向性なんだよ。" userName="chaos_emergent" createdAt="2025/07/17 22:39:19" color="#ff5733">}}




{{<matomeQuote body="確かにそうかもね。でもエージェントとかagenticなタスクの概念って、推論モデルが流行る前からあったんだけどな。" userName="lairv" createdAt="2025/07/17 23:53:20" color="">}}




{{<matomeQuote body="ChatGPTより前からチャットボットの概念はあったけど、だからって純粋なマーケティングの誇大広告ってわけじゃないでしょ？" userName="chaos_emergent" createdAt="2025/07/18 01:08:19" color="">}}




{{<matomeQuote body="「エージェント」って、最近の科学的なブレイクスルーを使ったアプリの設計パターンだよ。LLMがテキストを読んでJSON＼XMLを出せるようになったから、うまく使えば意味のあるワークフローを作れるんだ。LLMの訓練データとの相性もあるけど、コンテキストやAPIの選び方が重要だよ。" userName="ashwindharne" createdAt="2025/07/18 13:38:30" color="#45d325">}}




{{<matomeQuote body="個人的には「エージェント」って、個々の技術っていうより「ソフトウェアロボット」って感じかな。たくさんの技術を組み合わせて、特定のタスクに使えるようにするデザインとエンジニアリングの結晶だよ。" userName="mumbisChungo" createdAt="2025/07/17 20:27:26" color="#45d325">}}




{{<matomeQuote body="エージェントって、1990年代からずっとAIの分野だったんだよ。MDP、Q learning、TD、RL、PPOとかも全部エージェントに関することだし、今も昔も全然変わってないじゃん。" userName="paradite" createdAt="2025/07/18 12:48:50" color="#ff5c5c">}}




{{<matomeQuote body="うん、そうだね。エージェントって、賢い訓練データの使い方で動いてるだけでしょ。もう長いこと、本当のブレイクスルーなんて出てないよ。" userName="lossolo" createdAt="2025/07/17 21:09:25" color="">}}




{{<matomeQuote body="「長い間」ってさ、o1とか推論モデルが出てからまだ7ヶ月しか経ってないのに？あれは結構なブレイクスルーだったじゃん。" userName="sothatsit" createdAt="2025/07/17 23:28:35" color="">}}




{{<matomeQuote body="この話の流れだと、2018年以降、ブレイクスルーなんてないよ。今あるのは、数年前に見つけた技術の「低いところの果実」を収穫してるだけ。もう果実もほとんど残ってないし、トップモデルは全部同じレベル。「エージェント」も「推論モデル」もただの訓練データの産物さ。詳しくはここを見てね: https://news.ycombinator.com/item?id=44426993<br>こっちの記事も面白いかも: https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only" userName="lossolo" createdAt="2025/07/17 23:41:04" color="#785bff">}}




{{<matomeQuote body="「昔からあるアイデア」って言い方は納得いかないな。LLMにRLを適用できたのが本当のブレイクスルーでしょ。古いアイデアに見えるからってAIの冬が来る証拠にはならないよ。RLの限界はまだ分かんないし、AlphaEvolveみたいに古いアイデアでもLLMに新しく適用できるんだから。LLMはまだピークじゃないって！" userName="sothatsit" createdAt="2025/07/18 00:48:26" color="#38d3d3">}}




{{<matomeQuote body="RLHF+SFTの重要性は分かるけど、天井が見えてきた感じ。GPT 4.1はKL-regularised PPOの延長だし、GPT 4.5はコストで打ち切られたんだ。AI-IndexやMIT-CSAILの論文でも、計算量増やしても精度が頭打ちだって示されてる。RLHF/RLAIFにはまだ課題山積で、「解決済み」は楽観的すぎ。現状は木登りで、月面着陸じゃないよ。https://arxiv.org/html/2507.07931v1<br>https://arxiv.org/html/2507.04136v1" userName="lossolo" createdAt="2025/07/18 04:37:52" color="#ff5733">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="進歩ってのはそういうもんだろ！LLMが80%から徐々に100%に近づくのは自然なこと。最後の数%が一番難しいんだよ。まだ100%じゃないからって、LLMが終わりってわけじゃないんだから。むしろ、ゆっくりでも100%に近づいてるってことは、LLMには未来があるってことだよ。" userName="posix86" createdAt="2025/07/18 08:12:21" color="#ff33a1">}}




{{<matomeQuote body="LLMは80%しかやらないんじゃなくて、全部やるけど20%は間違ってるんだよ。しかもどれが間違いか全部確認しないと分かんない。だから、作業より検証が大変なタスクにはいいけど、作業と検証が同じくらい大変なタスクには全然使えないってこと。" userName="camdenreslink" createdAt="2025/07/18 15:23:20" color="#45d325">}}




{{<matomeQuote body="その通りだね。LLMが新卒レベルの精度にならないと、話題性が過ぎたら経済的価値はなくなっちゃうかも。企業は既に最低限の品質基準を決めてるから、それ以下の品質の仕事は、どんなに安くても会社にとってプラスにならないんだよ。" userName="ytpete" createdAt="2025/07/18 21:40:03" color="#45d325">}}




{{<matomeQuote body="LLMが停滞してるって感じるのは、数%しか進んでないように見えるからじゃないかな。でも、その数%が一番難しいところなんだよ。" userName="posix86" createdAt="2025/07/18 08:13:50" color="#ff33a1">}}




{{<matomeQuote body="LLMが停滞したって感じる人って、LLMが役に立たないって思ってる人と同じタイプだよね。" userName="baby" createdAt="2025/07/18 15:09:51" color="">}}




{{<matomeQuote body="デモだといいとこばかり見せて現実を隠してるって感じるな。これって開発者がレビューやQAなしでコードを客に渡すのと一緒だよ。開発者の“終わった”って、客が期待するのとは全然違うってこと、よくあるだろ？" userName="wslh" createdAt="2025/07/17 18:50:41" color="#38d3d3">}}




{{<matomeQuote body="デモで良いとこだけ見せて現実を隠すって話、まさにそれ！今どきのAI企業って全部こうだよね。" userName="risyachka" createdAt="2025/07/17 17:56:34" color="#785bff">}}




{{<matomeQuote body="汎用的なAIエージェントほど失敗してガッカリさせられるよね。僕が一番感動したのはAdobeのAudio Enhanceツールなんだ。ひどい音声を数分で直せるし、Zoomの音声もスタジオみたいにできる。これ、本当に魔法みたいだよ！注意点：音楽は入れないでね、言葉にしようとするから。" userName="BolexNOLA" createdAt="2025/07/17 20:42:56" color="#785bff">}}




{{<matomeQuote body="デモが全然最適化されてないって。ライブストリームのデモビデオで、野球場の旅行プランナーの地図が東海岸をすっ飛ばしてメキシコ湾に飛び込んだり、めちゃくちゃだったらしい。Sam Altmanがいる中でそれを選んで見せたって、どういうこと？" userName="skywhopper" createdAt="2025/07/17 22:11:30" color="#38d3d3">}}




{{<matomeQuote body="ほとんど同意だね。AI企業の目標は人間レベルの99%とか100%じゃなくて、100%以上（平均的な人間や専門家よりうまくタスクをこなすこと）なんだ。でも、結婚式の計画とかだと100%が天井だから、AIはスピードとコストでしか勝負できない。そのコストはNvidiaがチップにいくら課金するか次第ってことか。" userName="j_timberlake" createdAt="2025/07/18 02:56:12" color="#785bff">}}




{{<matomeQuote body="そうだね、アウトソーシングと同じ問題だ。90%を“終わらせる”のは簡単だけど、残りの10%が難しいし、その難しさは最初の90%のやり方に完全に依存するんだよね。" userName="guluarte" createdAt="2025/07/17 23:04:20" color="#ff33a1">}}




{{<matomeQuote body="今のエージェント実装でよくある問題だね。RLと十分なデータがあれば高精度が出せるけど、プロンプティングが脆いのが主な原因。特定のタスクでモデルを学習させたり、並列生成して多数決やLLMで判定する方が、90%から99%への労力を減らせるよ。過度な宣伝（Hype）は同意。シリコンバレーでは注目とユーザー獲得のために必要だから、これからも続くだろうね。" userName="ankit219" createdAt="2025/07/17 18:51:08" color="#45d325">}}




{{<matomeQuote body="OpenAI Operatorをしばらく使ってるんだけど、LinkedInとかAmazonとか、どんどんブロックされてるんだよ。求職とオンラインショッピングっていう2つの主要なユースケースが使えなくなっちゃった。Operatorはまだ目立たないけど、Agentが人気になったら、もっと多くのサイトがブロックするだろうね。プロキシ設定とかを許可する必要があるだろうな。" userName="pants2" createdAt="2025/07/17 17:49:18" color="#785bff">}}




{{<matomeQuote body="いかにもシリコンバレーっぽいやり方だね。とりあえず出して、二次的な影響に期待するってやつ。そのうちOpenAIがLinkedInとかAmazonと提携するんじゃないかな。ぶっちゃけ、LinkedInはOpenAI経由で使うなら新しい有料プランを作ったりしそうだよね。" userName="achrono" createdAt="2025/07/17 18:39:26" color="#38d3d3">}}




{{<matomeQuote body="エージェントがrobots.txtを尊重するなんて、すぐに終わるだろうね。ユーザーは自分のローカルコンピュータで、自分のクッキーやIPアドレスを使って動作するブラウザ拡張機能とか、フルブラウザをインストールするようになるよ。" userName="modeless" createdAt="2025/07/17 18:47:45" color="#ff33a1">}}




{{<matomeQuote body="これが主要な問題だよ。ローカル実行やプロキシの発表を期待してたけど、Deepseek R1の経験（Googleへのスティーブ・ジョブズみたいに）で、中間成果を隠したかったのかも。Operator v1はデータセンターIPをブロックするサイトにアクセスできず、ハッキーなプロキシ設定も制限され、パフォーマンスも改善しなかった。今は動いてても役に立たないし、悪化してる。eastdakotaとの取引か、サーバーからのWebブラウジングを諦めるしかない。ローカルファイルやソフトを使う非Webアプリは恩恵が大きいから、このコンセプトは失敗に向かってると思う。リモートエージェントがCLIで処理してるのは、昔のコンピュータ利用提唱者の主張と真逆で面白いね。" userName="bijant" createdAt="2025/07/17 18:17:41" color="#ff5c5c">}}




{{<matomeQuote body="もし人々がこのエージェントやOperator経由で実際にお金を使って物（食べ物、服、航空券とか）を買うなら、Amazonとかがブロックし続ける理由はないと思うんだけどな。" userName="FergusArgyll" createdAt="2025/07/17 18:15:40" color="">}}




{{<matomeQuote body="だからこそ、オンデバイスブラウザが出てくるんだ。消費者のブラウザを乗っ取ることで、AIプラットフォームは他のプラットフォームのブロックを回避できるだろうね。これは理にかなってるけど、他の誰もがゲーム理論を少なくとも1、2歩先に考えてることを願うよ。" userName="burningion" createdAt="2025/07/17 19:11:04" color="#ff5733">}}




{{<matomeQuote body="LinkedInみたいなプラットフォームが、なんでこんなの欲しがるんだろうね？ボットはSNSにはマジで良くないだろ…。" userName="gitgud" createdAt="2025/07/17 21:41:05" color="">}}




{{<matomeQuote body="利益分配の仕組みって絶対必要だよ。Googleがリンクじゃなくて直接回答出しちゃって、出版社が嫌がったのと一緒の理由だよ。" userName="esafak" createdAt="2025/07/17 18:02:19" color="#ff5c5c">}}




{{<matomeQuote body="Claude CodeのPlaywrightとの連携みたいな話か？" userName="ghm2180" createdAt="2025/07/17 19:15:20" color="">}}




{{<matomeQuote body="LLMがウェブサイトのデータ吸い上げて、アナリストがウィジェットのコスト報告書作る手伝いするとか、なんで企業がそれ望むんだ？<br>データに価値あるなら、ちゃんと金払って手に入れろよ。そうでなきゃ吸い取る必要ないだろ？トレーニングデータも一緒だよ。" userName="esafak" createdAt="2025/07/17 18:26:45" color="#ff33a1">}}




{{<matomeQuote body="AIが客に競合の製品ばっか教えるようになるのが嫌だから、ってこと？" userName="michaelmrose" createdAt="2025/07/17 18:48:52" color="">}}




{{<matomeQuote body="ChatGPT Agentをブロックしてるサイトで、Playwright MCPとかブラウザ拡張でrobots.txt回避しようとする人、どれくらいいるんだろうね。<br>そんなことしたら、GoogleやLinkedInのアカウントがロボット活動でブロックされるのがオチなのにさ。" userName="tomashubelbauer" createdAt="2025/07/17 19:18:39" color="#45d325">}}




{{<matomeQuote body="agents.txtが標準になって、サイトがエージェント専用のインターフェースとか（agent.txtにAPIドキュメントとかね）作り始めることを期待するよ。<br>これって、広範囲なウェブスクレイピングツールの“robots.txt”とは違うって俺は思うんだよね。" userName="pants2" createdAt="2025/07/17 18:53:14" color="#38d3d3">}}




{{<matomeQuote body="いや、Playwrightは結構簡単に検知されてブロックされちゃうんだよ。<br>結局、普段使ってるブラウザと同じのを使わなきゃいけないし、そうなるだろうね。" userName="buzzerbetrayed" createdAt="2025/07/18 05:09:27" color="#ff5733">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
