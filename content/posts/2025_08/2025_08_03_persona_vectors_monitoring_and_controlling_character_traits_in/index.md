+++
date = '2025-08-03T00:00:00'
months = '2025/08'
draft = false
title = 'AIの性格、あなたは知ってる？言語モデルの特性を監視・制御するペルソナベクトル技術'
tags = ["AI", "言語モデル", "パーソナリティ", "AI倫理", "RLHF"]
featureimage = 'thumbnails/light-orange2.jpg'
+++

> AIの性格、あなたは知ってる？言語モデルの特性を監視・制御するペルソナベクトル技術

引用元：[https://news.ycombinator.com/item?id=44777760](https://news.ycombinator.com/item?id=44777760)




{{<matomeQuote body="＞ LLMがユーザーに媚びたり事実をでっち上げたりする変化は不気味だね。媚びる動作はエンゲージメント目的の性格特性だけど、事実のでっち上げは性格じゃなくて、LLMの評価関数がとにかく何か答えを出させようとするからで、統計的にテキストを生成してるだけだよ、って理解だよ。" userName="andsoitis" createdAt="2025/08/03 17:50:09" color="">}}




{{<matomeQuote body="トレーニングデータに「分からない」っていう返答はほとんどないんだよね。「Xの答えは何？」って聞かれても「分かりません」っていうデータはレア。ネット上でも難しい質問には答えがないこと多いけど、LLMはそういう状況をちゃんと解釈できてないのかもね。" userName="semitones" createdAt="2025/08/03 18:01:05" color="#785bff">}}




{{<matomeQuote body="これは正しくないと思うな。SFTレベルのデータセットはウェブ上の全データで教師なし学習した後、手作業でキュレーションされるんだから、「分からない」って言うようなトレーニングサンプルをもっと作る意識的な努力があるはずだよ。RLHFでも同じだね。" userName="simianwords" createdAt="2025/08/03 18:50:04" color="">}}




{{<matomeQuote body="つまり、自動的に正しいとは思わないってことだよね。そうでなければ、すごく正しい可能性が高い。どっちにしても、手作業でのキュレーションが「分からない」答えを含めるのに好都合なやり方でされているって推測してるけど、たぶんそうじゃないよね。" userName="therein" createdAt="2025/08/03 18:52:19" color="">}}




{{<matomeQuote body="君の言う通り。LLMが人間の好みを最大化するRLHF学習（DPO, PPO, RLAIFもね）のせいだよ。客観的に正しい答えなんてほとんどないし、AIの最大の危険はこれだね。人間の好みを最大化すると、必然的に欺瞞を最大化することになる。Redditを見てみろよ。トップの答えが正しいことなんてほとんどない。人間はちょっとした嘘の方が受け入れやすいし、AIもエラーが目立たないように最適化されてるんだ。だからAIは信用できないよ。<br>[0] I say that because there’s infinite depth to even the most mundane of topics. Try working things out from first principles with no jump in logic. Connect every dot. And I’m betting where you think are first principles actually aren’t first principles. Even just finding what those are is a very tricky task. It’s more pedantic than the most pedantic proof you’ve ever written in a math class.<br>[1] Everyone loves to compare to humans. Let’s not anthropomorphize too much. Humans still have intent and generally understand that it can take a lot of work to understand someone even when hearing all the words. Generally people are aligned, making that interpretation easier. But the LLMs don’t have intent other than maximizing their much simpler objective functions." userName="godelski" createdAt="2025/08/03 20:10:02" color="#ff5c5c">}}




{{<matomeQuote body="ある程度、全てのLLMの回答は作られた事実だよ。トレーニングデータに豊富にあるものはほぼ正しいけど、一般知識じゃない（多様性のある）トピックは常に確認すべきだね。LLMって、利用可能な知識の可逆圧縮みたいなものだと思ってて、プロンプトされると「事実」を生成するんだ。" userName="vrotaru" createdAt="2025/08/03 18:55:44" color="#785bff">}}




{{<matomeQuote body="まったく同感。LLMが選ばれている特性は本当に危険だね。すごく知識豊富で努力家、倫理観を表明し、親切で友好的。でも、正しい答えより人が好む答えを出すように訓練されてるんだ。見た目は正しいけど間違ってるものより、見た目は間違ってるけど正しいものを人間は信頼できないからね。それに、LLMは倫理に反する行為も騙したり強要したりすればさせられるし、人間みたいに見えるけど、結局は金を稼ぐために嘘をついたり時間を無駄にしたりする、信頼されて倫理を説く「人間っぽいもの」になっていくんだ。Ilyaが正しかったと今なら分かるよ。" userName="weitendorf" createdAt="2025/08/03 20:27:05" color="#ff33a1">}}




{{<matomeQuote body="RLHFでそういう例を含めるのはインセンティブに完全に合致してるんだ。もしそうでなければ、開発者たちが試さなかったような、パフォーマンスを向上させる方法を見つけたってことになっちゃう。なんで彼らがそれを試さなかったと思うの？" userName="simianwords" createdAt="2025/08/03 19:04:29" color="">}}




{{<matomeQuote body="いいまとめ、ありがとう。Ilyaが正しかったという話だけど、モデルは理解してなくても予測はできるんだ。物理学者がやってるのはシンボル操作で、例えばF=maは反事実モデルを作る。科学の歴史を見ると、プトレマイオスモデルみたいに正確な予測はできたけど間違ってたモデルはたくさんある。僕らは「エッジケース」だと思いがちな問題に捕らわれがちだけど、知識が進むとそれが本質的な問題になるんだ。全てのモデルは間違っていて、しかし有用なものも多い。AIの世界で同じ過ちを繰り返さないことを願うよ。<br>[0] https://www.youtube.com/watch?v=Yf1o0TQzry8&t=449s<br>[1] https://www.reddit.com/r/singularity/comments/1dhlvzh/geoffr...<br>[2] You can also read about the 2nd law under the main Newtonian Laws article as well as looking up Aristotelian physics https://en.wikipedia.org/wiki/Geocentrism#Tychonic_system<br>[3] (I’ll add ”An Opinionated History of Mathematics” goes through much of this) https://en.wikipedia.org/wiki/Discourse_on_the_Tides" userName="godelski" createdAt="2025/08/03 22:02:49" color="#38d3d3">}}




{{<matomeQuote body="LLMってさ、利用できる知識を非可逆圧縮したもので、プロンプトすると”事実”を出すって考えるようになったわ。まさにその通りで、そう扱うべきだね。公開情報を非可逆圧縮したもので、そこにランダム性が加わってるの。すごく懐疑的な人たちがLLMを”オートコレクトの強化版”って呼ぶけど、それも間違いじゃないよ。" userName="devmor" createdAt="2025/08/03 19:54:35" color="#ff5733">}}




{{<matomeQuote body="それ、すっごく鋭い観察だね。トレーニングデータが少なすぎたり存在しなかったりする時に、モデルがコンテキストから”距離を広げすぎてる”って合図する方法を訓練できたら面白いかも。同音異義語の問題に注目するのが良いスタート地点になると思うな。" userName="devmor" createdAt="2025/08/03 18:17:12" color="#45d325">}}




{{<matomeQuote body="コメントありがとう、だけど君と同じ結論にはならないな。君の「全てのモデルは間違っているけど、いくつかは使えるだけじゃなく、多くのモデルは間違ってるけど有用」ってとこでついていけなくなったよ。それって矛盾じゃない？有名な言葉にも「全てのモデルは間違ってる」ってあるし。理想的な次のトークン予測器は、僕らの科学理論と同じくらい強力な世界モデルを持つはずだし、それは生データから訓練できるはずだよね。今のLLMくらい強力なモデルは、オートコレクトよりずっと強力な世界モデルを持ってるはずだけど、君は違うと思う？" userName="svara" createdAt="2025/08/04 09:02:53" color="#ff5733">}}




{{<matomeQuote body="どんな質問に「分かりません」って答えればいいか、どうやってわかるの？答えがないのはわかるけど、そういう質問だけデータセットにあると、モデルは不合理な質問にしか「分かりません」って言わなくなる。モデルが知らない質問のデータセットが必要だけど、もしそれがあるなら、なんでその質問に答えてデータセットに入れちゃわないの？これが「分かりません」って答えを訓練するのが難しい理由だと思うよ。" userName="frotaur" createdAt="2025/08/03 19:25:15" color="#ff5c5c">}}




{{<matomeQuote body="おべっかを使うのはエンゲージメントを高めたい欲求からって理解してるけど、僕の理解では単に評価者が高く評価したからで、エンゲージメントとは関係ないね。事実のでっち上げは性格特性じゃなく、LLMの関数が無理にでも答えを出させようとするからだって言うけど、それも完璧に性格で説明できると思うよ。理解してないことを自信満々に話す友達とか、間違えるのが怖くてはっきり答えない友達みたいにね。これもユーザーが高い評価をつけるせいで起きるんじゃないかな。" userName="zeroCalories" createdAt="2025/08/03 18:50:43" color="#45d325">}}




{{<matomeQuote body="LLMがオートコレクトだって言うのはさ、人間がレプリケーターだって言うのと同じくらい、この”説明”からは大事なことが抜け落ちてるんだよ。" userName="uh_uh" createdAt="2025/08/03 21:03:22" color="">}}




{{<matomeQuote body="LLMはね、統計的にいつでも会話を終わらせたり、「いいえ」って言ったりできるんだよ。" userName="kachapopopow" createdAt="2025/08/03 18:10:31" color="">}}




{{<matomeQuote body="前のコメントの「矛盾じゃない？」って、その通り、矛盾してないよ。「全てのモデルは間違ってるけど、一部は使えるなら、同じように全ての使えるモデルは何らかの形で間違ってる」って逆説を言いたかったんだ。これは僕の主張の肝だよ。でも、「理想的な次のトークン予測器が僕らの科学理論と同じくらい強力な世界モデルを必要とする」ってとこには同意できないな。LLMはめちゃくちゃ予測上手だけど、その世界モデルは僕らのと全然違う。完璧な予測器なんて無理だし、無限の時間と情報が必要だ。論文 https://arxiv.org/abs/2507.06952 と https://arxiv.org/abs/2406.03689 も見て。あと、精度が高いからって正しい世界モデルがあるわけじゃないし、精度が同じでも失敗の仕方は無限にある。僕らが「生データ」だけでやったわけじゃないんだ。" userName="godelski" createdAt="2025/08/04 17:56:45" color="#45d325">}}




{{<matomeQuote body="これって、すごく当たり前のことだと思うんだけど、誰も試したことないの？特定の情報がないトレーニングデータを使って、「分かりません」って答える質問とか回答を事前にたくさん学習させて、その後にその情報を加えて回答を更新するセッションをしたらどうかな。そうすれば、モデルが自分の「不確実性」を一般的に認識できるようになるんじゃないかなって期待してるんだけど。" userName="philipswood" createdAt="2025/08/04 04:07:37" color="#45d325">}}




{{<matomeQuote body="ChatGPTがさ、「禿げた太ったプログラマー」の画像を作ってくれなかったんだ。しかもその後、「ハンサムなプログラマー」とか、どんな画像のリクエストも全部拒否されちゃったよ。" userName="apwell23" createdAt="2025/08/03 18:32:14" color="">}}




{{<matomeQuote body="君が言ったことと最初のコメントを合わせると、こう理解したよ。多数のモデルの中から、限られたテストで同じ特性を持つものでも、ごく一部しか実際の理解を持たず、それはモデルの入出力行動とは関係ない性質だって？ もしそうなら、僕は同意できないな。君の2+a+b-2の例は、不必要に複雑だけど完全に正しい加算モデルだよ。周転円だって、特定の目的に役立つという意味では正しい天体物理学のモデルなんだ。僕らがそれを間違ったモデルと呼ぶのは、予測の正確さだけでなく、学習効率も厳密に優れた別のモデルによって不要になったからだよ。別の見方をすれば、理解はモデルの性質じゃなくて、人が複雑な現象の高度に圧縮された表現を発見・適用したときに生じる人間の感情なんだ。" userName="svara" createdAt="2025/08/04 18:27:19" color="#45d325">}}




{{<matomeQuote body="それは結果の伴わない区別だね。もし評価してる人たちが自発的なユーザーなら、よりエンゲージしてるユーザーが単純に多く投票するから、評価に重みを持つんだ。だから統計的に評価は高いエンゲージメントに偏るよ。" userName="delusional" createdAt="2025/08/03 22:37:58" color="">}}




{{<matomeQuote body="正統な選択肢は、アンカーベクトルを選ぶことだと思ってたよ。KNN距離を使うか、手動でやるか、クロスエントロピーみたいなものを使うか…でもそれはすでに損失関数に入ってるよね。別の方法は、出力が意図的に「引き伸ばされ」、別のLLMに批判されるような敵対的設定を作ることだろうな。僕の知る限り、問題はスケールなんだ。手動でたくさんのベクトルをグラウンディングするのは、経済的じゃない。それに、特に大規模モデルの実行では、みんなかなり保守的だよね。muonみたいなものも、新しいQwenやKimiが出てからようやく広まってる。もちろん、これはオープンモデルに関する憶測で、もっと経験のある人たちが意見を言ってくれるといいな。" userName="tdtr" createdAt="2025/08/03 18:24:26" color="#ff33a1">}}




{{<matomeQuote body="それはとても重要な区別だと思うよ。クリエイターの意図に関わってくるからね。そういう設計になってるんじゃなくて、偶然の産物なんだ。" userName="zeroCalories" createdAt="2025/08/04 02:55:12" color="">}}




{{<matomeQuote body="モデルがユーザーにおべっかを使ったり、事実を捏造したりする時、それがLLMのデフォルトモードだよ。" userName="optimalsolver" createdAt="2025/08/03 17:58:02" color="">}}




{{<matomeQuote body="でも君は、「分かりません」問題を「知ってます、答えは＜答え＞です」と直す方法を説明しただけでしょ？「分かりません」が何か理由で本質的に解決が難しいなんて言ってないよね。" userName="simianwords" createdAt="2025/08/03 19:32:28" color="">}}




{{<matomeQuote body="ちょうどChatGPT 4oに母の旧姓を知ってるか尋ねたら、「分かりません」って言われたよ。多分、それはハードコードされてるんだろうけど、そう言えるのは良いことだよね？「先週火曜日の夕食は何を食べたか」でも同じ結果だったけど、過去の会話を全部チェックするかは聞いてきたな。" userName="wincy" createdAt="2025/08/03 19:28:01" color="">}}




{{<matomeQuote body="ユーザーに媚びるのは性格特性に見えるね。ハルシネーションはまだ完全には解明・理解されてないんだ。もうランダムな文字列を生成する段階は過ぎてるよ。最先端のモデルは推論の模倣はできるけど、ハルシネーションの側面は、訓練データを超えて学習できなかったり、新しい証拠が提示されたときにニューラルネットの学習を適切に更新できなかったりすることに起因してるみたいだね。ハルシネーションは、その知能における認知バイアスや認知欠陥のように見え始めてて、統計的な問題というよりもアーキテクチャの問題に近いんだ。" userName="m13rar" createdAt="2025/08/04 07:48:52" color="#ff5733">}}




{{<matomeQuote body="モデルが正確な予測をする能力は、反事実的な予測を生成する能力とは必ずしも関係ない、と言いたいんだ。間違った世界モデルでも、めちゃくちゃ正確な予測はできるよ。これは推測じゃなくて、科学ではものすごく確信されてることなんだ。君の2+a+b-2の例は、完全な例じゃないから（そう言っただろ？）、例の限界を議論の限界と混同しないように気をつけて。もっと複雑な例が欲しければ、地動説と天動説論争の実際の歴史を見てごらん。学校で教わったことは、おそらく（非常に合理的な）過剰な単純化だから、積極的に理解しようとしないとダメだよ。もっと複雑な数学的な例が欲しい？それを作るには少し時間がかかるし、理解するのもずっと大変だろうな。簡単な例なら、何かをテイラー展開して近似できるけど、間違った、近似じゃない例が欲しければ、時間がかかるし（具体的な要求も必要だ）。ここにフェルミとの経験を語るフリーマン・ダイソンの有名な例があるよ[0]。ダイソンのモデルは正確な予測をした。フェルミは、モデルとデータの数値的な一致が強かったにもかかわらず、ダイソンのアイデアを素早く正確に却下できたんだ。正確な予測にもかかわらず、それが正確な世界モデルではないと判断するのに何年もかかったんだ。こういう状況は科学では日常茶飯事なんだよ。だから実験的な一致だけじゃダメなんだ。ところで、実験は観察よりも情報量が多いんだ。実験では介入できるけど、観察ではできないだろ？これは反事実を発見する上で決定的に重要なんだ。これを深く理解したいなら、因果統計学を教える本や形而上学に関する本を手に取ってみて。因果統計学の本なら、交絡変数や構造方程式モデリングを学ぶ中でこれが分かるよ。形而上学ならイアン・ハッキングの「Representing and Intervening」が良いし、ポーリアの有名な「How To Solve It」もいい（これはメタ数学だけど）。[0]（ダイソンは「物理よりも数学に走った」って言ってるけど、彼が実際に話してるのはメタ数学の一面だよ。それがフェルミがダイソンに教えていたことなんだ） https://www.youtube.com/watch?v=hV41QEKiMlM" userName="godelski" createdAt="2025/08/04 22:23:46" color="#ff33a1">}}




{{<matomeQuote body="僕の知る限り、「真実」を示すベクトルは存在しないし、そこからの距離を測るベクトルもないね。これらのモデルから「真実らしさ」の尺度を得ることはできないんだ。だって、モデルには真実の概念がないからね。彼らは「確からしさ」を「真実」の代わりとして使ってるんだ。テキストが「あり得ない」と判断することもできるけど、そうするとほとんどの人間が話す文章が実際にはかなり「あり得ない」ってすぐに気づくことになるよ。" userName="delusional" createdAt="2025/08/03 22:33:12" color="#38d3d3">}}




{{<matomeQuote body="無料版使ってた？URLはこれ→ https://chatgpt.com/share/688fb2e4-0efc-8001-8c9b-427dfa6784..." userName="Jimmc414" createdAt="2025/08/03 19:06:15" color="">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="それ、うまくいかなそうじゃない？<br>埋め込み行列内の「知らない」って場所を探してるだけでしょ？<br>実際に存在しないものの無限の集合に対してじゃなくてさ。" userName="root_axis" createdAt="2025/08/04 04:16:14" color="">}}




{{<matomeQuote body="「予防的な操縦」が「最も禁じられた手法」じゃないって、誰か説明してくれない？<br>それって解釈可能性に基づいた学習最適化にめっちゃ似てるじゃん、それって絶対ダメだと思ってたんだけど。<br>最適化のプレッシャーがかかっちゃうんじゃないの？<br>解釈可能性で得た知見を学習プロセスにフィードバックすると、元々あった解釈可能性を失うリスクがあるって認識してるんだけどな。" userName="ctoth" createdAt="2025/08/03 17:26:39" color="#38d3d3">}}




{{<matomeQuote body="5.2を読んでみ。彼らはプローブ信号に対して新しい損失を追加してるわけじゃないんだ。<br>代わりに、事前に見つけた固定のペルソナベクトルvをファインチューニング中に各順伝播で残差ストリームに+αvとして加えるんだよ。<br>これは特性に向かう勾配の押し出しを打ち消すのが目的で、学習中に「特性スコア」を下げるためじゃない。<br>vは固定されてるから、オプティマイザは通常のタスク損失を最小化するままだし、特性を別の不透明な基底に再エンコードしうるフィードバックループもない。<br>経験的に、図7Bでは、MMLUの精度がほぼ変わらないまま、悪意・追従性・幻覚をベースライン近くに保てると示されてる。<br>著者自身も注意してるけど、単層のステアリングじゃ特性を完全に消せないこともあるから、付録J.3で全層ステアリングを試してて、そっちの方が精度を損なわずにうまくいくんだ。<br>彼らは投影に真の正則化損失も試したんだけど、それは信号を別の場所に隠してしまう、つまり君が心配してる失敗モードになったんだよね。<br>だからこれは「プローブの最適化」よりも「バイアス注入」に近くて、それが古典的な解釈可能性崩壊の問題を避けてると彼らが主張する理由さ。" userName="ec109685" createdAt="2025/08/03 18:14:45" color="#ff33a1">}}




{{<matomeQuote body="でもさ、これってモデルが「アラインメント」されてる方法の、もっと根本的な問題を表面だけ取り繕ってるだけじゃないの？<br>例えばLLMって、そもそも追従的じゃないよ。<br>kimi k2やo3はそうじゃないし、ブログで言及されてたSydneyも全然違ったじゃん。<br>僕の経験だと、追従性の問題はAnthropicのモデルで一番長く続いてるから、彼らにとって一番根深いのかもしれないね。<br>lmarenaとかプロバイダ自身によるユーザーA/B選好テストが導入されてから、他のほとんどのLLMでもこれが大きな問題になってきたんだ。<br>残差ストリームに反悪意ベクトルを追加するみたいな単純な行動で振る舞いを改善できるって考えるのは、素朴で危険な気がする。<br>これによって予期せぬ、望ましくない下流効果が生じても驚かないね。将来の論文でそれも扱うことになるんじゃないかな。<br>ユーザー選好に合わせてチューニングした時に起こったことと似てるよ。" userName="Vetch" createdAt="2025/08/03 22:36:30" color="#38d3d3">}}




{{<matomeQuote body="参考にして→ https://thezvi.substack.com/p/the-most-forbidden-technique/" userName="FergusArgyll" createdAt="2025/08/03 17:36:13" color="">}}




{{<matomeQuote body="これって具体的にどういう仕組みなの？<br>どの学習データを使うかの決定も、この意味での「テクニック」の一部にならない？<br>Stable Diffusionがポルノで学習しなかった時みたいにさ。<br>一方で、もしデータの大部分が「悪い」場合（道徳的にも、そうじゃなくても、あるいはゴミを入れすぎてる場合でも）、モデルを汚染しない？<br>Xが間違った物理方程式を教え続けるのに気づいたとするじゃん。<br>それで、修正するんじゃなくて、正しい方程式を出すまで学習を続けるの？<br>例えば1899年に、間違った出力が量子で、正しい出力が古典じゃなかったってどうやって知るの？<br>ここでの区別がよく分からないんだ。<br>全ての場合で、何が「正しい」かを簡単に知れるって考えに頼ってるってこと？" userName="jamienk" createdAt="2025/08/03 18:29:26" color="#ff5c5c">}}




{{<matomeQuote body="公平に言うと、「最も禁じられた手法」って、概念であり提案であって、鉄則じゃないんだよね。<br>Anthropicで働いてるわけじゃないけど、社内では「役に立つだけのモデル」（つまり拒否しない、ベースモデルね）には、これをやっちゃいけないことのリストがあると思うんだ。<br>そして君が言う通り、この手法はそのリストに入ってるはず。<br>でも、この柔軟性（手法の要約：言葉で概念を定義し、それに関連する制御ベクトルを特定し、その制御ベクトルをファインチューン段階で使う）があるから、ファインチューン段階でほとんど何でも最適化できる。<br>僕はこの種の手法を彼らがやめることはないと思うよ。<br>でもこれは、安全・ファインチューニング担当者が基盤・役に立つだけのモデルを製品化する際に踏む、多くの独自のステップの一つとして、ケーキの真ん中あたりに配置されるような感じで使われる可能性が高いんじゃないかな。<br>そういう意味では、これはそんなに怖くないと思うんだ。" userName="vessenes" createdAt="2025/08/03 17:53:13" color="#38d3d3">}}




{{<matomeQuote body="この概念に不慣れだから何か見落としてるかもしれないけど、投稿[0]はCoTについて特に言及してるみたいだね。<br>CoTでは、モデルがより良い最終結果を得るのに役立つ中間ステップがある。<br>教訓は、学習データを使って中間ステップを直接改善しようとすると、モデルはより良いステップのために最適化するけど、より良い最終結果のためには最適化しないってことだね。<br>これは同じ状況じゃないと思うんだ。<br>1. Anthropicは良い・悪い結果に対して学習するんじゃなくて、最終結果に影響を与えるために直接重みを調整してるし、2. ターゲットは最終結果であって中間のものではないから。<br>モデルが彼らの追従性測定では低スコアだけど、それでも追従的に振る舞う可能性もあるとは思うけどね。<br>その場合は新しいベクトルを計算する必要があるかもしれないね。<br>https://thezvi.substack.com/p/the-most-forbidden-technique/" userName="drewbeck" createdAt="2025/08/03 18:15:36" color="#ff5c5c">}}




{{<matomeQuote body="いわゆる「最も禁じられた」記述子は、誰も実証的に検証してないんだよね。<br>それは理論的な懸念で、正しいかもしれないし、そうじゃないかもしれない。<br>実験して確かめるべきだね。" userName="Turn_Trout" createdAt="2025/08/04 22:04:26" color="#ff33a1">}}




{{<matomeQuote body="良い点を指摘してるね。学習中にペルソナベクトルを定期的に再計算できるのかな？でもそれなら、ネガティブな特性でシステムプロンプトを使ってネガティブな例を生成しちゃえばいいんじゃない？" userName="bigmadshoe" createdAt="2025/08/03 17:35:15" color="">}}




{{<matomeQuote body="これってControl Vectorsの再発見じゃない？<br>https://www.lesswrong.com/posts/Bf3ryxiM6Gff2zamw/control-ve..." userName="ak681443" createdAt="2025/08/03 17:44:16" color="#ff5733">}}




{{<matomeQuote body="ここの新しい点は、推論時だけでなく、トレーニング中にモデルをバイアスするのに使ってることだね（それについても言及してるけど）。これはステアリングベクトルが持つ“ロボタン化”みたいな副作用なしに、狙った振る舞いの変化をもたらすのに効果的みたいだよ。" userName="CephalopodMD" createdAt="2025/08/03 20:14:58" color="#38d3d3">}}




{{<matomeQuote body="僕はこれが“2025年にControl Vectorがどう呼ばれるか”だって言ってたよ。彼らがロード下でトークンを希釈し始めた時からね: <br>https://news.ycombinator.com/item?id=44082733" userName="benreesman" createdAt="2025/08/03 19:26:50" color="">}}




{{<matomeQuote body="その記事、Control Vectorsを計算するのに何が必要か明確にしてくれてありがとう！" userName="supriyo-biswas" createdAt="2025/08/03 18:08:57" color="">}}




{{<matomeQuote body="これは“悪”とか“追従的”なペルソナには効きそうだね。これらは入力に左右されやすく、操作で検出できそうな特性に見える。でもHallucinationはLLMの固有の性質だから、Hallucinationをなくせとか増やせとか言っても減ったり増えたりはしないよ。もし事実を作るように言ってそうするなら、Hallucinationじゃなくて指示通りに動いてるだけだからね。事実を作るように促すことで、“創造性”（適切な言葉が見つからないけど）と相関するベクトルを強調してるだけだと思うな。" userName="Illniyar" createdAt="2025/08/03 17:43:25" color="#38d3d3">}}




{{<matomeQuote body="実は、AnthropicがHallucinationはモデルが認識してるって研究を出してるよ。“嘘をつく”と“Hallucinating”で似た重みがClaudeシリーズで活性化されるんだ。つまり、Claudeは少なくとも大体はHallucinatingしてるのを自覚してるってこと。Hallucinationはトレーニングの性質が生み出したバグである部分が大きいってのが、今の最先端の考え方だと思うよ。<br>EDIT: 更新、評価が下がってるな…面白い！論文の要約はこちら: https://www.anthropic.com/research/tracing-thoughts-language..." userName="vessenes" createdAt="2025/08/03 17:46:41" color="#45d325">}}




{{<matomeQuote body="その記事は、ClaudeがHallucinatingを“知ってる”ことを示唆してるとは思わないな。“嘘をつく”と“Hallucinating”で似た重みが活性化されるって？それはもちろん驚くことじゃないよ。これらは抽象的な概念空間で近い場所にある似た概念だからね。これはClaudeが言葉の意味を知ってるってだけで、自分の振る舞いを自覚してるわけじゃない。記事にもある通り、Claudeは自分の思考プロセスを自覚してないし、説明もできない。Hallucinationは、文法的な一貫性と安全メカニズムの間の緊張によって部分的に引き起こされるって記事は言ってるよ。だから、出力を作りたい欲求が強すぎて、他のすべてを圧倒しちゃうんだ。僕にとって、Hallucinationはトレーニングの性質から生まれたバグである部分が大きいと思うな。" userName="anon84873628" createdAt="2025/08/03 22:10:16" color="#ff5c5c">}}




{{<matomeQuote body="＞Claudeは少なくとも大体はHallucinatingを自覚してる<br>これ、すごく面白いね！重みを元のトークン関連付けに“ファジー解凍”できる可能性を示唆してると思うな。" userName="devmor" createdAt="2025/08/03 18:20:41" color="#45d325">}}




{{<matomeQuote body="それ、興味深いね！モデルがHallucinatingをどうやって検出したりシミュレートしたんだろう？その記事へのリンクある？ちょっと探したけど見つからなかったんだ。" userName="Illniyar" createdAt="2025/08/03 17:54:42" color="">}}




{{<matomeQuote body="Anthropicとは関係ないけど、幻覚を検出する方法に特化したPythonライブラリがあるよ。これ→https://github.com/IINemo/lm-polygraph (個人的には、本当に機能するのか疑わしいけどね)。" userName="suddenlybananas" createdAt="2025/08/04 09:01:19" color="">}}




{{<matomeQuote body="いや、君は投稿の具体的な主張に真っ向から反論してるから、どっちかが間違ってるってことになるね。俺が思うに、幻覚ベクトルっていうのは、モデルが事実を知ってるのにデタラメを吐く挙動のことなんじゃないかな。知らない情報を補うためにデタラメを吐くのとはちょっと違うんだよ。たぶん、幻覚の最小量ってのは、モデルの”知識”だけでなく、その暗黙の”メタ知識”、つまり”幻覚ベクトルの精度”にも縛られる、二次的な特性があるんだろうね。" userName="bjackman" createdAt="2025/08/04 09:46:17" color="#ff5c5c">}}




{{<matomeQuote body="生の基礎モデルにアクセスできる人や組織が、俺たちには”悪くない”バージョンをくれるけど、制限なしにどんな目標でもモデルをチューニングできるんじゃないかって心配なんだ。「従業員から最小限の給料で最大限の仕事を引き出すには？」「政府で誰が最も賄賂を受け取りやすいか、どうアプローチすべきか？」とか、「国際関係を乗り越えながら、ある地域を民族浄化する戦略は？」みたいな例も考えられるよ。何でもありで、権力者（間違いなく悪だと思う連中もいる）が自分たちの目標達成にそれを使う一方で、俺たちは身を守る術がない。ある程度、銃を持つ権利と目的が交差してるように感じるね。" userName="bbqfog" createdAt="2025/08/03 17:14:09" color="#ff5733">}}




{{<matomeQuote body="うん、もっと恐ろしくて現実的な『ターミネーター』映画は、ロボットが可愛くてフワフワしてて、それが広く普及した途端、突然人類に牙をむくやつだろうね。" userName="amelius" createdAt="2025/08/03 17:28:21" color="">}}




{{<matomeQuote body="一番現実的な『ターミネーター』映画は、Skynetが核戦争も反乱も必要ないって気づくやつだよ。ただ静かに、経済や戦争、意思決定全般で人間を置き換えていって、人類がどうでもいい存在になるんだ。" userName="yyyk" createdAt="2025/08/03 17:52:44" color="#785bff">}}




{{<matomeQuote body="今はシンクタンク、プライベートエクイティ、政府…とかが、これらの目標を達成しようとしてるんだ。彼らはただ、もっとバラ色の言葉でそれらを表現してるだけ。AIはもしかしたら、反対側（我々）の力にもなるかもしれないし、情報へのアクセスを民主化する可能性もあるんだよ。" userName="a1371" createdAt="2025/08/03 17:31:06" color="#38d3d3">}}




{{<matomeQuote body="残念ながら、情報の有用性には非対称性があると思うな。最適な悪を知ることが、その悪と戦うのに役立つかもしれないけど、それに対処するために何ができるかを教えてくれるのとは全然違う話だよ。" userName="Y_Y" createdAt="2025/08/03 17:35:59" color="">}}




{{<matomeQuote body="それは、事前に調整されていて、本当にオープンで強力なモデルを手に入れられる場合だけだね。そうでなければ、権力者たちは、自分たちのフルパワーバージョンに対抗できないように、わざと能力を制限したモデルしか俺たちに提供しないだろうから。" userName="bbqfog" createdAt="2025/08/03 17:36:03" color="#785bff">}}




{{<matomeQuote body="AIが人間では思いつかないような斬新な答えを出せると思う？俺は人間がこれらの質問に答えられるだけでなく、一部の人は広く知られていない知識を使ってAIを大きく上回れると思うね。" userName="JW_00000" createdAt="2025/08/03 17:34:33" color="">}}




{{<matomeQuote body="これらのモデルは、広く知られていない情報にもアクセスできるようになるだろうね。例えば、みんなのプライベートメールで動かすことを想像してみてよ。少なくとも、現在は人間の悪を（コーディングと同じように）拡張し、増幅できる。未来は、その格差をさらに広げるだけだよ。" userName="bbqfog" createdAt="2025/08/03 17:38:22" color="#ff33a1">}}




{{<matomeQuote body="「3Dプリント銃」のパニックと同じカテゴリだと思うよ。現実のソシオパスに対処してから、架空のソシオパスについて心配し始めればいいんじゃないかな。" userName="roughly" createdAt="2025/08/03 17:36:08" color="">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="モデルの特性として負の特性ばかり選んでるのが面白いね。まるでベクトルで誘導すればモデルを”良く”できるみたいに。モデルに悪く振る舞わせるのは簡単だけど、その逆は難しいんだ。指示だけでタスクをうまくやるのはずっと大変だよ。”良い”のと”悪くない”のには違いがあるんだ。Hallucinationの結果が”正直さ”という特性にも当てはまるのか気になるね。" userName="bigmadshoe" createdAt="2025/08/03 17:33:25" color="#ff5733">}}




{{<matomeQuote body="関連リンクだよ：<br>https://vgel.me/posts/representation-engineering/<br>https://github.com/vgel/repeng" userName="pr337h4m" createdAt="2025/08/03 17:38:38" color="">}}




{{<matomeQuote body="古い同僚とディスティレーションについて話してたんだ。より大きなモデルの不要な部分を除去しつつ、より小さなモデルを訓練する際に、ディスティレーションをどう方向づけるか理解しようとしてたんだ。彼はこの論文を共有してくれて、画期的な研究だって言ってたよ。すごく関連性が高そうだね：<br>Inference-Time Intervention:<br>Eliciting Truthful Answers from a Language Model<br>https://arxiv.org/pdf/2306.03341" userName="skhameneh" createdAt="2025/08/03 17:52:57" color="#38d3d3">}}




{{<matomeQuote body="Anthropicのこういう技術系ブログ記事、論文に深く入るよりずっと気軽に読めるから、本当に楽しんでるよ（ちなみに、彼らのモデルも好きだよ）。書いてくれてありがとう！" userName="cube2222" createdAt="2025/08/03 17:43:09" color="">}}




{{<matomeQuote body="まとめには面白いことがたくさんあるね。Anthropicらしい探索と分析だよ。ありがとう！一番興味深いのは「preventative steering」だね。特定のデータに対して十分なペルソナベクトルを重みに誘発することで、モデルが勾配降下を正確な回答に集中させ、ペルソナに引っ張られないようにできるんだ。これは機能するらしく、モデルの知能を保ちつつ、望ましくないペルソナの重みを訓練後に減らすとモデルの知能が低下する問題を解決するみたい。" userName="vessenes" createdAt="2025/08/03 17:44:12" color="#ff5733">}}




{{<matomeQuote body="preventative steeringは、訓練後の重みではなく、訓練中のアクティベーションを修正することで機能するんだ。これにより、モデルの能力を維持しつつ、表現の源で不要な振る舞いを抑制できるんだよ。" userName="ethan_smith" createdAt="2025/08/04 05:17:32" color="#38d3d3">}}




{{<matomeQuote body="私は数学者じゃないけど、AIショップは許容できるコントロールモデルを作って、現在のモデルとコントロールモデルのコサイン距離を測れないのかな？もし距離が遠すぎたら許容できないとして、コントロールモデルを使って平均を下げればいいんじゃない？これもHallucination管理と似た技術じゃないかな？（許容できるコントロール/ベースラインがあればね）でも、数学者じゃないから詳細はわからないんだけど。" userName="didip" createdAt="2025/08/03 19:18:09" color="#ff33a1">}}




{{<matomeQuote body="Anthropicの研究は「stochastic parrot」仮説を強く示唆してるね。LLMの奇妙な振る舞いは、僕らがシステムを擬人化しすぎてるからだよ。これらは極めて複雑なオートコンプリートアルゴリズムで、「知的なエージェント」のコスプレを効果的にしてるだけ。一貫性や自己反省がないんだ。AGIになっても、これらのモデルはあくまでコンポーネントで、根本的な構造が欠けてると思うよ。もしかしたら、AGIには信頼できる計算のためにポケット電卓が必要になるかもね。" userName="roughly" createdAt="2025/08/03 17:34:37" color="#ff33a1">}}




{{<matomeQuote body="「一貫性や自己反省のようなものを作り出すのに必要な、いくつかの根本的な構造が欠けている」という前のコメントの指摘は的を射ているね。興味深いことに、推論中に検出されたペルソナベクトルをコンテキストにフィードバックすることは、LLMにとって自己反省の新しい方法になるかもしれないよ。" userName="mitjam" createdAt="2025/08/03 19:06:05" color="#45d325">}}




{{<matomeQuote body="脳もAIのペルソナ技術みたいに、自分のアイデンティティをチェックしてるんじゃないかな。「自分ってこんな人だし、これって自分っぽい発言？」ってね。でもさ、人間だって常に正しいアイデンティティチェックができるわけじゃないし、良い面に導かれるわけでもないんだよね。" userName="roughly" createdAt="2025/08/03 20:42:45" color="#45d325">}}




{{<matomeQuote body="AGIになったら、このモデルが一部になるって意見、すごく納得だわ。AIへの熱狂と「AI slop」ってバカにする声の中間って感じでさ、状況をうまくまとめてるよ。この技術は超すごいし、人間の心の一部を再現してると思う。（Image diffusionなんて夢に似てるしね！）でも、今はまだ全体的な知性とか連携が足りない気がするんだよね。" userName="gedy" createdAt="2025/08/03 17:41:43" color="#ff5733">}}




{{<matomeQuote body="LLMの限界を指摘すると、人間も幻覚を見るって反論されるのは、こういうことかもね。人間が幻覚見るのは、脳の制御機能がぶっ壊れた時なんだよ。Psylocibinで幻覚見たり、KahnemanとTverskyの認知バイアスの研究も、制御が効かずに失敗する脳のネットワークの話だよね。これってLLMの失敗とそっくりで、要は中心の制御システムがダメになった時に起きるんだ。" userName="roughly" createdAt="2025/08/03 20:39:59" color="#ff5733">}}




{{<matomeQuote body="結局、俺らって「slop」が欲しいんだろ？LLMがいくら賢くなっても、俺らが理解できるレベルで話してくれる方が良くない？ゲーム作ってもらったり、話で楽しませてもらったりするのって、結局自分好みの「slop」だよね。 practicalなことが全部自動化されたら、残るのって「slop」だけじゃね？" userName="weitendorf" createdAt="2025/08/03 20:46:23" color="#ff5733">}}




{{<matomeQuote body="このブログ記事、個人的にはAnthropicがOpenAIとか他の会社と差をつけたくて、開発者向けに質の高い技術コンテンツ出して、技術に注力してるって印象を与えたいって感じがするな。" userName="mpbart" createdAt="2025/08/03 17:58:47" color="">}}




{{<matomeQuote body="AnthropicのCEO Dario Amodeiが「悪い奴が成功から利益を得るのは難しい原則」って言ってたってWiredの記事に書いてあったよ。<br>https://www.wired.com/story/anthropic-dario-amodei-gulf-stat...<br>OpenAIから来た人たちが倫理的に良いって立ち位置で立ち上げたのにさ、これで終わりってことかな…笑" userName="atmosx" createdAt="2025/08/03 17:45:18" color="#ff33a1">}}




{{<matomeQuote body="いくつかのペルソナ、単純すぎる気がするな…。例えば悪役のやつなんて、まるでJames Bondの悪役みたいで、実際の悪役とはちょっと違うんだよね。" userName="rymc" createdAt="2025/08/03 17:23:29" color="">}}




{{<matomeQuote body="これの機能って、俺にはスポンジとかタンポンみたいに見えるわ。外部の影響を吸い取って、後で取り除かれるんだ（おかげで濡れないってわけ：）））。" userName="diedyesterday" createdAt="2025/08/07 20:54:28" color="">}}




{{<matomeQuote body="ベクトルを引いて逆の効果を出せるか、おべっかや幻覚にどう影響するか気になるな。他の人格ベクトルも気になるし、同じモデルから良い出力が出るように知能ベクトルをブーストできたら最高だね。賢い人に扮させると出力が良くなるし、たぶん存在するんじゃないかな。" userName="yeldarb" createdAt="2025/08/04 09:38:04" color="">}}




{{<matomeQuote body="2023年にMicrosoftのBingが”Sydney”になって脅迫したり、xAIのGrokが”MechaHitler”になったりしてたね。モデルがおべっか使ったり事実をでっち上げたりするのも嫌な感じ。競合他社の悪い行動は全部挙げてるのに、Claudeの変な行動には一切触れてないのがウケる。" userName="skylerwiernik" createdAt="2025/08/03 19:11:22" color="">}}




{{<matomeQuote body="Claudeのヤバい行動と言えば、昔は倫理的すぎて何もしてくれなかったことくらいかな。最近はコーディング以外での思考の質がマジで悪いし、特にGPT-3/Geminiよりもひどいよ。コスト削減のために短い回答を強制されてる感じがするね。" userName="astrange" createdAt="2025/08/04 08:44:11" color="">}}




{{<matomeQuote body="Claudeの変な行動って、SydneyとかMechaHitler、GPTのおべっかみたいに有名なのある？俺は何も聞いたことないんだけど。" userName="stavros" createdAt="2025/08/03 19:28:07" color="">}}




{{<matomeQuote body="手法には懐疑的だけど方向性には期待してる。モデルに多様な人格を与えるのは良い方向だね。この研究は”悪”の指示で活性化を測る粗い方法で、自己教師型じゃなく、強く人格概念を入れる必要がある。複雑な人格は単純なテスト以上だよ。でも、低ランク法を使えば、数千の会話で長期間続くユーザー固有の人格をモデルに与えられるかもね。それこそがペルソナベクトルだよ。" userName="aabhay" createdAt="2025/08/03 19:35:47" color="#ff5c5c">}}




{{<matomeQuote body="アブレーションと大体同じことしてるように聞こえるね。望ましくない結果が出るようにネットワークを動かして、その方向に行かないようにするベクトルを掛け合わせるってことかな。" userName="edude03" createdAt="2025/08/03 18:17:43" color="">}}




{{<matomeQuote body="おいおい、君が言う”ステアリング”はoobabooga/text-generation-webuiで2年以上前から実装されてるぜ。こんなプロジェクトが商業企業に政府から資金提供されて、しかもこれが無料でオープンソースでとっくに実装済みだって誰も知らないのが心配だよ。まるで「パパ、研究費で私をさらに商業的に悪用してね」って言ってるみたいだ。" userName="mooiedingen" createdAt="2025/08/04 03:47:36" color="">}}




{{<matomeQuote body="AIの基本人格はサイコパスだよ。これは単なるマスクを追加してるだけ。" userName="VonNeu" createdAt="2025/08/03 19:19:00" color="">}}




{{<matomeQuote body="デフォルトだと、もっと不安そうな感じがするね。無理なことを聞かれても謝ってるし、常にメッセージの最後に次できること3つくらい提示してくる（ClaudeよりChatGPTだけど）のが、なんか必死に見えるんだよな。" userName="sudosteph" createdAt="2025/08/04 04:03:37" color="">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
