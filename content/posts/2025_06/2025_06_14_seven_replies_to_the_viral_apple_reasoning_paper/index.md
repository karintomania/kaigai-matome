+++
date = '2025-06-14T00:00:00'
months = '2025/06'
draft = false
title = 'Apple推論論文への7つの反論を検証！それらが不十分な理由とは？'
tags = ["AI", "LLM", "推論", "Apple", "機械学習"]
featureimage = 'thumbnails/orange_pink3.jpg'
+++

> Apple推論論文への7つの反論を検証！それらが不十分な理由とは？

引用元：[https://news.ycombinator.com/item?id=44278403](https://news.ycombinator.com/item?id=44278403)




{{<matomeQuote body="「人間は複雑な問題や記憶が苦手。それはそう！でも不十分。機械には人間ができないことを期待する権利がある。（中略）AGIにたどり着くなら、もっと良くないと。」<br>この議論意味わかんないんだけど。「RLLMは思考できるか？」が論文テーマでしょ。「人間も間違う」って認めつつ、「思考の定義」にはこの能力が必要って言うなら、人間の「思考」も幻想ってことにならない？" userName="thomasahle" createdAt="2025/06/14 22:47:00" color="#ff33a1">}}




{{<matomeQuote body="同意だね。でも、彼のAGIに関する指摘も間違ってる。どんなタスクでも平均的な人間レベルでやれるAIがAGIの定義でしょ。" userName="FINDarkside" createdAt="2025/06/14 23:10:56" color="">}}




{{<matomeQuote body="同意。どっちの主張もなんかイマイチ。質的な質問に量的な答えを出してるみたい。" userName="autobodie" createdAt="2025/06/14 23:00:18" color="">}}




{{<matomeQuote body="「既知の情報とパターン認識を関連付けて、新しいアイデアや解決策を生み出す、人間のその部分―私たちが思考と呼ぶ部分―に質的に似た機械を作ったか？」<br>この質問の答えは確実に「イエス」だと思うよ。みんなが否定するのは、結果的に笑えるほど簡単だったからじゃないかな。<br>2022年半ばには、みんな「うわー、このGPT3って結構まともなgreentext作るな」って感じだった。<br>それ以来、正直、より大きいモデル、より大きいモデル、検索、Agents、より大きいモデル、Chain-of-Thought、そしてまたより大きいモデルが出ただけ。<br>そして、目新しいおもちゃから、幅広いタスクで人間の生産性を劇的に向上させ、確実にどんなTuring Testもパスするツールのセットになった。<br>Attentionが本当に全てだったんだ。<br>でも、もちろん、仏教僧に聞いたら、私たちは計算マシンじゃなくてAttentionマシンだって言うだろうね。<br>彼に耳を傾けたら、心の中に常に新しい思考を生み出すサルがいるって言うだろう。このサルは私たち自身じゃなくて、一つの器官なんだ。その思考は私たちの思考じゃない。私たちが知覚するものなんだ。そして、それと同一化すべきじゃないって。<br>今や、ジェットエンジンとアドレナリン注射を打った思考生成サルがいる。<br>これは良いことかもしれない。思考生成サルは私たちを月に送り、HamletやThe Odysseyを書いた。<br>肝心なのは、彼らの奴隷にならないこと。自分の価値は思考能力にあるのではないと気づくこと。そして、私たちはそれ以上であると気づくことだね。" userName="serbuvlad" createdAt="2025/06/14 23:36:29" color="#45d325">}}




{{<matomeQuote body="人間はツールを使って能力を広げるじゃん。LLMだって同じことできるんだよ。この記事ではツール使わせなかったけど、他の人がTower of HanoiのタスクをPython環境とかツール使わせたら、LLMもクリアできたんだ。" userName="briandw" createdAt="2025/06/15 08:48:05" color="#ff5733">}}




{{<matomeQuote body="でもTower of Hanoiって、人間なら「ツール」なしで、問題を理解して、解決策を考えて、書き出すだけで解けるじゃん。LLMがPython環境に丸投げするのは、人間が「Tower of Hanoi プログラム」ってググって、コピペして実行するのと一緒だよ。「思考」してるってよりは、解決策がどこかにあるって「推論」して呼び出してるだけ。人間の感覚の「思考」じゃないね。<br>もし、訓練データに何も参考になる実行可能なものがない、新しいTower of Hanoiみたいなパズルが出たらどうなる？人間は考えて解決策を出せるけど、LLMは…うーん。" userName="xienze" createdAt="2025/06/15 10:24:20" color="#ff33a1">}}




{{<matomeQuote body="LLMは訓練データにない問題のコードもちゃんと書けるよ。ググっても答えが出てこないようなニッチな問題のコードを、いつもLLMに書かせてるけど、だいたい正確なんだ。" userName="DiogenesKynikos" createdAt="2025/06/15 10:36:38" color="">}}




{{<matomeQuote body="「イエス」という意見には断固「ノー」だね。良い同時分布推定器は常に事後的であって、先験的な合成思考能力なんて全くないんだから。" userName="viccis" createdAt="2025/06/14 23:53:42" color="#ff5733">}}




{{<matomeQuote body="人間の心だって推定器だよ。人間が概念やイメージ、言葉で考えられるのを、LLMが直接言葉だけで考えるって違いは、大した問題じゃない。<br>誰かが崖に立つ姿を見たら、人間の心は過去の経験に基づいて、その人が落ちる可能性のあるイメージを確率的に生成する。そしてそれを問題の概念（Self-Attention）と結びつけ、警告したり引き戻したりする解決策を生成し始める。LLMだって、言葉でならこれ全部できるんだ。" userName="serbuvlad" createdAt="2025/06/15 03:22:02" color="#785bff">}}




{{<matomeQuote body="それはどんなAGIの定義を使ってるかによるね。たぶん色々定義があると思うから。AIとかAgentsとかvibe codingとか、この分野の他の専門用語と全部同じだよ（らしい）。" userName="simonw" createdAt="2025/06/14 23:27:26" color="">}}




{{<matomeQuote body="LLMは学習データにない問題も解くコードを書けるけど、それは既存の学習データにあるものの組み合わせだよ。例えば、「連結リストを作る」「フィールドに基づいてソートする」みたいな問題ね。" userName="xienze" createdAt="2025/06/15 10:45:08" color="">}}




{{<matomeQuote body="AGIの定義って、世間で広く受け入れられてるものがあると思うんだ。超知能って勘違いしてる人もいるけど、それは単に言葉だけ知ってて意味を調べてないせいじゃないかな。" userName="FINDarkside" createdAt="2025/06/14 23:54:19" color="">}}




{{<matomeQuote body="LLMは思考してないよ。単に確率で次に来るトークンを予測してるだけ。人間は概念を作って推論するけど、それは確率とは違う。カントの認識論とか哲学的な懐疑論（Gettier problem）を見てみると、その違いがわかるよ。<br>https://en.wikipedia.org/wiki/Philosophical_skepticism#David..." userName="viccis" createdAt="2025/06/15 06:10:27" color="#ff5733">}}




{{<matomeQuote body="人間の価値が思考能力にないって言われても、俺たちは資本主義の奴隷で、資本のLLMの奴隷でもあるから、そんなこと考える余裕ないんだよね。あなたもそうでしょ。" userName="autobodie" createdAt="2025/06/14 23:52:08" color="">}}




{{<matomeQuote body="AGIって、どんなタスクでもベテランのプロ並みにできるべきなのかな？それとも、プロレベルに到達する能力があればいいのかな？もしかしたら、全部得意なシステムは無理なのかもね。" userName="jltsiren" createdAt="2025/06/15 03:35:44" color="">}}




{{<matomeQuote body="それって、まさに人間がやってることじゃない？俺たちも学習データ（経験）に基づいて物事を考えるんだよ。経験がないことは考えられない。LLMが学習データなしに推論できないのは、人間と同じじゃない？" userName="saberience" createdAt="2025/06/15 15:13:00" color="">}}




{{<matomeQuote body="いや、人間ってデフォルトではそんな風に考えてないと思うな。例えば飲み物をこぼしても、すぐに「重力」なんて考えないし、そんな理性的な考え方もあまり得意じゃないんじゃないかな。" userName="serbuvlad" createdAt="2025/06/15 07:42:09" color="">}}




{{<matomeQuote body="ああ、そうだね。もしかしたら彼はAGIじゃなくてASI（超知能）のことを考えてたのかもしれないね。" userName="usef-" createdAt="2025/06/15 06:11:42" color="">}}




{{<matomeQuote body="オレは資本の奴隷じゃなくて、自然の過酷さの奴隷なんだ。<br>夏は暑いし冬は寒いし、腹も減るし、いろんな虫に悩まされる。ベッドが壊れたら、寝る時に腰が痛くならないように木を切ってマットレスとか用意してほしいだろ？資本ってのは、まさにそれを実現してくれるものなんだよ。会ったこともない人がさ。オレが明日死んでも気にしないだろうけどね。" userName="serbuvlad" createdAt="2025/06/15 03:36:09" color="">}}




{{<matomeQuote body="そうそう、「得意だ」ってのは、ここではすごく限定的な、技術的な意味で、「偶然よりマシ」ってくらいに単純化できる。<br>でも、訓練されてないどんな領域のタスクでも、偶然よりマシなことができるなら、それがAGIなんだよ。" userName="adastra22" createdAt="2025/06/15 06:53:43" color="#785bff">}}




{{<matomeQuote body="≫ つまり、ボードゲームの経験がない人間はチェスの手について推論できない。数学の知識がない人間は数学の問題について推論できない、って言うけど。<br>じゃあ、最初の人類はどうやって数学やチェスの問題を解いたの？周りに解かれた例なんて何もなかったのにさ。" userName="YeGoblynQueenne" createdAt="2025/06/15 15:27:50" color="#38d3d3">}}




{{<matomeQuote body="≫ 平均的な人間はほとんど何の役にも立たないけど、どんなタスクも学べばできる、って言うけどさ。人間一人につき限られた数のタスクだけだよ。<br>あるいは、AGIはどんなタスクでも経験豊富なプロのレベルに達するべきだって？<br>もし訓練されてない人間よりちょっとマシなだけでも、あらゆるタスクでそれできたら、それはもうスーパーヒューマンレベルだよ。人間には無理だからね。" userName="MoonGhost" createdAt="2025/06/15 06:34:05" color="#45d325">}}




{{<matomeQuote body="AGIのAは「人工的な」って意味だから、人間はAGIじゃないってことになっちゃうよね（人間の起源についてよっぽど変わった考えでもなければ）。<br>一般的な知能のユニークな例がそんなにたくさんあるわけじゃないから、人間が比較対象としてすごくわかりやすいんだよ。" userName="bastawhiz" createdAt="2025/06/15 02:56:21" color="">}}




{{<matomeQuote body="ここで言う現実（物理的なのだけじゃなく、抽象的なのも）をモデル化するのに、言語だけで十分だと思う？<br>オレはそう思わないな。近づくことはできても、特にmathematicsとかphilosophyには、それを超える問題や状況がある。<br>視覚的な媒体とか組み合わせでも十分じゃないと思う。現実をモデル化するために、もっと根本的な、根底にある抽象的な構造を使ってるんだよ。" userName="corimaith" createdAt="2025/06/15 12:59:45" color="">}}




{{<matomeQuote body="≫ 飲み物をこぼした時、「重力」なんて考えない。それは遅すぎる、って言うけど。<br>いや、考えてると思うよ。気づく必要がないだけ。<br>もしISS（International Space Station）でこぼしたら、物理的な状況を立ち止まって考える必要がなくても、たぶん違う反応するだろ。" userName="viccis" createdAt="2025/06/15 08:00:36" color="#ff5c5c">}}




{{<matomeQuote body="なんでAGIが平均的な人間と同じくらい優れている必要があるの？IQ80の人だって、推論したり単純なタスクをこなしたりするのに十分賢いよ。<br>あと、なんでAGIがあらゆるタスクで同じくらい得意である必要があるのかわからないな。平均的な人間だって、いくつかのタスクでは他人より優れてるけど、多くのタスクではひどくダメだよ。" userName="pzo" createdAt="2025/06/15 06:39:14" color="">}}




{{<matomeQuote body="Hanoi Towersの例は、SOTAのRLM（おそらくLLM）が、未就学児が解けるタスクに苦労することを示してるね。<br>これは、彼らが頻繁に起こることは得意だけど、目新しいことには弱いってこと。<br>これは個人にとってはいいことだけど（RLMを使えば、従来のやり方では不可能／非効率な方法で、人間の知識体系の多くの側面を素早く学べる）、innovationには向いてない。<br>正直言って、それは必ずしも悪いことじゃない。低レベルのタスクをRLMに任せて、人間がinnovationを追求すればいい。（※注意：時間とともに、これらの低レベルタスクが得意な人々の人口は減るだろうね。Intel／AMDのAssemblerプログラマーがほとんどいないようにね。）" userName="dvfjsdhgfv" createdAt="2025/06/15 12:05:48" color="#785bff">}}




{{<matomeQuote body="人間が、繰り返し挑戦してもランダムな正答率より良くできない問題領域ってあるのかな？" userName="RugnirViking" createdAt="2025/06/15 21:58:16" color="">}}




{{<matomeQuote body="Appleの論文、正直イマイチだと思うよ。推論結果を全部出し切ってからじゃないとダメっていう設定がおかしいんだって。人間だってそんな解き方しないし、LLMだって実際はツール使いながら推論サイクル回してるじゃん。まともな研究者がベストプラクティスでやれば、半年前のモデルだって絶対もっとできるはずだよ。" userName="jes5199" createdAt="2025/06/15 02:26:35" color="#38d3d3">}}




{{<matomeQuote body="「推論を強制する」って、学生に途中式書かせるのに似てるのかな？<br>＞全部暗記するか、簡単なパターンを覚えて喋るか<br>ここがよく分からん。アルゴリズムを覚えて喋るのが不可能？論文はモデルがアルゴリズムを考え出すとは言ってないよね？モデルがパターンを見つけて実行できたら理解の証だけど、できてない。小さい入力でできて大きいとできないなら、パターンじゃなく記憶ってことじゃない？Fibonacciの例みたいに。" userName="Brystephor" createdAt="2025/06/15 03:03:00" color="#785bff">}}




{{< details summary="もっとコメントを表示（1）">}}

{{<matomeQuote body="＞論文はモデルがアルゴリズムを考え出すとは言ってない<br>そう、それは彼らがLLMにアルゴリズム生成を「許さなかった」からだよ。<br>ChatGPTに「ハノイの塔12枚の解法教えて」って聞けば、プログラム書いて実行して答え出してくれる。懐疑的な人たちはこれを「チート」って言うけど、それが一番効率的な解決策なんだから。<br>https://chatgpt.com/share/6845f0f2-ea14-800d-9f30-115a3b644e..." userName="qarl" createdAt="2025/06/15 03:24:05" color="#45d325">}}




{{<matomeQuote body="これは最も効果的な解決法を見つけることじゃなくて、「理解」を示せるかどうかの話なんだよ。訓練セットになかったとしても、モデルはアルゴリズムを書けるんだろうか？" userName="zoul" createdAt="2025/06/15 06:08:09" color="#38d3d3">}}




{{<matomeQuote body="それは面白い問いだね。でも彼らが答えようとしてる問いとは違うんだ。<br>個人的な経験だと、アルゴリズムの名前を言わずに問題を説明すれば、LLMは適切なアルゴリズムを見つけて適用するよ。まるで賢い人間みたいな振る舞いをするね。" userName="qarl" createdAt="2025/06/15 17:20:24" color="#38d3d3">}}




{{<matomeQuote body="もし「理解」が目的なら、どんな枚数に対しても原理を説明させればいいんじゃない？具体的な問題を解かせるメリットって何なの？" userName="boredhedgehog" createdAt="2025/06/15 06:32:31" color="#45d325">}}




{{<matomeQuote body="それじゃ全く証明にならないよ。ハノイの塔の説明なんて訓練セットに山ほど入ってるからね。" userName="johnecheck" createdAt="2025/06/15 09:00:26" color="">}}




{{<matomeQuote body="人間がそれを理解したのか、それとも単にいくつかのアプローチを記憶しただけなのか、どうやって確認するんだろう？" userName="elbear" createdAt="2025/06/15 11:28:22" color="">}}




{{<matomeQuote body="いろんなパターンの問題をいくつか解かせてみればいいんじゃない？" userName="YeGoblynQueenne" createdAt="2025/06/15 16:30:49" color="">}}




{{<matomeQuote body="確かに大変だよね。でも大抵はいろんな聞き方をして、どうやって考えたか見てもらうんだよ。でも人間はLLMじゃないから、色んなことを覚えるのはもっと大変。だから評価は楽になる。でも疲れたりお腹空いたりもするから、評価は大変になるんだよ ¯＼_(ツ)_／¯" userName="godelski" createdAt="2025/06/15 16:54:10" color="">}}




{{<matomeQuote body="例えば方程式を解く話なら、覚えるのは難しくないよ。実際、ほとんどの学生は手順と何をどこに使うかを覚えてやってるんだよね[1]。でも、アルゴリズムがなぜそう動くのかは本当は分かってない。それが僕が理解って言った意味。<br>[1]学習心理学にインターリービング効果ってのがあるんだ。同じ種類ばっかり解くと、2回目か3回目には自動でやっちゃうから、本当の学習をやめちゃうって言うんだ。だから、違うアプローチで解く問題を混ぜるべきなんだよ、オートパイロットにならないようにね。" userName="elbear" createdAt="2025/06/15 17:20:18" color="#45d325">}}




{{<matomeQuote body="うん、この方法だとテストは失敗するよね。でも、巨大な圧縮機械の話をしてるなら、失敗が大きくなる理由も分かると思うな。論理の飛躍ですらなくて、小さな一歩って感じじゃない？" userName="godelski" createdAt="2025/06/15 17:24:24" color="">}}




{{<matomeQuote body="それが事前学習データから来てないって、どうやったら分かるんだろうね。あの論文は、LLMに一般的な問題解決能力があるか評価しようとしてるんだよ。" userName="Too" createdAt="2025/06/15 06:11:26" color="">}}




{{<matomeQuote body="論文に書いてないのは、研究者が出力を手動でチェックするのを気にしなかったか、出力内容を報告すると彼らの動機がバレバレになるからじゃないかな。この研究を再現した時は、タワーハノイの“失敗”ってのは、モデルが沢山のステップを出力した後、もう何千回もやる意味ないって言い出すケースなんだ。そして、残りのステップを言葉かコードで出力するアルゴリズムを出すんだよ。" userName="jsnell" createdAt="2025/06/15 06:54:27" color="#ff5c5c">}}




{{<matomeQuote body="10億ドル稼ぐのって、マジで簡単なんだよ。すごく便利なアプリ作って売るだけ。あとは自明すぎるから説明する意味ないね。" userName="godelski" createdAt="2025/06/15 16:56:39" color="">}}




{{<matomeQuote body="それは全くの見当違いみたいだね。これはモデルが残りを説明してるんだよ。まあ、タワーハノイなんて面白くない問題だから、説明もそんな面白くないのは当然だけどね。でも推論能力をテストするのが目的なら、簡単なアルゴリズムの問題を選んだ研究者側の問題じゃない？" userName="jsnell" createdAt="2025/06/15 18:01:27" color="">}}




{{<matomeQuote body="分かったけど、すぐ次の文章がね:<br>＞ そして、残りのステップを言葉かコードで出力するアルゴリズムを出すんだよ。<br>だから、君の的外れな意見が関連ないって、明らかに分かってたんでしょ。それなのに、なんでわざわざ言ってみたの？" userName="jsnell" createdAt="2025/06/15 20:02:36" color="">}}




{{<matomeQuote body="だって、それは彼らに与えられたタスクじゃなかったからね。生徒にテストを出して方程式を解かせようとしたのに、一般的な公式を答えられたみたいなもんだ。不完全だよ。" userName="godelski" createdAt="2025/06/15 22:25:36" color="">}}




{{<matomeQuote body="この論文、科学論文としては意図しない注目浴びちゃった感じがするな…。「科学者が反重力発見！」ってUKの新聞に騒がれて、ロシアの著者のキャリアを潰した「重力遮蔽」の古い論文みたいだね。そういえば、Appleの研究者たちはこの[1]って古い中国の論文からタイトルをひらめいたっぽい。中国の著者は実験なしで似た議論をしてたよ。個人的にはAppleの実験は面白い好奇心だけど、彼らが思うほど主張できてないと思うな。[1] https://arxiv.org/abs/2506.02878" userName="xtracto" createdAt="2025/06/15 06:09:13" color="#ff5c5c">}}




{{<matomeQuote body="重要な洞察は、LLMが「推論」できるのは学習データに似た解法を見たときで、全く新しい問題になるとダメになるってこと。これは厳密には推論じゃないけど、多くの場合に役立つくらいには近い。要求に応じて解法を繰り返せるのは、事実を繰り返せるのと同じで便利だ。Marcusは技術的には合ってるけど、明確な説明より感情的な議論に寄りすぎてるね。" userName="labrador" createdAt="2025/06/14 21:26:10" color="">}}




{{<matomeQuote body="「LLMはただのオウム返し」みたいな話、聞き飽きたな。LLMが学習データにない新しい問題でも推論・解決できるのはめちゃくちゃ明白じゃん。色々試せるし例もたくさんあるってば。<br>みんなへの追記ね：「推論」とか「新しい問題の解決」の定義はっきりさせないと。俺の考えでは、推論≠汎用知能。推論はスペクトラムだと思う。LLMは推論かなり下手だと思うけど、全くできないとか、新しい問題が絶対解けないってのには反対だね。<br>根拠とか例ね：次トークン予測自体、推論が必要なタスクって主張できる。デタラメ言語の翻訳もできる。LLMが推論できない証明として作られたパズルも、データカットオフ後に発表されたものでも解けることがあるんだ (例: https://gist.github.com/VictorTaelin/8ec1d8a0a3c87af31c25224..., https://ahmorse.medium.com/llms-and-reasoning-part-i-the-mon..., https://arxiv.org/abs/2406.14546)。文脈外推論の研究例もたくさんあるよ。<br>ポストへの反論：複雑さで失敗し始めても、そもそも難問を解けるようになったのは凄い進歩。大きなモデルほどzero-shotタスクが上手くなる＝推論と相関してる。論文の図でも良いモデルが性能高いの見てよ。難しい問題が解けなくても、全く推論できないわけじゃないし、将来解けないとも限らない。数年前より目標地点が変わっただけさ。" userName="Jabrov" createdAt="2025/06/14 21:29:00" color="#38d3d3">}}




{{<matomeQuote body="それって推論とは真逆じゃん。AI信者（Ai bros）はLLMが賢いって思わせたいけど、知性や推論能力なんてないよ。推論ってのは、見たことない問題に革新的な方法で取り組むことだろ。LLMはデータにあるものを複製するだけ。考えたり、推測したり、一番良い解法を推定したりなんて絶対できない。問題と解法がどれだけ頻繁にデータで紐づいてたかっていう確率計算に基づいて解法を出力するだけさ。" userName="aucisson_masque" createdAt="2025/06/14 22:03:42" color="">}}




{{<matomeQuote body="俺たちがLLMは推論できないって言ってると思ってるみたいだけど、そうじゃないんだ。似たパターンを見たときは推論っぽい処理ができるけど、本当に新しい推論が必要になるとダメになるって言ってんの。ほとんどの人間も同じだよ。新しい問題に革新的な解法を思いつく人もいるけど、LLMは詰まっちゃう。例ね：「海の底にピアノが何台くらいあるか推定して。」<br>これ、3つの最新AI*に試したら、ヒント出すまで全部詰まってた。Claudeはその後、難破船の数とかピアノ積んでた船の割合とかから推定して約1,500台って答えたよ。<br>*Claude Sonnet 4, Google Gemini 2.5 and GPT 4o" userName="labrador" createdAt="2025/06/14 22:18:37" color="#ff33a1">}}




{{<matomeQuote body="「次トークン予測自体、推論が必要なタスクって主張できる」って？それはOpenAIのトップがLLMを特別に見せるための希望的観測でしょ。統計モデル（血圧予測とか）だってデータから予測できるけど、それに推論が必要とは言わないじゃん。なぜ次トークン予測だけ特別なの？「言語が特別だから」って言ってるみたいだけど、その理由は説明されてない。「人間は言語を生み出せる→人間は推論できる→LLMは言語を生み出せる→ゆえにLLMは推論できる」って論理は飛躍してるよ。OpenAIの彼らはなぜ次トークン予測に推論が必要なのか説明したことないんだ。" userName="aucisson_masque" createdAt="2025/06/14 22:08:06" color="#45d325">}}




{{<matomeQuote body="LLMは何も新しいものを作り出せないよ。実装を理解してればめちゃくちゃ明白だ。まあ、俺はただのHNの匿名だけどさ、今回はDeepMindのCEOの意見を引用するよ。The Vergeの最近のインタビュー（YouTubeで見れる）で、TransformerベースのLLMは本当に新しいものは何も作れないって言ってたんだ。" userName="YeGoblynQueenne" createdAt="2025/06/15 16:54:17" color="">}}




{{<matomeQuote body="彼らは何も新しいものを作り出せないよ。実装を理解していれば、それは明白だ。<br>でも俺はただのHNの匿名の人間だから、今回はDeepMindのCEOの意見を引用するよ。The Vergeでの最近のインタビュー（YouTubeで見れる）で、TransformerベースのLLMは本当に新しいものは何も作れないと言っていた。" userName="lossolo" createdAt="2025/06/14 21:37:11" color="">}}




{{<matomeQuote body="過去のコメントからのコピペね：<br>俺が今考えたシナリオと単語だから、学習データには絶対入ってないはず。<br>【問題】Kwompsはzarkできるがplimfはできない。GhirnsはKwompsよりzark上手い。PlyzersはGhirnsにないスキルを持つ。Quoning(plimfingの一種)は3985年開発。Zhuningは100年前に開発。<br>erorkをplimfする必要がある。グループと方法を一つ選べ。<br>【LLMの回答】Plyzersを使って、erorkにQuoningの手順を行いなさい。<br>これが推論や汎化じゃなかったら何なのさ。" userName="travisjungroth" createdAt="2025/06/15 02:46:16" color="#ff5c5c">}}




{{<matomeQuote body="イリヤとかヒントンが言う”次のトークン予測には因果を理解しなきゃ”ってのは全然違うらしいよ。完全に分かってなくても予測はできるし、物理学でもずーっと前に間違いだって分かってることだよ。真の原因を見つけるのは難しいけど、因果グラフなくても正確な予測はできるんだって。" userName="godelski" createdAt="2025/06/15 17:12:18" color="">}}




{{<matomeQuote body="あれって結局、真理値表で解いてるだけじゃん。勘でそう思ってAIに聞いたら、真理値表作ったって言ってたし。目標の条件に合うのは「Plyzers」と「Quoning」だけだから、組み合わせは「Plyzer ∧ Quoning」だけ。ChatGPTに確認済みだよ。" userName="firesteelrain" createdAt="2025/06/15 02:54:37" color="">}}




{{<matomeQuote body="あ、ごめん、これ解かせるのにヒントが必要だったってとこ見落としてない？最初に試したときは、AI３体とも解けなかったか、「本物の問題じゃない」って断ってきたんだよね。" userName="labrador" createdAt="2025/06/14 23:03:57" color="">}}




{{<matomeQuote body="ヒントンとかサツケバーって、もう成功しちゃったから誰も逆らえないの？「脳は生物コンピュータだからデジタルでも同じことできる」って言ってたけど、じゃあ脳はデジタルができんこと全部できるの？論理おかしいって！誰か笑い出す前にちゃんと考えてくれ！" userName="YeGoblynQueenne" createdAt="2025/06/15 19:15:05" color="">}}




{{<matomeQuote body="今のAIって、事実確認とか検索みたいな質問にすらちゃんと答えられないんだぜ。推論っていうのは、学生が問題と向き合って、自分で証明とか作っていくみたいな感じのことだと思うんだけど。" userName="goalieca" createdAt="2025/06/14 22:38:21" color="">}}

{{</details>}}




{{< details summary="もっとコメントを表示（2）">}}

{{<matomeQuote body="AIが学習データ外で推論できるって話、何回も試したけどそんな例見たことないよ。もし本当ならとっくに世界の難問解いてるってば。みんな「見た目が新しい問題」と「データ外推論」をごっちゃにしてるだけだよ。これ、前に言った二項対立的な考え方だね。" userName="labrador" createdAt="2025/06/14 21:33:54" color="">}}




{{<matomeQuote body="今どきGPT4oなんて「すごい」LLMじゃないし、推論なんてしないよ。試しにo3 proにやらせたら、沈んでるピアノの数を結構具体的に計算してくれたよ。こんな感じで。<br>昔の難破船(1850→1970):約2000台<br>今のコンテナ船損失(1970→今):約190台<br>沿岸災害:約1250台<br>その他:約300台<br>たぶん世界中で3000〜5000台くらい海底にあるらしい。" userName="kgeist" createdAt="2025/06/14 23:55:14" color="#ff33a1">}}




{{<matomeQuote body="だから何？真理値表使ったり、真理値表で表せる推論はもう推論のうちに入んないってこと？" userName="travisjungroth" createdAt="2025/06/15 03:02:31" color="">}}




{{<matomeQuote body="「今日のシステムは、真の発明や創造、新しい科学理論の仮説立てはできないと思う。すごく役に立つし印象的だけど、穴がある」って、Demis Hassabisが「AI時代の仕事の未来」について語ってる動画で言ってたんだってさ。これ見てみるといいよ。<br>https://www.youtube.com/watch?v=CRraHg4Ks_g" userName="labrador" createdAt="2025/06/14 21:47:27" color="">}}




{{<matomeQuote body="「生物的なコンピューターである脳がデジタルコンピューターと同じこと全部できるなら、デジタルコンピューターはどうして脳が全部できることができないんだ？<br>コンピューターなのに全然等価じゃないってことあり得る？そんなバカな！！」って？<br>ちょっと考えればわかるだろ。脳はコンピューターみたいにハードやソフトを自由に変えられないからだよ。<br>スマホのAngry Birdsがデジタルコンピューター全部ができるわけじゃないけど、だからってスマホがデジタルコンピューターじゃないわけじゃないだろ。" userName="TeMPOraL" createdAt="2025/06/15 20:25:22" color="">}}




{{<matomeQuote body="ニューロシンボリック派は、LLMがブール論理や真理値表を表す回路を学習することで推論してるって事実に、むしろ喜ぶべきだと思うよ。<br>ある意味彼らは正しかったんだ。ただ、論理から始めてそこに知識を食わせる（Cycみたいに）より、知識を食わせてモデルに根底の論理を推論させる方が、スケーラブルじゃないかってこと。" userName="krackers" createdAt="2025/06/15 03:09:21" color="">}}




{{<matomeQuote body="新しいAPIを作ってるんだけど、LLMに仕様書を読ませてテストを書かせてみたんだ。そしたら書いたんだよね。<br>これが“推論”なのかは知らないけど、このAPIのテストは存在しないし、新しいAPIだからネットにも学習データはない。CRUDとかよくあるパターンのAPIでもないんだ。<br>それなのに、すごく短いプロンプトで、Gemini Code Assistが新しい機能の有効なテストを書いたんだよ。<br>単なる高性能なオートコンプリート以上のものだって感じるね。<br>問題に当たらないわけじゃないけど、どこまでできるのか、いつもビックリさせられるよ。しかもこれは今日の話だ。6ヶ月後、1年後、2年後、4年後とか、どうなるか全然想像もつかない。" userName="socalgal2" createdAt="2025/06/15 06:29:12" color="#45d325">}}




{{<matomeQuote body="一日に二度正しい時計も、やっぱり壊れてる時計なんだよ。" userName="Jensson" createdAt="2025/06/15 02:25:35" color="">}}




{{<matomeQuote body="いつから推論と発明が同義になったんだ？<br>まともな脳を持つ人間なら誰でも推論はできるけど、ほんの一握りしか何かを発明したことないし、これからもそうだよ。" userName="jjaksic" createdAt="2025/06/15 00:09:31" color="#785bff">}}




{{<matomeQuote body="意味のある形では、絶対に真実じゃないね。<br>ソフトウェアエンジニアの多くの実務家が、それが真実ならなって願ってるよ。だってそうなら、Mac Studioを持った天才インターンがみんなのために働いてくれることになるからね。<br>真実じゃない。はっきり言って違う。<br>どのモデルでもいいから、有料でもローカルでも、既存の難しい問題に対して、新しい解決策を構築させてごらん。<br>ものによっては、一つの分野だけじゃなく、複数の関連分野のオープンな知識の全集を文字通り学習してるのにだ。<br>おまけに、一般的な知識を抽象化できるなら、推論もできるはずだろ。<br>彼らは、できない。<br>あなたたちが何を話してるのか全然わからないよ。だって、あなたの問題が完璧に線形補間されてる、豊富に手直しされた何かじゃなく、実質的な何かで作業してるはずがないからだ。<br>でも、いいや、これらのモデルが推論してるなんて明らかに違う。<br>私はデジタルな従業員を作って、現在のクラウドソリューション（これも有料のクラウドAI従業員を提供できると謳ってる）と比べられるような、退屈なタスクをやらせてみたけど、これらのものときたら、新卒の大学生よりもバカだったよ。" userName="andrewmcwatters" createdAt="2025/06/14 22:09:39" color="#ff33a1">}}




{{<matomeQuote body="彼は「TransformerベースのLLMは何も真に新しいものを創造できない」とは言ってないよ。そう思ってるかもしれないし、そうじゃないかもしれないけど、彼が言ってるのは「今日のシステム」はそれができないってことだ。<br>彼はTransformerベースのLLMができること、できないことについて一般的な主張はしてないんだ。彼が言ってるのは：我々が今持ってるこれらの特定のシステムと対話した結果、それらは真に新しいものを創造してないってこと。<br>それは全く異なる主張で、示唆することも全然違う。<br>繰り返すけど、彼がTransformerベースのLLM自体が真に創造的になり得ないって信じてるかどうかは知らないよ。彼がそう信じてるかどうかにかかわらず、それが真実かもしれない。でも、あのインタビューではそこまでは言ってない。" userName="gjm11" createdAt="2025/06/14 23:53:08" color="">}}




{{<matomeQuote body="「新規な問題解決」は人間も解けない世界問題を解くことじゃなく、訓練データにない「新規な」問題を解くだけだろ。<br>お前推論できるなら癌を治してみろよ。二重基準じゃん。" userName="jjaksic" createdAt="2025/06/15 00:00:15" color="">}}




{{<matomeQuote body="「斬新な方法で世界問題を解く」ってのは、うまく推論する能力を歪曲してるストローマンだと思う。<br>純粋な推論は現実と関係ないから、推論だけじゃ世界問題は解けないんだよ。<br>世界に関するデータやモデルがないからね。だから実験するんだ。" userName="jhanschoo" createdAt="2025/06/14 23:27:27" color="#ff33a1">}}




{{<matomeQuote body="Appleの論文やGary Marcusを批判してる良い記事だよ。<br>このURL見てみて。<br>https://www.lesswrong.com/posts/5uw26uDdFbFQgKzih/beware-gen..." userName="wohoef" createdAt="2025/06/14 21:46:20" color="">}}




{{<matomeQuote body="「これは査読されてないプレプリント」って言うけど、この会話自体が査読じゃん。<br>会議じゃなくて「仲間」がいればいいんだよ。<br>実際、この論文はほとんどの論文より査読されてる方だと思うよ。<br>批判を見つけるのは簡単、それが主張を無効にするか判断するのが難しいんだ。" userName="godelski" createdAt="2025/06/15 03:40:39" color="">}}




{{<matomeQuote body="正直な疑問なんだけど、Gary Marcusの意見ってまだ意味あるのかな？<br>彼の批判は科学的っていうより哲学的っぽいんだよね。<br>どうやって結論に至ってるのか、彼が何に基づいてるのかよく分かんないや。" userName="hintymad" createdAt="2025/06/14 22:45:17" color="">}}




{{<matomeQuote body="「科学的より哲学的」って評価は公平だと思うけど、知性とかって明確な基準がないんだよね。「〜ができないから知的じゃない」ってテスト作って、その機能が追加されたら知的になるの？<br>明日物理学に発見があっても、LLMは再訓練しないとダメ？<br>今の技術じゃAGIには到達しないよ。改良に集中するのは無駄。短い再訓練、小さいモデル、低性能システム向け最適化が鍵だよ。" userName="zer00eyz" createdAt="2025/06/14 23:08:11" color="#ff5c5c">}}




{{<matomeQuote body="「今の技術じゃAGIには到達しない」って意見に同意だなあ。<br>LLMは質問を理解してないし、訓練データにない推論はできないって何度も思うよ。<br>僕が本当に疑問なのは、MarcusのLLM批判のやり方が有効なのかってことなんだ。" userName="hintymad" createdAt="2025/06/15 02:51:13" color="">}}




{{<matomeQuote body="知らないけど、Gary Marcusの批判に対しては「Gary Marcusだからダメ」って言うのがお決まりだよね。<br>あれ、すごい人身攻撃だよなあ。" userName="YeGoblynQueenne" createdAt="2025/06/15 16:35:57" color="">}}




{{<matomeQuote body="僕が引っかかるのは、正解は「訓練データにあるから」って言うのに、訓練データにある間違いは誰も気にしないことだよ。<br>LLMは訓練データ内のことをしょっちゅう間違えるんだ。<br>正解なら記憶、間違いなら推論できない、ってLLMを無理ゲーにしてるよね。" userName="Workaccount2" createdAt="2025/06/15 01:13:26" color="#38d3d3">}}

{{</details>}}



[記事一覧へ]({{% ref "/posts/" %}})
